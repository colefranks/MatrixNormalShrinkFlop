\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{amsthm,mathtools,braket}
\usepackage{amssymb,bm, hyperref}
\usepackage{xcolor, float}
\usepackage[capitalize]{cleveref}

\newtheorem{theorem}{Theorem}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{prop}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corol}[theorem]{Corollary}
\newtheorem{fact}[theorem]{Fact}
\newtheorem{claim}[theorem]{Claim}
\newtheorem{remark}{Remark}
\newtheorem{definition}{Definition}
\newtheorem{example}{Example}
\floatstyle{boxed}\newfloat{Algorithm}{ht}{alg}

\DeclareMathOperator{\Lie}{Lie}
\DeclareMathOperator{\Lin}{L}
\DeclareMathOperator{\ope}{op}
\DeclareMathOperator{\poly}{poly}

\DeclarePairedDelimiter{\abs}{\lvert}{\rvert}
\DeclarePairedDelimiter{\norm}{\lVert}{\rVert}
\DeclarePairedDelimiter{\ip}{\langle}{\rangle}


\newcommand{\junk}[1]{}


\newcommand{\R}{{\mathbb{R}}}
\newcommand{\C}{{\mathbb{C}}}
\newcommand{\ot}{\otimes}
\newcommand{\mat}{\operatorname{Mat}}
\newcommand{\op}{\operatorname{op}}
\newcommand{\ch}{\operatorname{ch}}
\newcommand{\cut}{\operatorname{cut}}
\newcommand{\vol}{\operatorname{vol}}
\newcommand{\Q}{{\mathbb{Q}}}
\newcommand{\N}{{\mathbb{N}}}
\renewcommand{\vec}{\bm}
\newcommand{\Z}{{\mathbb{Z}}}
\renewcommand{\S}{\mathbb{S}}

\newcommand{\E}{\mathbb{E}}
\renewcommand{\Pr}{\mathbb{P}}

\newcommand\eps{\varepsilon}

\newcommand\cN{\mathcal{N}}
\newcommand\FF{\mathcal{F}}
\newcommand\HH{\mathcal{H}}
\newcommand\GG{\mathcal{G}}
\newcommand\SL{\operatorname{SL}}
\newcommand\PD{\operatorname{PD}}
\newcommand\Herm{\operatorname{Herm}}
\newcommand\Sym{\operatorname{Sym}}
\newcommand\GL{\operatorname{GL}}
\newcommand\TT{\mathcal{T}}
\newcommand\PP{\mathcal{P}}
\newcommand\tr{\operatorname{Tr}}
\newcommand\rk{\operatorname{rk}}
\newcommand\RR{\mathcal{R}}
\newcommand\CC{\mathcal{C}}
\newcommand\BB{\mathcal{B}}
\newcommand\II{\mathcal{I}}
\renewcommand\AA{\mathcal{A}}
\newcommand{\MM}{\mathcal{M}}
\newcommand\AP{\mathcal{AP}}

\newcommand{\eqdef}{:=}
\newcommand{\maps}{\colon}
\newcommand{\ale}{\lesssim}
\newcommand{\age}{\gtrsim}
\newcommand{\CF}[1]{{\color{purple}[CF: #1]}}
\newcommand{\AR}[1]{{\color{orange}[AR: #1]}}
\newcommand{\RMO}[1]{{\color{red}[RMO: #1]}}
\newcommand{\MW}[1]{{\color{red}[MW: #1]}}
\newcommand{\TODO}[1]{{\color{blue}[TODO: #1]}}

\crefname{Algorithm}{Algorithm}{Algorithms}


\title{Ideas towards operator norm bounds}
\author{Cole Franks, Rafael Oliveira, Akshay Ramachandran, Michael Walter}
\date{February 2020}

\begin{document}

\maketitle
\tableofcontents

\section{Numerics ideas}

What's going on in the kglasso stuff? They are generating an erdos renyi and making sure that the least eigenvalue is $1$, then trying to approximate each. I wonder how it'd do without scaling the eigenvalues into oblivion?

What's easiest? Convert to matlab or nah?



\section{Tensor Scaling}
We will maintain similar notation. We have $n$ samples of $X \sim \mathcal{N}(0,\frac{1}{n} \otimes_{a} \frac{1}{d_{a}} I_{a})$ with $D := \prod_{a} d_{a}$. We don't have a KLR style analysis at the moment, but strong convexity is enough by the FM analysis, and this can be proven by just controlling each bipartite piece. So the operator scaling analysis does give us very good bounds for $\|\mu\|_{op}$ and expansion with $nD \gg \max_{a} d_{a}^{2} \log^{c}(D)$. These bounds are not enough though, so in this section we will follow the FM analysis to give the requirements, then show the required strong convexity, and show how to maintain this under perturbation.

\subsection{FM Analysis}
Recall that $\forall a: Q^{a} \approx \frac{1}{d_{a}} I_{a}$, so if we can show $\forall a \neq b: \langle Q^{ab}, Z \otimes Y \rangle \lesssim \frac{\|Z\|_{F} \|Y\|_{F}}{\sqrt{d_{a} d_{b}}}$ then we have strong convexity with $\langle Z, \nabla^{2} Z \rangle \gtrsim \sum_{a} \frac{\|Z_{a}\|_{F}^{2}}{d_{a}}$, i.e. the Hessian is strongly diagonally dominant. We will derive our requirements on strong convexity, perturbation bounds, and initial error. Assume we have the following strong convexity
\[ \forall Z: \sum_{a} \frac{\|Z_{a}\|_{F}^{2}}{d_{a}} \leq \kappa^{2}: \langle Y, \nabla^{2}_{e^{Z}}, Y \rangle \geq \lambda \sum_{a} \frac{\|Y_{a}\|_{F}^{2}}{d_{a}}  \]

Choose $X$ to be the geodesic towards the optimum and $g(t) := f(e^{tZ})$ with the opt at $t=1$:
\[ g'(1) = \int_{0}^{1} g''(t) + g'(0) \geq \lambda \sum_{a} \frac{\|Z\|_{F}^{2}}{d_{a}} - |\langle \nabla_{a}, Z_{a} \rangle| \]
\[ \geq \sum_{a} \frac{\|Z_{a}\|_{F}}{\sqrt{d_{a}}} \left( \lambda \frac{\|Z_{a}\|_{F}}{\sqrt{d_{a}}} - \sqrt{d_{a}} \|\nabla_{a}\|_{F}  \right)  \]
\[ \geq \sqrt{\sum_{a} \frac{\|Z_{a}\|_{F}^{2}}{d_{a}}} \left( \lambda \sqrt{\sum_{a} \frac{\|Z_{a}\|_{F}^{2}}{d_{a}}} - \sqrt{\sum_{a} d_{a} \|\nabla_{a}\|_{F}^{2}} \right)  \]
This is $> 0$ if $\forall a: \lambda > d_{a} \frac{\|\nabla_{a}\|_{F}}{\|Z_{a}\|_{F}}$ or $\lambda^{2} > \frac{ \sum_{a} d_{a} \|\nabla_{a}\|_{F}^{2} }{\sum_{a} d_{a}^{-1} \|Z_{a}\|_{F}^{2}} $.

Since standard perturbation bounds ($e^{Z} \approx I + Z$) only work for small $Z$, we will require
\[ \forall a: \|\nabla_{a}\|_{F} \ll \frac{1}{d_{a}} \]
\[ \forall a: \|Z_{a}\|_{F}^{2} \ll 1 \implies \langle Y, \nabla_{e^{Z}}^{2}, Y \rangle \geq \Omega(1) \sum_{a} \frac{\|Y_{a}\|_{F}^{2}}{d_{a}}   \]


\subsection{Moment Map}
For $g \sim \frac{1}{nD} \otimes_{a} I_{a}$, we want to bound $\|Q^{a} - sI_{a}\|_{F}$ using a net:
\[ \E \langle \sum_{t} g_{t} g_{t}^{*}, X_{a} \rangle = \sum_{i} x_{i} \chi(\frac{1}{d_{a}}, \frac{TD}{d_{a}}) = \langle \frac{1}{d_{a}} I_{a}, X \rangle = 0 \]
\[ \log \E \exp \theta \langle \sum_{t} g_{t} g_{t}^{*}, X_{a} \rangle = \log \E \exp \theta \sum_{i} x_{i} \chi(\frac{1}{d_{a}}, \frac{TD}{d_{a}})  \]
\[ = \sum_{i} \frac{-TD}{2 d_{a}} \log \left( 1 - 2 \theta \frac{x_{i}}{TD} \right)   \]
\[ \lesssim \theta^{2} \frac{\|X\|_{F}^{2}}{2 d_{a} TD} \hspace{10mm} \forall \theta < \left( \frac{\|X\|_{op}}{TD} \right)^{-1}  \]
\[ \implies \Pr[ \langle \sum_{t} g_{t} g_{t}^{*}, X_{a} \rangle \geq \epsilon \|X\|_{F} ] \leq
\begin{cases}
\exp( - \Omega(\epsilon^{2} TD d_{a}) ) & \epsilon  < \frac{\|X\|_{F}}{d_{a} \|X\|_{op}}
\\ \exp ( - \Omega(\epsilon TD) \frac{\|X\|_{F}}{\|X\|_{op}} ) & \epsilon \geq \frac{\|X\|_{F}}{d_{a} \|X\|_{op}}
\end{cases}
\]
We will need the following settings of $\epsilon$ in future:
\[ \epsilon \approx \frac{1}{\sqrt{d_{a}}} \implies \Pr [ d_{a} \|Q^{a} - sI_{a}\|_{F}^{2} \gtrsim c ] \leq \exp(d_{a}^{2} \log d_{a} - c \frac{TD}{\sqrt{d_{a}}})  \]
For which we need $TD \gtrsim \max_{a} d_{a}^{5/2} \log d_{a}$.
\[ \epsilon \approx \frac{1}{d_{a}} \implies \Pr [ d_{a} \|Q^{a} - sI_{a}\|_{F}^{2} \gtrsim \frac{c}{d_{a}} ] \leq \exp(d_{a}^{2} \log d_{a} - c^{2} \frac{TD}{d_{a}})   \]
For which we need $TD \gtrsim \max_{a} d_{a}^{3} \log d_{a}$.

\begin{remark}
Note we lose out on the subgaussian part of the bound only when $\frac{d_{a} \|X\|_{op}^{2}}{\|X\|_{F}^{2}}$ is large. It is quite possible that for our setting, we can bound e.g. the condition number or stable rank of is small w.h.p. In particular if we can show the only relevant part of the net has $s \|X\|_{F}^{2} \geq d \|X\|_{op}$ then we only incur a $\sqrt{s}$ loss in required samples.
\end{remark}

Actually there seems to be a simpler way to prove these statements using the $\|\cdot\|_{op}$ bounds derived earlier.
\[ \|Q^{a} - sI_{a}\|_{op} \leq \frac{\sqrt{f(d)}}{d} \implies d_{a} \|Q^{a} - sI_{a}\|_{F}^{2} \leq f(d)  \]
So this means in the first case, we need $TD \gtrsim \max_{a} d_{a}^{2} \log D$ and the second case we need $TD \gtrsim \max_{a} d_{a}^{3} \log D$. But by this analysis we only get $1/poly$ failure probability.


\subsection{Net proof of Expansion}
Recall again we have $Q := \sum_{i \in [n]} X_{i} X_{i}^{*}$ with i.i.d $X \sim \mathcal{N}(0,\frac{1}{n} \otimes_{a} \frac{1}{d_{a}} I_{a})$. For symmetric test matrices $Z,Y$ with eigenvalues $\{z_{i}\},\{y_{j}\}$ respectively:
\[ \E \langle Q, Z_{a} \otimes Y_{b} \rangle = \sum_{ij} z_{i} y_{j} \chi(\frac{1}{d_{a} d_{b}}, \frac{nD}{d_{a} d_{b}}) = \langle \frac{1}{d_{a}} I_{a}, Z \rangle \langle \frac{1}{d_{b}} I_{b}, Y \rangle = 0 \]

\[ \log \E \exp \theta \langle Q, Z_{a} \otimes Y_{b} \rangle = \log \E \exp \theta \sum_{ij} z_{i} y_{j} \chi(\frac{1}{d_{a} d_{b}}, \frac{nD}{d_{a} d_{b}})  \]
\[ = \sum_{ij} \frac{-nD}{2 d_{a} d_{b}} \log \left( 1 - 2 \theta \frac{z_{i} y_{j}}{nD} \right)   \]
\[ \lesssim \theta^{2} \frac{\|Z\|_{F}^{2} \|Y\|_{F}^{2}}{2 d_{a} d_{b} nD} \hspace{10mm} \forall \theta < \left( \frac{\|Z\|_{op} \|Y\|_{op}}{TD} \right)^{-1}  \]
\[ \Pr[ \langle Q, X \otimes Y \rangle \geq \lambda \frac{\|Z\|_{F} \|Y\|_{F}}{\sqrt{d_{a} d_{b}}} ] \leq
\begin{cases}
\exp( - \lambda^{2} nD ) \hspace{7mm} \lambda  < \frac{\|Z\|_{F} \|Y\|_{F}}{\sqrt{d_{a} d_{b}} \|Z\|_{op} \|Y\|_{op}}
\\ \exp ( - \lambda nD \frac{\|Z\|_{F} \|Y\|_{F}}{ \sqrt{d_{a} d_{b}} \|Z\|_{op} \|Y\|_{op}} )
\end{cases}
\]
So for $\lambda \approx \frac{1}{k}$ we need $nD \gtrsim \max_{a} d_{a}^{3} \log D > \max_{a,b} \sqrt{d_{a} d_{b}} (d_{a}^{2} + d_{b}^{2}) \log D$

\begin{remark}
Note again we lose out when $\frac{d \|X\|_{op}^{2}}{\|X\|_{F}^{2}}$ is large. So we would like to show that w.h.p. the singular vectors of our bipartite operator have e.g. small condition number or large stable rank. Again if we can show the relevant part of the net has $s \|X\|_{F}^{2} \geq d \|X\|_{op}^{2}$ then we incur an $s$ factor loss in samples. This is reminiscent of the fact that eigenvectors for random matrices have many delocalization properties, so will look into that.
\end{remark}

\subsection{Delocalization}
I have a few claims which, if true, would give another proof (along with Pisier's) of constant expansion at the start with $1/poly$ failure probability. Unfortunately to make this robust we would either need a robust form of delocalization, which I think is false in general, or exponential failure probability, which again may be false in general.

Recall $M^{ab}$ is the operator defined by the off-diagonal block of the Hessian. For every choice of bases (torus) $U,V$ we have a matrix $M^{ab}_{UV}$ where each entry is $\chi(\frac{1}{d_{a}d_{b}}, \frac{TD}{d_{a}d_{b}})$. In particular each entry is of exponential type.

\begin{claim}
Let $U,V$ be the (random) basis for the optimizers of $\|M\|_{op}$. Then $(U,V)$ and $M_{UV}$ are distributionally independent.
\end{claim}

\begin{theorem} [Informal]
For $M \in \R^{n \times n}$ populated by iid variables $X$ such that $\E X = 0, \E X^{2} = 1$ and $X$ is of exponential type, we have delocalization of eigenvectors with $1/poly$ failure probability:
\[ v \in S^{n-1}, \|v\|_{\infty} \gtrsim \frac{\log^{c} n}{\sqrt{n}} \implies \forall \lambda \in \C: \|Mv - \lambda v\|_{2} \gtrsim \frac{1}{\sqrt{n}}   \]
\end{theorem}

%https://arxiv.org/pdf/1306.2887.pdf
%https://arxiv.org/pdf/1306.3099.pdf
%https://arxiv.org/pdf/1601.03678.pdf

\begin{claim}
The above informal claim can be extended to singular vectors.
\end{claim}

\begin{corollary}
Conditioned on the above delocalization event, our net only has to cover $\{X \mid d \|X\|_{op}^{2} \lesssim \log^{c}(d) \|X\|_{F}^{2}$, and so we only require $TD \gtrsim d_{a}^{2} \log^{c} D$.
\end{corollary}


\subsection{Robust proof of Expansion}
In this section we will show expansion under perturbations of the form $\otimes_{a} e^{\delta_{a}}$. Note if $\|\delta\|_{op} \ll 1$ then we can approximate $e^{\delta} - I \approx O(\delta)$.
\[ \mu := \E \langle Q, e^{\delta_{a}} Z e^{\delta_{a}} \otimes e^{\delta_{b}} Y e^{\delta_{b}} \otimes_{c} e^{2\delta_{c}} \rangle = \langle \frac{e^{2\delta_{a}}}{d_{a}} , Z \rangle \langle \frac{e^{2\delta_{b}}}{d_{b}}, Y \rangle \prod_{c \neq a,b} Tr[\frac{e^{2\delta_{c}}}{d_{c}} ]  \]
\[ \log \E \exp \theta \langle Q, e^{\delta_{a}} Z e^{\delta_{a}} \otimes e^{\delta_{b}} Y e^{\delta_{b}} \otimes_{c \neq a,b} e^{2\delta_{c}} \rangle = \log \E \exp \theta \sum_{\vec{i}} x^{\vec{i}} \chi(\frac{1}{D},T)   \]
\[ \leq \theta \mu + \theta^{2} \frac{\|e^{\delta_{a}} Z e^{\delta_{a}}\|_{F}^{2} \|e^{\delta_{b}} Y e^{\delta_{b}}\|_{F}^{2} \prod_{c} \|e^{2\delta_{c}}\|_{F}^{2}}{2 TD^{2}} \]
\[ \forall \theta < \left( \frac{\|e^{\delta_{a}} Z e^{\delta_{a}}\|_{op} \|e^{\delta_{b}} Y e^{\delta_{b}}\|_{op} \prod_{c} \|e^{2\delta_{c}}\|_{op}}{TD} \right)^{-1}    \]

\begin{lemma}
For $\|\delta\|_{op} \ll 1$
\[ \frac{1}{d} Tr[e^{2\delta}] \leq 1 + O(\|\delta\|_{op})  \]
\[ \langle \frac{1}{d} I, e^{\delta} Z e^{\delta} - Z \rangle \leq \frac{O(\|\delta\|_{op})}{\sqrt{d}} \|Z\|_{F}   \]
\[ \|e^{\delta} Z e^{\delta}\|_{F}^{2} \leq (1 + O(\|\delta\|_{op})) \|Z\|_{F}^{2}   \]
\end{lemma}
\begin{proof}
Let $\delta := e^{\delta'} - I$ and note $\|\delta\|_{op} \approx \|\delta'\|_{op}$ for small $\delta'$. So we bound
\[ Tr[e^{2\delta'} - I] = \langle I, e^{2\delta'} - I \rangle \leq d \|2\delta + \delta^{2}\|_{op} \leq d O(\|\delta\|_{op})   \]
For the second line we use Cauchy Schwarz $\|Z\|_{1} \leq \sqrt{d} \|Z\|_{F}$
\[ \langle I, (I + \delta) Z (I + \delta') - Z \rangle = \langle 2 \delta + \delta^{2}, Z \rangle \leq O(\|\delta\|_{op}) \|X\|_{1} \leq O(\|\delta\|_{op}) \sqrt{d} \|Z\|_{F}  \]
The second line is similar but simpler using $\|AB\|_{F} \leq \|A\|_{op} \|B\|_{F}$
\end{proof}

So using the above lemma, we have bounds for $\langle Z, I_{a} \rangle = \langle Y, I_{b} \rangle = 0$:
\[ \mu \leq \frac{\|Z\|_{F} \|Y\|_{F}}{\sqrt{d_{a} d_{b}}} O(\|\delta_{a}\|_{op} \|\delta_{b}\|_{op}) \prod_{c} (1 + O(\|\delta_{c}\|_{op})  \]
\[ \log \E \exp \theta (... - \mu) \leq \theta^{2} \frac{\|Z\|_{F}^{2} \|Y\|_{F}^{2}}{2TD} \prod_{c} (1 + O(\|\delta_{c}\|_{op}))  \]
\[ \forall \theta < \left( \frac{\|Z\|_{op}\|Y\|_{op}}{TD} \right)^{-1} \prod_{c} (1 - O(\|\delta_{c}\|_{op}))  \]
So basically, as long as $\|\delta\|_{op} \ll 1$ everything is of the same order as in the unperturbed case, and therefore if we run a net on all parts simultaneously (of size $\exp(\sum_{a} d_{a}^{2})$) we get roughly the same probabilistic bounds as the start.

This creates a bottleneck though as in general the inequality $\|X\|_{op} \leq \|X\|_{F}$ could be tight, so in order to guarantee a small enough perturbation we can only move in the ball $\|X\|_{F} \ll 1$. This is why we wrote out the conditions required for $\|\nabla_{a}\|_{F} \ll \frac{1}{d_{a}}$, as therefore we can assume $\|Z_{a}\|_{F} \ll 1$ and require $\lambda \approx 1$.

\begin{remark}
Here is where we definitely would like a KLR style analysis to exploit the fact that we actually have robustness in $\|\cdot\|_{op}$.
\end{remark}


\subsection{Deterministic Robust Proof of Expansion}

We follow Michael's write-up in our setting. We can assume we have sufficiently strong expansion initially and we would like to show that any scaling ($e^{Y} \cdot g$ with $\forall a: \|Y_{a}\|_{F} \ll 1$) only changes the Hessian by a small amount.
\begin{fact}
Recall the formula for Hessian at $v$:
\[ \langle X, \nabla_{v}^{2}, X \rangle = \langle \frac{v v^{*}}{\|v\|_{2}^{2}}, \left( \sum_{a} X_{a} \otimes I_{\overline{a}}  \right)^{2} \rangle - \left( \sum_{a} \langle \frac{v v^{*}}{\|v\|_{2}^{2}}, X_{a} \otimes I_{\overline{a}} \rangle  \right)^{2}  \]
\end{fact}

Initially, we have expansion of the form
\[ \langle X, \nabla^{2}, X \rangle \geq (1-\epsilon) \sum_{a} \frac{\|X_{a}\|_{F}^{2}}{d_{a}} - \lambda \sum_{a \neq b} \frac{\|X_{a}\|_{F} \|X_{b}\|_{F}}{\sqrt{d_{a} d_{b}}} \geq (1-\epsilon') \sum_{a} \frac{\|X_{a}\|_{F}^{2}}{d_{a}}   \]
So for how large of a perturbation can we show that every quadratic form changes $<< \sum_{a} \frac{\|X_{a}\|_{F}^{2}}{d_{a}} =: \|X\|^{2}$?

\begin{lemma}
\[ \|\sum_{a} X_{a} \otimes I_{\overline{a}}\|_{op} \leq \sum_{a} \|X_{a}\|_{op} \leq \sum_{a} \|X_{a}\|_{F} \leq \sqrt{ \left( \sum_{a} d_{a} \right) \left( \sum_{a} \frac{\|X_{a}\|_{F}^{2}}{d_{a}}  \right)} \]
All inequalities can be tight, say at $X_{a} = \sqrt{d_{a}} E_{11}$. In Michael's notation, for standard tensor rep
\[ \sup_{X} \frac{\|\Pi(X)\|_{op}}{\sum_{a} \|X_{a}\|_{F}} = k  \text{  ;  } \hspace{7mm}
\sup_{X} \frac{\|\Pi(X)\|_{op}^{2}}{\|X\|^{2}} = \sum_{a} d_{a}  \]
\end{lemma}

\begin{theorem}
For $\|v\|_{2} = 1$ and $w := e^{Y} \cdot v / \|e^{Y} \cdot v\|_{2}$ with perturbation $\|\sum_{a} Y_{a} \otimes I_{\overline{a}}\|_{op} = \|\Pi(Y)\|_{op} \ll 1$:
\[ |\langle X, \nabla_{w}^{2}, X \rangle - \langle X, \nabla_{v}^{2}, X \rangle| \leq O(\sum_{a} d_{a}) \|X\|^{2} \|\Pi(Y)\|_{op}   \]
\end{theorem}
\begin{proof}
Assume wlog $\sum_{a} \frac{\|X_{a}\|_{F}^{2}}{d_{a}} = 1$:
\[ |\langle X, \nabla_{v}^{2}, X \rangle - \langle X, \nabla_{w}^{2}, X \rangle| \]
\[ \leq |\langle w w^{*} - v v^{*}, \left( \sum_{a} X_{a} \right)^{2} \rangle| + |\langle w w^{*}, \sum_{a} X_{a} \rangle^{2} - \langle v v^{*}, \sum_{a} X_{a} \rangle^{2}| \]
\[ = |\langle w w^{*} - v v^{*}, \Pi(X)^{2} \rangle| + |\langle w w^{*} + v v^{*}, \Pi(X) \rangle \langle w w^{*} - v v^{*}, \Pi(X) \rangle|    \]
\[ \leq \|w w^{*} - v v^{*}\|_{1} \|\Pi(X)\|_{op}^{2} + 2 \|\Pi(X)\|_{op} \|w w^{*} - v v^{*}\|_{1} \|\Pi(X)\|_{op}  \]
\[ = 3 (\sum_{a} d_{a}) \|w w - v v^{*}\|_{1}   \]
Now we just have to bound perturbations of rank one matrices:
\[ \|w w^{*} - v v^{*}\|_{1} \leq \sqrt{2} \|w w^{*} - v v^{*}\|_{F} \leq 2 \sqrt{2} \|w - v\|_{2}  \]
Finally we can just bound the action of $Y$:
\[ \|w - v\|_{2} \leq \|\frac{e^{Y} \cdot v}{\|e^{Y} \cdot v\|_{2}} - e^Y \cdot v\|_{2} + \|e^{Y} \cdot v - v\|_{2}  \]
\[ \leq |1 - \|e^{Y} \cdot v\|_{2}| \|w\|_{2} + \|e^{Y} - I\|_{op} \|v\|_{2} \leq 2 \|e^{Y} - I\|_{op} \leq O(1) \|\Pi(Y)\|_{op} \]
\end{proof}

\begin{claim}
For unit vectors $v,w: \|v v^{*} - w w^{*}\|_{F}^{2} \leq 2 \|v - w\|_{2}^{2}$
\end{claim}
\begin{proof}
Assume wlog $v = e_{1}, w = (x,y) \in \R^{2}$ and note
\[ v v^{*} - w w^{*} = \begin{pmatrix} 1 - x^{2} & -xy \\ -xy & -y^{2} \end{pmatrix}  \]
has $Tr = 1 - x^{2} - y^{2} = 0$ and $\det = (1-x^{2})(-y^{2}) - x^{2} y^{2} = - y^{2}$; so the eigenvalues are $\pm y$.
\[ \implies \|v v^{*} - w w^{*}\|_{F}^{2} = 2 y^{2} = 2(1-x^{2}) \]
\[ \|v - w\|_{2}^{2} = (1-x)^{2} + y^{2} = 2(1-x) \]
Finally note $1 - x^{2}$ is concave and $2(1-x)$ is a tangent line at $x=1$.
\end{proof}

Recall the sufficient condition for $\lambda = \Omega(1)$-expansion
\[ \sum_{a} d_{a} \|\nabla_{a}\|_{F}^{2} \lesssim \sum_{a} d_{a}^{-1} \|Y_{a}\|_{F}^{2}  \]
So we require $\langle X, \nabla_{e^{Y}}, X \rangle \gtrsim \|X\|^{2}$. We can get this bound for all $\|\Pi(Y)\|_{op} \ll (\sum_{a} d_{a})^{-1}$. Therefore we can get a proper perturbation bound for $\forall a: \|Y_{a}\|_{F} \ll (\sum_{a} d_{a})^{-1}$ which implies $\|Y\|^{2} \ll (\max_{a} d_{a})^{-3}$, so we require $\forall a: \|\nabla_{a}\|_{F} \ll (\max_{a} d_{a})^{-2}$. This is worse than the requirement from the larger net argument above.

\CF{cole agrees - needs $D > d_a^5!$}

\subsection{Better Deterministic Robustness}
\begin{lemma}\label{lem:block-perturbation-appendix}
For perturbation $v \to \otimes_{a} e^{\delta_{a}} \cdot v =: w$ where $\forall a: \|\delta_{a}\|_{op} \ll 1$, and let $\{\sigma_{1}^{ab}, \sigma_{2}^{ab}\}$ be the matrix $\|\cdot\|_{F} \to \|\cdot\|_{F}$ norm and matrix norm on subspace $\perp$ to $(I,I)$ for each bipartite part respectively:
\[ \forall a,b: \sigma_{2}^{ab}(w w^{*}) - \sigma_{2}^{ab}(v v^{*}) \leq O \left( \sum_{a} \|\delta_{a}\|_{op}  \right) \sigma_{1}^{ab}(v v^{*})   \]
The same is true for the diagonal blocks.
\end{lemma}
\begin{proof}
To lower bound the diagonal block, we just need a spectral lower bound on $\{Q_{a}\}$, since $\langle vec(X), M^{a} (vec(X)) \rangle := \langle Q_{a}, X^{2} \rangle$.
\[ \| e^{\delta_{a}} Q_{a} e^{\delta_{a}} - Q_{a}\|_{op} = \leq O(\|\delta_{a}\|_{op}) \|Q_{a}\|_{op}   \]
Now we address a perturbation on $b \neq a$. For a spectral lower bound, we choose test $Z \succeq 0$ and let $\delta := e^{2\delta_{b}} - I$:
\[ \langle e^{\delta_{b}} v v^{*} e^{\delta_{b}} - v v^{*}, I_{\overline{a}} \otimes Z_{a} \rangle
= \langle v v^{*}, \delta \otimes Z \rangle = \langle Z, V^{*} \delta V \rangle   \]
Here $V \in \R^{d_{b} \times d_{a}}$ is the matricized version of $v$. But now since $Z \succeq 0$, the argument is clear
\[ \leq \langle Z, V^{*} |\delta| V \rangle \leq \|\delta\|_{op} \langle Z, V^{*} I V \rangle = \|\delta\|_{op} \langle v v^{*}, I_{\overline{a}} \otimes Z \rangle     \]

The argument for the off-diagonal blocks is similar. We first argue the change of $\sigma^{ab}$ is small under perturbations where $\forall c \neq a,b: \delta_{c} = 0$. Let $M^{ab}$ be the matrix versions of the bipartite operators:
\[ \langle vec(Y), M_{v}^{ab}(vec(Z)) \rangle := \langle v v^{*}, I_{\overline{ab}} \otimes Z \otimes Y \rangle \]
\[ \langle vec(Y), M_{w}^{ab}(vec(Z)) \rangle := \langle w w^{*}, I_{\overline{ab}} \otimes Z \otimes Y \rangle\]

\[ \implies M_{w} = (e^{\delta_{b}} \otimes e^{\delta_{b}}) M_{v} (e^{\delta_{a}} \otimes e^{\delta_{a}})   \]
\[ \implies \|M_{w} - M_{v}\|_{op} \leq O(\|\delta_{a}\|_{op} + \|\delta_{b}\|_{op}) \|M_{v}\|_{op}   \]
where in the last step we used $\delta \ll 1$.

The more difficult part of the argument to see (at least for me) is how $\sigma^{ab}$ changes if some other part $c \neq a,b$ is changed, i.e. $\forall d \neq c: \delta_{d} = 0$. First we define $\delta := e^{2 \delta_{c}} - I$, and test vectors $Z,Y$:
\[ \langle w w^{*} - v v^{*}, I_{\overline{ab}} \otimes Z \otimes Y \rangle = \langle v v^{*}, \delta \otimes Z \otimes Y \rangle  = \langle Z \otimes Y, V^{*} \delta V \rangle \]
Here $V \in \R^{d_{c} \times d_{a}d_{b}}$ is the matricized version of $v$, i.e. the $k$-th element of $ij$-th column is $(V_{ij})_{k} := v_{ijk}$. Now in order to use our operator norm bounds, we need to deal with cancelations, so we split into positive and negative parts $Z := Z_{+} - Z_{-}, Y := Y_{+} - Y_{-}$:
\[ |\langle Z \otimes Y, V^{*} \delta V \rangle| \leq |\langle Z_{\pm} \otimes Y_{\pm}, V^{*} \delta V \rangle |  \]
Now we analyze one of these terms (by abuse of notation $Z, Y \succ 0$):
\[ \leq \langle Z \otimes Y, V^{*} |\delta| V \rangle \leq \|\delta\|_{op} \langle Z \otimes Y, V^{*} V \rangle = \|\delta\|_{op} \langle v v^{*}, I_{\overline{ab}} \otimes Z \otimes Y \rangle   \]
Each of these terms we can bound by $\sigma^{ab}_{1} \|Z\|_{F} \|Y\|_{F}$. (Note we can save a $2$-factor on these four terms since they are decompositions of $Z,Y$). So by iterating this argument over all $c$, we get the desired bound.
\end{proof}

\begin{remark}
Instead of $F$, we could use any pair of dual norms and get the same result. In particular, we will use these results to bound the $1 \to 1$ and $op \to op$ norms of the channels. Explicitly, if we redefine $\sigma := \|\Phi\|_{p \to p}$ and $(p,q)$ are Holder conjuguates:
\[ \langle v v^{*}, \delta \otimes Z \otimes Y \rangle \leq \|\delta\|_{op} \langle v v^{*}, I_{\overline{ab}} \otimes Z \otimes Y \rangle \leq \sigma \|\delta\|_{op} \|Z_{\pm}\|_{p} \|Y_{\pm}\|_{q} \]
\[ \leq \sigma \|\delta\|_{op} (2\|Z_{+}\|_{p}^{p} + 2\|Z_{-}\|_{p}^{p})^{1/p} (2\|Y_{+}\|_{q}^{q} + 2\|Y_{-}\|_{q}^{q})^{1/q} = 2 \sigma \|\delta\|_{op} \|Z\|_{p} \|Y\|_{q}     \]
\end{remark}

\subsection{Conclusion}
At the end of the day we require $nD \gtrsim \max_{a} d_{a}^{3} \log D$ to get small enough $\nabla$ and robust expansion. Note this is $d \log D$ away from the existence/ uniqueness threshold for the solution.

\subsection{Ideas for KLR style analysis}
\begin{enumerate}
    \item The delocalization type arguments may be helpful since we know the movement is spread out.
    \item Follow gradient flow and show we have strong convergence for $\Omega(\log \max_{a} d_{a})$ time, after which the operator norm of the gradient can never be bigger than the initial one. Therefore if we have the first point, it is enough to cover the $O(1)$-size operator norm ball.
    \item Even if we get that perturbations come from $O(1)$-size operator norm ball, without some improved delocalization (or something) we would still need an extra $d$ factor.
    \item One important lemma in KLR is that the operator norm of moment map is roughly monotone decreasing over all time (where the fudge is of the order of the change in objective function $\|v\|_{2}^{2}$ over time). This is not true for $k=3$. But we have some extra structure e.g.: the error of a row and column are positively correlated.
    \item Let $\epsilon(t) := \max_{a} d_{a} \|\nabla_{a}(t)\|_{op}$. Maybe we can show
    \[ T_{\epsilon} := \inf \{ t \mid \epsilon(t) > 100 t \epsilon(0) \}  \]
    is large enough. In particular by the argument above $T_{\epsilon}$ cannot be larger than $\log d$. Maybe there's some contradiction whereby we can always increase $T_{\epsilon}$.
\end{enumerate}

Recall the gradient flow is
\[ \partial_{t} v(t) = \sum_{a} I_{\overline{a}} \otimes X_{a} \cdot v(t)   \]
where $X_{a} := d_{a} \mu_{a}(t)$. Then we have
\[ \partial_{t} \sum_{a} d_{a} \|\mu_{a}(t)\|_{F}^{2} = \sum_{a} d_{a}^{2} \langle Q^{a}, \mu_{a}^{2} \rangle + \sum_{a \neq b} d_{a} d_{b} \langle Q^{ab}, \mu_{a} \otimes \mu_{b} \rangle  \]
\[ \geq (1-\epsilon) \sum_{a} d_{a}^{2} \frac{\|\mu_{a}\|_{F}^{2}}{d_{a}} - \lambda \sum_{a \neq b} d_{a} d_{b} \frac{\|\mu_{a}\|_{F} \|\mu_{b}\|_{F}}{\sqrt{d_{a} d_{b}}}   \]
\[ \geq (1-\epsilon') \sum_{a} d_{a} \|\mu_{a}\|_{F}^{2}   \]


\subsection{Incoherence}

\begin{definition}
For two orthonormal bases $\{u_{i}\},\{v_{i}\} \subseteq \R^{d}$, the coherence between them is
\[ \gamma(U,V) := \sqrt{d} \max_{i,j} |\langle u_{i}, v_{j} \rangle|  \]
\end{definition}

Let $(\epsilon/d_{a},u)$ be the eigenpair associated with operator norm of $\mu_{a}$ and recall that under gradient flow we move in direction $X := -\sum_{a} e_{a} \otimes d_{a} \mu_{a}\}$
\[ -\partial_{t=0} \|\mu_{a}\|_{op} = -\langle e_{a} \otimes uu^{*}, \nabla^{2} X \rangle     \]
\[ = \langle \rho^{\{a\}}, d_{a} \mu_{a} u u^{*} \rangle + \sum_{b \neq a} \langle \rho^{\{a,b\}}, u u^{*} \otimes d_{b} \mu_{b} \rangle    \]
\[ = \frac{1}{d_{a}} (1+\eps) d_{a} \frac{1}{d_{a}} \eps + \sum_{b \neq a} d_{b} \langle \rho^{\{a,b\}}, u u^{*} \otimes \mu_{b} \rangle   \]

Let's focus on a single $(a,b)$ term, and denote $T : L(\R^{d_{b}}) \to L(\R^{d_{a}})$ as
\[ \langle T(Y_{b}), X_{a} \rangle := \langle \rho^{\{a,b\}}, X_{a} \otimes Y_{b} \rangle \]
So the term we are analyzing is $\langle T(\mu_{b}), u u^{*} \rangle$, which can be viewed as a diagonal of the matrix $T(\mu_{b})$. We have good bounds on this matrix:
\[ \langle I_{a}, T(\mu_{b}) \rangle = \langle \rho^{\{a,b\}}, I_{a} \otimes \mu_{b} \rangle = \|\mu_{b}\|_{F}^{2}  \]
Now define $Y := T(\mu_{b}) - \frac{\|\mu_{b}\|_{F}^{2}}{d_{a}} I_{a}$ to be projection orthogonal to the identity:
\[ \|Y\|_{F} = \sup_{X} \frac{ \langle Y,X \rangle }{\|X\|_{F}} = \sup_{X \perp I} \frac{ \langle Y, X \rangle }{\|X\|_{F}} = \sup_{X \perp I, \|X\|_{F} = 1} \langle \rho^{\{a,b\}}, X \otimes \mu_{b} \rangle \leq \frac{\lambda}{\sqrt{d_{a} d_{b}}} \|\mu_{b}\|_{F}     \]
Now if $Y = \sum_{i} y_{i} v_{i} v_{i}^{*}$ is the eigendecomposition, we have a bound on the variance of the eigenvalues. Let $\mu_{a} = \sum_{i} x_{i} u_{i} u_{i}^{*}$, and if we have a bound on the coherence of $U,V$, we can bound the competing term:
\[ d_{b} \langle \rho^{\{a,b\}}, u u^{*} \otimes \mu_{b} \rangle = d_{b} \langle \rho^{\{a,b\}}, \frac{1}{d_{a}} I_{a} \otimes \mu_{b} + (u u^{*} - \frac{1}{d_{a}} I_{a}) \otimes \mu_{b} \rangle    \]
\[ = \frac{d_{b} \|\mu_{b}\|_{F}^{2}}{d_{a}} + d_{b} \langle Y, u u^{*} \rangle
= \frac{d_{b} \|\mu_{b}\|_{F}^{2}}{d_{a}} + d_{b} \sum_{i} y_{i} \langle u, v_{i} \rangle^{2}    \]
\[ \leq \frac{d_{b} \|\mu_{b}\|_{F}^{2}}{d_{a}} + d_{b} \left( \sum_{i} y_{i}^{2} \langle u, v_{i} \rangle^{2}  \right)^{1/2} \left( \sum_{i} \langle u, v_{i} \rangle^{2} \right)^{1/2}   \]
\[ \leq \frac{d_{b} \|\mu_{b}\|_{F}^{2}}{d_{a}} + d_{b} \frac{\gamma(U,V)}{\sqrt{d_{a}}} \|Y\|_{F}
\leq \frac{d_{b} \|\mu_{b}\|_{F}^{2}}{d_{a}} + \lambda \gamma \frac{\sqrt{d_{b}} \|\mu_{b}\|_{F}}{d_{a}}    \]

So adding up all these terms, we get a competing force of
\[ \sum_{b \neq a} \frac{d_{b}}{d_{a}} \|\mu_{b}\|_{F}^{2} + \frac{\lambda \gamma}{d_{a}}  \sqrt{d_{b}} \|\mu_{b}\|_{F} < \frac{1}{d_{a}} \left[ \|\mu\|_{*}^{2} + \lambda \gamma \sqrt{k} \|\mu\|_{*}   \right]   \]
Now the $a$-th part is pushing $\approx \eps_{a}/d_{a} = \|\mu_{a}\|_{op}$. For comparison, if $\epsilon_{b} := d_{b} \|\mu_{b}\|_{op}$, then we have the bound
\[ \|\mu\|_{*}^{2} = \sum_{b} d_{b} \|\mu_{b}\|_{F}^{2} \leq \sum_{b} d_{b}^{2} \|\mu_{b}\|_{op}^{2} = \sum_{b} \epsilon_{b}^{2}   \]
So the above says if $\lambda \gamma \ll 1/k$, any eigenvalue that is on the same order as the average will be shrinking in magnitude.

\begin{fact}
Random unitaries are very incoherent:
\[ \max_{ij} \langle u_{i}, e_{j} \rangle^{2} \leq \frac{\log d}{d}    \]
with $1/poly(d)$ failure probability.
\end{fact}

\begin{claim}
The eigenbases of $\{\mu_{a}\}, \{T_{ba}(\mu_{b})\}$ are roughly as incoherent as random unitaries. This is true even after running gradient flow for some time.
\end{claim}

The problem remaining is that eigenvectors are not continuous objects, so a bound on incoherence at various discrete times does not imply a bound for all continuous times.

%\subsection{Separability}

%If the state $\rho$ is separable,





\subsection{Experiments}
\begin{enumerate}
    \item Check $\|\mu(t)\|_{op}$ is (close to) monotone decreasing
    \item Check $\int_{0}^{T} \|\mu(t)\|_{op} \gg \|\int_{0}^{T} \mu(t)\|_{op}$. I'm assumming this last term is roughly the operator norm of the scaling, but can check that too: what is the operator norm of log of the scaling at time $T$?
    \item Plot $d \|\mu(t)\|_{op}^{2} / \|\mu(t)\|_{F}^{2}$. Should be $\Tilde{O}(1)$ for some amount of time.
\end{enumerate}


\subsection{Operator Norm Monotonicity}

Our quantum setting is that we have $\{X_{1}, ..., X_{n}\}$ gaussian samples $X \sim \cN(0, \frac{1}{n} \otimes_{a} \frac{1}{d_{a}} I_{a})$ and we want to show the optimizers of the log-Likelihood function is "nearby". In the classical setting we have $k$-tensor with entries from the $\chi$-square distribution with mean $\frac{1}{nD}$ and $n$ degrees of freedom: $\chi(\frac{1}{D},n)$. For any given torus $T \subseteq G$, the Kempf-Ness function of our quantum input on this torus looks the same as a classical input.

\begin{claim}
If we condition on $T$ being the "minimizing torus" so that the optimizer of the log-likelihood function lies in this torus, then the input looks like the classical setting.
\end{claim}

By the above reduction, if we can prove a bound on the optimizer for a fixed torus, then we have shown the same bound in the quantum setting.

\begin{fact}
For $A \in \R^{n \times m}$, $\|A\|_{\infty \to \infty} \leq \max_{i \in n} \sum_{j \in [m]} |A_{ij}|$.
\end{fact}

We consider $A$ to be a bipartite marginal, so with entries $\chi(\frac{1}{d_{a} d_{b}}, \frac{nD}{d_{a} d_{b}})$. With high probability the row sums are $\in \frac{1 \pm \eps}{d_{a}}$ and the column sums are $\in \frac{1 \pm \eps}{d_{b}}$, so $\|A\|_{\infty \to \infty} \leq \frac{1+\eps}{d_{a}}$. We want to show that the gradient flow decreases the operator norm of the moment map:
\[ -\partial_{t} \langle \mu, e_{a} \otimes e_{i} \rangle = (\rho_{a})_{i} d_{a} (\mu_{a})_{i} + \sum_{b \neq a} \langle e_{i} , A^{ab} d_{b} \mu_{b} \rangle    \]
Here $(\rho_{a})_{i} \in \frac{1 \pm \eps}{d_{a}}$, and the first term has the same sign as $(\mu_{a})_{i}$ and so is pushing the correct way. We would like to show all other forces (from $b \neq a$) cannot push enough to go the wrong way.
\[ -\partial_{t} |(\mu_{a})_{i}| \geq |(\mu_{a})_{i}| d_{a} (\rho_{a})_{i} - \sum_{b \neq a} \|A^{ab}\|_{\infty \to \infty} \|d_{b} \mu_{b}\|_{\infty}      \]
Then if e.g. $\forall b \neq a: \|A^{ab}\|_{\infty \to \infty} \ll 1/k d_{a}$ and we consider $(a,i)$ to be the marginal with highest error, this error is decreasing exponentially.

Let $\langle y, \vec{1}_{b} \rangle = 0$:
\[ \E \sum_{j} y_{j} \chi(\frac{1}{d_{a} d_{b}}, \frac{nD}{d_{a} d_{b}}) = \sum_{j} y_{j} \frac{1}{d_{a} d_{b}} = 0   \]
\[ \log \E \exp \theta \sum_{j} y_{j} \chi(\frac{1}{d_{a} d_{b}}, \frac{nD}{d_{a} d_{b}}) = \sum_{j} \frac{-n D}{2d_{a} d_{b}} \log \left( 1 - \frac{2 \theta y_{j}}{n D} \right)    \]
\[ \lesssim \theta^{2} \frac{\|y\|_{2}^{2}}{n D d_{a} d_{b}} \hspace{5mm} \forall \theta \lesssim \left( \frac{\|y\|_{\infty}}{n D}  \right)^{-1}    \]
\[ \implies \Pr [|\cdot| \geq t] \leq \begin{cases}
\exp \left( - \frac{\Omega(n D d_{a} d_{b} t^{2})}{\|y\|_{2}^{2}} \right) & \forall t \lesssim \frac{\|y\|_{2}^{2}}{d_{a} d_{b} \|y\|_{\infty}}
\\ \exp \left( - \frac{ \Omega(n D t) }{\|y\|_{\infty}} \right) & o.w.  \end{cases}   \]
To bound $\|A\|_{\infty \to \infty} \ll 1/k d_{a}$, we choose $t \ll \frac{\|y\|_{\infty}}{k d_{a}}$ which is in the second case. So we get probability $\leq \exp( - \frac{n D}{k d_{a}} )$. If we run a net over $\R^{d_{b}} \perp \vec{1}_{b}$ of size $\exp(\Tilde{O}(d_{b}))$ and union bound over all rows, we get $\|A\|_{\infty \to \infty} \ll 1/k d_{a}$ whp whenever $n D \gg k d_{a}( d_{b} + \log d_{a} ) $.

To make this robust to perturbations we could either give a deterministic bound on how a perturbation affects $\|A\|_{\infty \to \infty}$; or we could run the net over all perturbations near $I$ of size $\exp( \sum_{a} d_{a} )$. Either way we should get that $\|\mu\|_{op}$ is decreasing for all time when $nD \sim \Tilde{O}_{k}(\max_{ab} d_{a} d_{b} )$


\subsection{Some Remarks on Extensions}

\begin{remark}
One fantastic thing about Michael's proof of \cref{lem:perturbation} is that it generalizes quite well to a statement about $\|\cdot\|_{p \to p}$. Explicitly, if $\sigma := \|M\|_{p \to p}$, then we have the following bound:
\[ |\langle v v^{*}, I_{\overline{ab}} \otimes Z_{\pm} \otimes Y_{\pm} \rangle| \leq \sigma \|Z_{\pm} \|_{p} \|Y_{\pm} \|_{q}    \]
\[ \|Z_{+}\|_{p} \|Y_{+}\|_{q} + \|Z_{+}\|_{p} \|Y_{-}\|_{q} + \|Z_{-}\|_{p} \|Y_{+}\|_{q} + \|Z_{-}\|_{p} \|Y_{-}\|_{q} \]
\[ \leq (2 \|Z_{+}\|_{p}^{p} + 2 \|Z_{-}\|_{p}^{p})^{1/p} (2 \|Y_{+}\|_{q}^{q} + 2 \|Y_{-}\|_{q}^{q})^{1/q} = 2 \|Z\|_{p} \|Y\|_{q}    \]
\AR{We may want to use this result for $p \in \{1,\infty\}$ to bound the confounding forces towards $\|\mu\|_{op}$.} 
\end{remark}

We can in fact move a more general statement quite simply. This line of reasoning also applies to the operator on $\perp I$, but shows why Pisier's theorem is spectacular. 

\begin{fact}
$\|\nabla_{ab}^{2}\|_{op \to op} = \|\rho^{a}\|_{op}$, and dually $\|\nabla_{ab}^{2}\|_{1 \to 1} = \|\rho^{b}\|_{op}$. 
\end{fact}
\begin{proof}
\[ \|\nabla_{ab}^{2}\|_{op \to op} = \sup_{\|X\|_{op} \leq 1, \|Y\|_{1} \leq 1} \langle \rho^{ab}, X \otimes Y \rangle  \]
Choose $U,V$ to be the bases of the optimizers. Then restricting to this torus, we really have a matrix $M_{ij} := \langle \rho^{ab}, u_{i} u_{i}^{*} \otimes v_{j} v_{j}^{*} \rangle$, and $\|\nabla_{ab}^{2}\|_{op \to op} = \|M\|_{\infty \to \infty}$ for this basis. 
\[ \|M\|_{\infty \to \infty} = \max_{i} \sum_{j} |M_{ij}| = \max_{i} \sum_{j} \langle \rho^{ab}, u_{i} u_{i}^{*} \otimes v_{j} v_{j}^{*} \rangle = \max_{i} \langle u_{i} u_{i}^{*}, \rho^{a} \rangle \leq \|\rho^{a}\|_{op}   \]
The second statement follows since $(\nabla_{ab}^{2})^{*} = \nabla_{ba}^{2}$. 
\end{proof}

\begin{theorem} [Riesz-Thorin]
Given $\|T\|_{p_{0} \to q_{0}}, \|T\|_{p_{1} \to q_{1}} < \infty$, we have the following log convexity: if $\frac{1}{p_{\theta}} = \frac{\theta}{p_{0}} + \frac{1-\theta}{p_{1}}$ and $q_{\theta}$ defined similarly:
\[ \|T\|_{p_{\theta} \to q_{\theta}} \leq \|T\|_{p_{0} \to q_{0}}^{\theta} \|T\|_{p_{1} \to q_{1}}^{1-\theta}    \]
\end{theorem}

\begin{corollary}
For $p_{0} = 1, p_{1} = \infty$, we have $\frac{1}{p} = \frac{1/p}{1} + \frac{1-1/p}{\infty}$. 
\[ \|\nabla_{ab}^{2}\|_{p \to p} \leq \|\nabla_{ab}^{2}\|_{1 \to 1}^{1/p} \|\nabla_{ab}^{2}\|_{op \to op}^{1-1/p} = \|\rho^{b}\|_{op}^{1/p} \|\rho^{a}\|_{op}^{1-1/p}  \]
\end{corollary}

In our setting, we actually care about $\nabla^{2} (I-P)$ where we project away from $I$. As shown elsewhere, for matrices we can actually establish improved bounds by a net argument, which by the above corollary shows that a matrix of $\chi^{2}$ variables actually shrinks all $p$-norms of inputs $\perp 1$. Unfortunately the net argument for the quantum setting loses an important dimension factor. Further, for the low sample regime, it can be shown that the above projection does not improve the bound $\|\nabla_{ab}^{2} (I-P)\|_{op \to op}$. This is another reason Pisier's result is astonishing. 



\subsection{A silly trick}

The same idea that showed monotonicity of the operator norm for operator scaling can be used to very slightly improve the required expansion for $k$-tensors. In the proper norm, our gradient direction will be $\{d_{a} \mu^{a}\}$, and so to use robustness we need a bound on perturbation size $\sum_{a} d_{a} \|\mu^{a}\|_{op} =: \sum_{a} \eps_{a}$ for all time. Let $\eps := \max_{a} \eps_{a}$. In the proper norm, we have weak two sided bounds at any time:
\[ \eps^{2} = d_{a}^{2} \|\mu_{a}\|_{op}^{2} \leq d_{a}^{2} \|\mu^{a}\|_{F}^{2} \leq d_{a} \left( \sum_{b} d_{b} \|\mu^{b}\|_{F}^{2} \right) \leq d_{a} \left( \sum_{b} \eps_{b}^{2} \right) \leq d_{a} k \eps^{2}   \]

Now recall the change in operator norm under negative gradient flow:
\[ -\partial_{t} \eps := - d_{a} \partial_{t} \langle u u^{*}, \mu^{a} \rangle = d_{a} \left(\langle \rho^{a}, (u u^{*}) (d_{a} \mu^{a}) \rangle + \sum_{b \neq a} \langle \rho^{ab}, u u^{*} \otimes d_{b} \mu^{b} \rangle \right)   \]
\[ \geq (1 \pm \eps) \left( \eps_{a} - \sum_{b \neq a} \eps_{b} \right) \geq (1 \pm \eps) (k-2) \eps    \]
Here we've used that $\eps_{a}$ is the biggest. To see the transition for the first to second line, recall $\|d_{b} \mu^{b}\|_{op} = \eps_{b} \leq \eps$, and for any basis $\{v_{i}\} \subseteq \R^{d_{b}}$ we have
\[ \langle \rho^{ab}, u u^{*} \otimes d_{b} \mu^{b} \rangle \leq \langle \rho^{ab}, u u^{*} \otimes \eps_{b} I_{b} \rangle = \eps_{b} \langle \rho^{a}, u u^{*} \rangle = \eps_{b} \frac{1 \pm \eps}{d_{a}}    \]
Here we've used $\pm$ depending on whether $\eps$ comes from a largest or a smallest eigenvalue. So what the above inequalities show is $\partial_{t} (\log \eps) \leq (1+\eps) (k-2) \approxeq (k-2)$. From here on I'll be ignoring low order terms. I'll mention later that they add a small error to the main theorem. Therefore we assume we initially have $1-o(1) \approxeq 1$ strong convergence at time $0$ and further that this is true up to another $o(1)$ term while our perturbation size is $o(1)$. 

\begin{claim}
$\forall t: \eps(t) \lesssim d^{\frac{(k-2)}{1 + 2(k-2)}} \eps(0)$
\end{claim}
\begin{proof}
Let $T$ be the first time when $\eps(t) \geq d^{(k-2) c} \eps(0)$: 
\[ (\log \eps)' \leq k-2 \implies T \geq \frac{\log \eps(T) - \log \eps(0)}{k-2} = c \log d  \]
On the other hand let $T^{*}$ be the last time when $\eps(T^{*}) \leq o(1)$, so up to this time we have very strong convergence:
\[ \forall t \leq T^{*}: \sum_{a} d_{a} \|\mu^{a}\|_{F}^{2} (t) \leq e^{-t} \sum_{a} d_{a} \|\mu^{a}\|_{F}^{2} (0) \leq e^{-t} k \eps(0)^{2}    \]
In particular if $T^{*} > \log k d^{1 - 2(k-2)c}$:
\[ \eps(T^{*})^{2} \leq e^{-T^{*}} d k \eps(0)^{2} < d^{2(k-2)c} \eps(0)^{2} = \eps(T)^{2}    \]
This would be a contradiction if $T \geq T^{*}$, so matching terms we get
\[ c \log d = \log k + (1 - 2(k-2)c) \log d \iff c = \frac{1}{1 + 2(k-2)}     \]
Again we ignore the lower order $\log k$ term. So if we have strong convergence up to time $T^{*}$ then the claim is proven. 
\end{proof}

The above claim shows that if $d^{\frac{(k-2)}{1 + 2(k-2)}} \eps < o(1)$, and we have $1-o(1)$ at time $0$, then in fact we have $1-o(1)$ strong convergence at all time and we remain in an $d^{\frac{(k-2)}{1 + 2(k-2)}} \eps < o(1)$ sized operator norm ball the whole time. Therefore it suffices to take $d^{3 - \frac{1}{1 + 2(k-2)}}$ samples to get strong convergence. Forgive me the $k$'s and this is our savings on samples. 


\subsection{The Final Concentration}
Above we were trying to bound competing forces on the largest eigenvalue of $\mu$ for all time to make sure our scaling remains inside a small $\|\cdot\|_{op}$ ball. \AR{Here we will show it is enough to have small forces initially. }

\AR{Correction: the following line of argument isn't really good enough. This naive way of bounding the competing forces means that our $\|\mu\|_{op}$ jumps into the worst regime (exponentially increasing) within a constant amount of time. This is because our bound on the force is proportional to the size of the perturbation. }

If our input at time $t$ is scaled by $e^{\delta}$, then we have the following rough bound on our errors:
\[ \|\mu_{b}(t) - \mu_{b}(0)\|_{op} = \sup_{v} \langle (e^{\delta} - I) \cdot \rho^{ab}, I_{a} \otimes v v^{*} \rangle \lesssim \delta \|T^{*}\|_{op_{a} \to op_{b}} \|I_{a}\|_{op} \|v v^{*} \|_{1} \leq \delta \|\rho^{b}\|_{op}    \]

\[ \|T(\mu_{b})(t)\|_{op} - \|T(\mu_{b})(0)\|_{op} \leq \|T(\mu_{b})(t) - T(\mu_{b})(0)\|_{op}  = \sup_{u} \langle u u^{*}, T(\mu_{b})(t) - T(\mu_{b})(0) \rangle  \]
Let $\mu_{b}(t) := \mu_{b}(0) + Y =: \mu_{b} + Y$
\[ \|T^{t}(\mu_{b}(t)) - T^{0}(\mu_{b}(0))\|_{op} \leq \|T^{t}(\mu_{b} + Y) - T^{t}(\mu_{b})\|_{op} + \|T^{t}(\mu_{b}) - T^{0}(\mu_{b})\|_{op}     \]
\[ \leq \|T^{t}\|_{op_{b} \to op_{a}} \|Y\|_{op} + \delta \|T^{0}\|_{op_{b} \to op_{b}} \|\mu_{b}\|_{op}    \]
\[ \leq \delta \|\rho^{a}(t)\|_{op} \|\rho_{b}\|_{op} + \delta \|\rho_{a}\|_{op} \|\mu_{b}\|_{op}   \]
Assuming $\|\mu_{b}\|_{op} \ll \delta/d_{b}$, this is on the order of $\delta/ d_{a} d_{b}$. Recall the formula for the top eigenvalue change:
\[ -\partial_{t} d_{a} \|\mu_{a}\|_{op} = \|d_{a} \rho^{a}\|_{op} \|d_{a} \mu^{a}\|_{op} + \sum_{b \neq a} d_{a} d_{b} \langle \rho^{\{a,b\}}, u u^{*} \otimes \mu_{b} \rangle     \]
\[ \geq  (1 \pm d_{a} \|\mu_{a}\|_{op}) \|d_{a} \mu^{a}\|_{op} -  \sum_{b \neq a} d_{a} d_{b} \|T_{b \to a}(\mu_{b})\|_{op}   \]
\[ \geq (1 \pm \epsilon_{a}) \epsilon_{a} - (k-1) \delta  \]
This is the competing force at time $t$, assuming it is very small at time $0$. Therefore we have that any (normalized) eigenvalue of $\mu$ that is greater than $(k-1) \delta$ at time $t$ is decreasing in absolute value. It should be enough to have $\delta \ll 1/k \log D$, and then in total the scaling should be of operator norm $\ll 1$, so we will have convergence. 

\section{Better upper bound for matrix model using HCIZ}


\subsection{Attempt at better upper bound for matrix normal model}
Note if $d_{a} > d_{b}$, the error of the $a$ marginal will be much larger than that of the $b$ marginal. So we could try to analyze what happens after one iteration of flip-flop where we normalize the $a$ to identity. We claim our input if our input is $X_{1}, ..., X_{n} \subseteq \R^{d_{a} \times d_{b}}$ iid from $N(0,I_{a \times b})$, and we normalize so that the rows of $X$ form an orthogonal basis for $\R^{d_{a}}$, our new input $Y$ will be uniform from the Stiefel manifold of bases for $d_{a}$ dimensional subspaces of $\R^{nd_{b}}$. Given this we can try to run a net argument to bound the $b$ marginal:

\[ Y := \left( \sum_{i=1}^{n} X_{i} X_{i}^{*} \right)^{-1/2} X      \]
\[ \mu_{b}(Y) = \sup_{\xi \in \R^{b}} \langle \xi \xi^{*}, \sum_{i} X_{i}^{*} \left( \sum_{j} X_{i} X_{i}^{*} \right)^{-1} X_{i} \rangle       \]

All this to say I would like to bound the following using HC formula:

\begin{theorem} 
For Hermitian $A,B \in \R^{N}$, wlog diagonal and increasing $a_{1} < ... < a_{N}, b_{1} < ... < b_{N}$ we have:
\[  \int_{U(N)} \exp \langle A, U B U^{*} \rangle dU = \prod_{k=1}^{N-1} k!  \frac{\det\{e^{a_{i} b_{j}}\}_{ij}}{\prod_{j > i} (a_{j} - a_{i}) (b_{j} - b_{i})} \]
\end{theorem}

I would like to choose $A$ to be a rank $d_{a}$ orthogonal projection and $B$ a rank $n$ orthogonal projection in $\R^{N = nd_{b}}$. Because these have non-distinct eigenvalues we use L'hopital's rule to take limits. 

\[ \lim_{a_{1} \to 0} e^{a_{1} b_{j}} = 1     \]
\[ \lim_{a_{1} \to 0} a_{j} - a_{1} = a_{j}     \]
So the matrix has first row all ones and denominator $\prod_{j > i > 1} (a_{j} - a_{i}) \prod_{j > 1} a_{j}$. In this case if we set $a_{2} = 0$, the second row will become the same as the first, so the matrix becomes singular. Luckily the denominator also has an $a_{2}$ term so we use L'hopitals:
\[ \lim_{a_{2} \to 0} \partial_{a_{2}} e^{a_{2} b_{j}} = b_{j}     \]
\[ \lim_{a_{2} \to 0} \partial_{a_{2}} \prod_{j > i > 1} (a_{j} - a_{i}) \prod_{j > 1} a_{j} = \prod_{j > i > 2} (a_{j} - a_{i}) \prod_{j > 2} a_{j}^{2}    \]
The last step is by the product rule, since the only non-vanishing term is when the $\partial$ is applied to the $a_{2}$ term. 

Now the matrix has first row $\vec{1}$ and second row $\vec{b}$. Therefore setting $a_{3} = 0$ will make the matrix singular, and so will the derivative. Again luckily there is an order two zero in the denominator $a_{3}^{2}$ so we apply L'hopitals twice:
\[ \lim_{a_{3} \to 0} \partial_{a_{3}}^{2} e^{a_{2} b_{j}} = b_{j}^{2}     \]
\[ \lim_{a_{3} \to 0} \partial_{a_{3}}^{2} \prod_{j > i > 2} (a_{j} - a_{i}) \prod_{j > 2} a_{j}^{2} = 2! \prod_{j > i > 3} (a_{j} - a_{i}) \prod_{j > 3} a_{j}^{3}    \]
Continuing this way we get the first $N-d_{a}$ rows 
\[ \begin{pmatrix} 1 & 1 & ... & 1
\\ b_{1} & b_{2} & ... & b_{N}
\\ b_{1}^{2} & b_{2}^{2} & ... & b_{N}^{2}
\\ & & ... & 
\\ b_{1}^{N-d_{a}-1} & b_{2}^{N-d_{a}-1} & ... & b_{N}^{N-d_{a} - 1}
\end{pmatrix}        \]
And the denominator terms including $A$ are 
\[ \prod_{k=1}^{N-d_{a}-1} k! \prod_{j > i > N-d_{a}} (a_{j} - a_{i}) \prod_{j > N-d_{a}} a_{j}^{N-d_{a}}     \]

Now we will set $b_{1} = ... = b_{N-n} = 0$ by the same process:
\[ \lim_{b_{1} \to 0} b_{1}^{0} = 1, \hspace{3mm} \lim_{b_{1} \to 0} b_{1}^{(i-1) \geq 1} = 0, \hspace{3mm} \lim_{b_{1} \to 0} e^{a_{i} b_{1}} = 1     \]
\[ \lim_{b_{1} \to 0} \prod_{j > i} (b_{j} - b_{i}) = \prod_{j > i > 1} (b_{j} - b_{i}) \prod_{j > 1} b_{j}     \]
Therefore the first column is $(1, 0, ..., 0, 1, 1, ..., 1)$, i.e. the first entry and the last $d_{a}$ entries are $1$. In the second iteration setting $b_{2} = 0$ would make the matrix singular and also make the denominator vanish:
\[ \lim_{b_{2} \to 0} \partial_{b_{2}} b_{2}^{0} = 0, \hspace{3mm} \lim_{b_{2} \to 0} \partial_{b_{2}} b_{2}^{1} = 1, \hspace{3mm} \lim_{b_{2} \to 0} \partial_{b_{2}} b_{2}^{(i-1) > 1} = 0, \lim_{b_{2} \to 0} \partial_{b_{2}} e^{a_{i} b_{2}} = a_{i}    \]
\[ \lim_{b_{2} \to 0} \partial_{b_{2}} \prod_{j > i > 1} (b_{j} - b_{i}) \prod_{j > 1} b_{j} = \prod_{j > i > 2} (b_{j} - b_{i}) \prod_{j > 2} b_{j}^{2}    \]
The second column is therefore $(0,1,0, ..., 0, a_{N-d_{a}+1}, ..., a_{N})$. One more iteration:
\[ \lim_{b_{3} \to 0} \partial_{b_{2}}^{2} b_{2}^{0} = 0, \hspace{3mm} \lim_{b_{2} \to 0} \partial_{b_{2}}^{2} b_{2}^{1} = 0, \hspace{3mm} \lim_{b_{2} \to 0} \partial_{b_{2}}^{2} b_{2}^{2} = 2!, 
\hspace{3mm} \lim_{b_{2} \to 0} \partial_{b_{2}}^{2} b_{2}^{(i-1) > 2} = 0, \lim_{b_{2} \to 0} \partial_{b_{2}}^{2} e^{a_{i} b_{2}} = a_{i}^{2}    \]
\[ \lim_{b_{2} \to 0} \partial_{b_{2}}^{2} \prod_{j > i > 2} (b_{j} - b_{i}) \prod_{j > 2} b_{j}^{2} = 2! \prod_{j > i > 3} (b_{j} - b_{i}) \prod_{j > 3} b_{j}^{3}     \]
Wlog at this point we assume $d_{a} > n$ so $N-d_{a} < N-n$. So continuing this way we have the top left block $(N-d_{a}) \times (N-n)$, the first $N-d_{a}$ columns are $\text{diag}\{(i-1)!\}$ and the next $d_{a} - n$ columns are all $0$. The bottom left block $d_{a} \times N-n$ is 
\[ \begin{pmatrix} 1 & a_{N-d_{a}+1} & a_{N-d_{a}+1}^{2} & ... & a_{N-d_{a}+1}^{N-n-1}
\\ 1 & a_{N-d_{a}+2} & a_{N-d_{a}+2}^{2} & ... & a_{N-d_{a}+2}^{N-n-1}
\\ & & ... & 
\\ 1 & a_{N} & a_{N}^{2} & ... & a_{N}^{N-n-1} 
\end{pmatrix}        \]
The denominator terms involving $b$ are 
\[ \prod_{k=1}^{N-n-1} k! \prod_{j > i > N-n} (b_{j} - b_{i}) \prod_{j > N-n} b_{j}^{N-n}     \]

Now we would like to set $b_{N-n+1} = ... = b_{N} = 1$. 
\[ \lim_{b_{N-n+1} \to 1} b_{N-n+1}^{i-1} = 1, \hspace{5mm} \lim_{b_{N-n+1} \to 1} e^{a_{i} b_{N-n+1}} = e^{a_{i}}     \]
\[ \lim_{b_{N-n+1} \to 1} \prod_{j > i > N-n} (b_{j} - b_{i}) = \prod_{j > i > N-n+1} (b_{j} - b_{i}) \prod_{j > N-n+1} (b_{j} - 1)    \]
We have left out the term $\prod_{j > N-n} b_{j}^{N-n}$ as we will set them all to one and so this term will end up being $1$. So the first column in this block is $(1, ..., 1, e^{a_{N-d_{a}+1}}, ..., e^{a_{N}})$. If we set $b_{N-n+2} = 1$ then the column will be the same and the denominator has a root:
\[ \lim_{b \to 1} \partial_{b} b^{i-1} = i-1, \hspace{5mm} \lim_{b \to 1} \partial_{b} e^{a_{i} b} = a_{i} e^{a_{i}}    \]
\[ \lim_{b_{N-n+2} \to 1} \partial_{b_{N-n+2}} \prod_{j > i > N-n+1} (b_{j} - b_{i}) \prod_{j > N-n+1} (b_{j} - 1) = \prod_{j > i > N-n+2} (b_{j} - b_{i}) \prod_{j > N-n+2} (b_{j} - 1)^{2}   \]
One more iteration:
\[ \lim_{b \to 1} \partial_{b}^{2} b^{i-1} = (i-1)(i-2), \hspace{5mm} \lim_{b \to 1} \partial_{b}^{2} e^{a_{i} b} = a_{i}^{2} e^{a_{i}}    \]
\[ \lim_{b_{N-n+3} \to 1} \partial_{b_{N-n+2}}^{2} \prod_{j > i > N-n+2} (b_{j} - b_{i}) \prod_{j > N-n+2} (b_{j} - 1)^{2} = \prod_{j > i > N-n+3} (b_{j} - b_{i}) 2! \prod_{j > N-n+3} (b_{j}-1)^{3}  \]
Continuing on this way we can write the right block. For $i \in \{0, ..., N-d_{a}-1\}$ and $j \in \{0, ..., n-1\}$, the entry in position $(i+1, N-n+j+1)$ is $(i)_{j} := i! / (i-j)!$. In the bottom right block we have for $i \in \{N-d_{a}+1, ..., N\}$ and $j \in \{0, ..., n-1\}$, the entry in position $(i,N-n+j+1)$ is $a_{i}^{j} e^{a_{i}}$. We have set all $b$ variables to scalars, so there are no more $b$ variables in the denominator and we have added a term $\prod_{k=1}^{n-1} k!$. 


Now we would like the set $a_{N-d_{a}+1} = ... = a_{N} = t$. 
\[ \lim_{a_{N-d_{a}+1} \to t} a_{N-d_{a}+1}^{j-1} = t^{j-1}     \]
\[ \lim_{a_{N-d_{a}+1} \to t} \prod_{j > i > N-d_{a}} (a_{j} - a_{i}) \prod_{j > N-d_{a}} a_{j}^{N-d_{a}} = \prod_{j > i > N-d_{a}+1} (a_{j} - a_{i}) \prod_{j > N-d_{a}+1} (a_{j} -t) \prod_{j > N-d_{a} + 1} a_{j}^{N-d_{a}} t^{N-d_{a}}    \]
Again setting $a_{N-d_{a}+2} = 1$ would make the matrix singular and the denominator would vanish so we have
\[ \lim_{a \to t} \partial_{a} a^{j-1} = (j-1) t^{j-2}    \]
\[ \lim_{a_{N-d_{a}+2} \to t} \partial_{a_{N-d_{a}+2}} \prod_{j > i > N-d_{a}+1} (a_{j} - a_{i}) \prod_{j > N-d_{a}+1} (a_{j} -t) \prod_{j > N-d_{a} + 1} a_{j}^{N-d_{a}} t^{N-d_{a}} \]
\[= \prod_{j > i > N-d_{a}+2} (a_{j} - a_{i}) \prod_{j > N-d_{a}+2} (a_{j} -t)^{2} \prod_{j > N-d_{a} + 2} a_{j}^{N-d_{a}} t^{2(N-d_{a})}     \]
Continuing on this way we can find the bottom left block. For $i \in \{0, ..., d_{a}-1\}$ and $j \in \{0, ..., N-n-1\}$, the entry in position $(N-d_{a}+i+1,j+1)$ is $(j)_{i} t^{j-i}$. For $j \in \{0, ..., n-1\}$ the entry in position $(N-d_{a}+i+1,N-n+j+1)$ is $\partial_{x=t}^{i} x^{j} e^{x}$. This is not an explicit formula but we will arrive at some recurrences involving these terms. The denominator is $\prod_{k=1}^{d_{a}-1} k! t^{d_{a}(N-d_{a})}$. In total we will rewrite the whole matrix after all the L'hopitals
\[ \begin{pmatrix}  X^{00} & X^{01} \\ X^{t0} & X^{t1} \end{pmatrix}     \]
The subscripts indicate the setting of $a_{i}, b_{j}$ in that block. Then $X^{00} \in \R^{(N-d_{a}) \times (N-n)}$ and its top left square block of size $(N-d_{a}) \times (N-d_{a})$ is a diagonal matrix with entries 
\[  (X^{00})_{i+1,i+1} = i! \]
The rest is $0$. $X^{01} \in \R^{(N-d_{a}) \times n}$ has entries 
\[ (X^{01})_{i+1,j+1} = (i)_{j}    \]
Note this is $0$ for $i < j$. $X^{t0} \in \R^{d_{a} \times (N-n)}$ has entries 
\[ (X^{t0})_{i+1,j+1} = (j)_{i} t^{j-i}     \]
Again this is $0$ for $j < i$. $X^{t1} \in \R^{d_{a} \times n}$ is the most complicated and has entries 
\[ (X^{t1})_{i+1,j+1} = \partial_{x=t}^{i} x^{j} e^{x}  \]

It is easy to calculate the beginning terms: $\partial_{x=t}^{0} x^{j} e^{x} = t^{j} e^{t}$ and $\partial_{x=t}^{i} x^{0} e^{t} = e^{t}$. Further note the following recurrence:
\[ \partial_{x}^{i} (x^{j} e^{x}) = \partial_{x}^{i-1} (j x^{j-1} e^{x} + x^{j} e^{x})   \]
We can rewrite this in block form where now $i,j$ index the $(i+1,j+1)$ entry of the given block:
\[ \begin{pmatrix} \text{diag}\{i!\}_{i=0}^{N-d_{a}-1} & 0_{(N-d_{a}) \times (d_{a}-n)} & \{(i)_{j}\}_{i=0,j=0}^{i=N-d_{a}-1, j=n-1}
\\ \{(j)_{i} t^{j-i}\}_{i=0,j=0}^{i=d_{a}-1,j=N-n-1} & & \{\partial_{x}^{i} x_{j} e^{x} \}_{i=0,j=0}^{i=d_{a}-1,j=n-1}
\end{pmatrix}     \]
We give new names for ease of use:
\[ \begin{pmatrix} A & 0 & B
\\ C & & D
\end{pmatrix}     \]

So the HC formula has reduced to the determinant of the above matrix divided by the following factorial terms:
\[ \left( \prod_{k=1}^{N-d_{a}-1} k! \right) \left( \prod_{k=1}^{N-n-1} k! \right) \left( \prod_{k=1}^{n-1} k! \right) \left( \prod_{k=1}^{d_{a}-1} k! \right) \left( t^{d_{a}(N-d_{a})} \right) / \left( \prod_{k=1}^{N-1} k! \right)     \]
The final term is from the numerator of the original formula. 


To calculate the determinant we will use Schur complement:
\[ \det \begin{pmatrix} A & 0 & B
\\ C & & D
\end{pmatrix} = \det(A) \det(D - C A^{-1} B)    \]
with slight abuse of notation as the first $d_{a} - n$ columns of the top right block are $0$ and so don't affect the Schur complement. In particular if $\{B_{j+1}\}$ are the columns of $B$ and $\{C_{i+1}\}$ are the rows of $C$, we want to calculate $\{ C_{i+1}^{*} A^{-1} B_{j+1} \}$. We can find an explicit for when $i=0$ or $j=0$ and then we will show that these inner products satisfy the same recurrence as the $D$ block. 

$C_{1} = (1, t, t^{2}, ..., t^{N-d_{a}-1})$, and $B_{1} = (1, 1, ..., 1)$. Since $A$ is diagonal $(A^{-1})_{i+1} = (i!)^{-1}$. Therefore:
\[ C_{1}^{*} A^{-1} B_{1} = \sum_{k=0}^{N-d_{a}-1} \frac{C_{1,k+1} B_{1,k+1}}{A_{k+1}} = \sum_{k=0}^{N-d_{a}-1} \frac{t^{k}}{k!}    \]
Note this is the first $N-d_{a}-1$ terms of the Taylor approximation for $e^{t}$ at $0$. For $B_{j+1}$ the first $j$ entries are $0$, and after this the terms are $j$ falling terms. 
\[ C_{1}^{*} A^{-1} B_{j+1} = \sum_{k=0}^{N-d_{a}-1} \frac{C_{1,k+1} B_{j+1,k+1}}{A_{k+1}} = \sum_{k = j}^{N-d_{a}-1} \frac{t^{k} k! / (k-j)!}{k!} = t^{j} \sum_{k = 0}^{N-d_{a}-j-1} \frac{t^{k}}{k!}      \]
Similarly we can note the first $i$ terms of $C_{i+1}$ are $0$:
\[ C_{i+1}^{*} A^{-1} B_{1} = \sum_{k = i}^{N-d_{a}-1} \frac{ t^{k-i} k! / (k-i)! \cdot 1}{k!} = \sum_{k=0}^{N-d_{a}-i-1} \frac{t^{k}}{k!}    \]
By our previous easy calculation of the first row and column of $D$, this gives easy approximations for the first row and column of the Schur complement: 
\[ (D - C A^{-1} B)_{i+1,1} = e^{t} - \sum_{k=0}^{N-d_{a}-i-1} \frac{t^{k}}{k!} \in \left(0, \frac{t^{N-d_{a}-i}}{(N-d_{a}-i)!} \right)   \]
\[ (D - C A^{-1} B)_{1,j+1} = t^{j} \left( e^{t} - \sum_{k=0}^{N-d_{a}-j-1} \frac{t^{k}}{k!} \right) \left(0, \frac{t^{N-d_{a}}}{(N-d_{a}-j)!} \right) \]
We will also show the same recurrence as $D$:
\[ C_{i+1}^{*} A^{-1} B_{j+1} = C_{i}^{*} A^{-1} B_{j+1} + j C_{i}^{*} A^{-1} B_{j}      \]
This is accomplished by the following recurrence relations:
\[ C_{i+1,k+1} = (k)_{i} t^{k-i} = k (k-1)_{i-1} t^{k-i} = k C_{i,k} \]
\[ A_{k+1} = k! = k (k-1)! = k A_{k}  \]
\[ B_{j+1,k+1} = (k)_{j} = k (k-1)_{j-1} = k B_{j,k}     \]
\[ B_{j+1,k} = (k-1)! / (k-1-j)! = (k-j) (k-1)! / (k-j)! = (k-j) B_{j,k} \]
Again noting the first $j$ terms of $C_{j+1}$ are $0$:
\[ C_{i+1}^{*} A^{-1} B_{j+1} = \sum_{k=j}^{N-d_{a}-1} \frac{C_{i+1,k+1} B_{j+1,k+1}}{A_{k+1}} = \sum_{k=j}^{N-d_{a}-1} \frac{k C_{i,k}}{k A_{k}} (k B_{j, k})      \]
\[ = j \sum_{k \geq j} \frac{C_{i,k} B_{j,k}}{A_{k}} + \sum_{k \geq j} \frac{C_{i,k}}{A_{k}} ((k-j) B_{j,k}) = j C_{i}^{*} A^{-1} B_{j} + C_{i}^{*} A^{-1} B_{j+1}     \]

So we have in fact shown all entries of the matrix $D - C A^{-1} B$ are nonnegative, and we have a simple recurrence as we go down the rows. One more simplification for the sake of bounds is to write everything in terms of the first row and column, for which we have clean bounds. In general our matrix $M$ satisfies $M_{i+1,j+1} = M_{i,j+1} + j M_{i,j}$. Under this condition we show

Above the main diagonal, i.e. $j \geq i$, the recurrence stops at the first row:
\[ M_{i+1,j+1} = \sum_{k=0}^{i} {i \choose k} (j)_{k} M_{1,j+1-k} = \sum_{k=0}^{i} \frac{i! j!}{k! (i-k)! (j-k)!} M_{1,j+1-k}     \]
Otherwise, if $i > j$ we have contribution from the first column
\[ M_{i+1,j+1} = \sum_{k=0}^{j} \frac{i! j!}{k! (i-k)! (j-k)!} M_{1,j+1-k} + \sum_{k=0}^{i-j} {j+k \choose k} j! M_{(i-j) +1-k,1} - {i \choose j} j! M_{11}    \]
This last term is because we counted $M_{11}$ in both sums. We can rewrite this 
\[ M_{i+1,j+1} = \sum_{k=0}^{j} \frac{i! j!}{k! (i-k)! (j-k)!} M_{1,j+1-k} + \sum_{k=j+1}^{i} (k)_{j} M_{i+2-k,1}    \]
\[ = \sum_{k=0}^{j} \frac{i! j!}{k! (i-k)! (j-k)!} M_{1,j+1-k} +  \sum_{k=0}^{i-j} (i-k)_{j} M_{k+1,1}   \]

These formulas can be proved simply by induction. From here we can use the following approximations:
\[ \det(X) = \sum_{\sigma \in S_{N}} (-1)^{|\sigma|} \prod_{i} X_{i,\sigma(i)} \leq \sum_{\sigma \in S_{N}} \prod_{i} |X_{i,\sigma(i)}| \leq \prod_{i} \left( \sum_{j} |X_{ij}| \right)     \]
\[ \det(X) \leq \prod_{i} \|X_{i}\|_{2}     \]
I'm having a bit of trouble actually doing bounds though. 


\section{Old stuff}

\subsection{Different Inner Product}

\begin{definition}
For desired marginals $\{R_{a}^{2}\}_{a \in [k]}$ (assume for now $R$ are Hermitian though we can pick different square roots if required), define inner product
\[ \langle Z, Y \rangle_{R} := \sum_{a} \langle R_{a} Z R_{a}^{*}, Y \rangle \]
\[ \|Z\|_{R}^{2} := \langle Z, Z \rangle_{R} = \sum_{a} \|R_{a} Z_{a}\|_{F}^{2}   \]
\end{definition}

We restate the projective likelihood function and define gradient and Hessian in this metric:

\begin{definition}
\[ f_{\vec X}(\Theta_1, \dots, \Theta_n) = \log \left\langle \sum_{i \in [n]} X_{i} X_{i}^{*},  \bigotimes_{a \in [k]} \Theta_a \right\rangle - \sum_{a \in [k]} \frac{1}{d_a} \log\det \Theta_a  \]
Also $\rho := \sum_{i} X_{i} X_{i}^{*}$ and $\{\rho^{S}\}_{S \subseteq [k]}$ are marginals.
\end{definition}

\begin{fact}
\[ (\nabla f(I))_{a} = d_{a} \rho^{\{a\}} - I_{a}  \]
\end{fact}
\begin{proof}
We can define $\nabla f$ dually as $\forall Z: \langle \nabla f(I), Z \rangle_{R} := \partial_{t=0} f(e^{tZ})$
\[ \partial_{t=0} f(e^{t Z_{a}}) = \partial_{t=0} \langle \rho, I_{\overline{a}} \otimes e^{tZ_{a}}  \rangle - \partial_{t=0} \frac{1}{d_{a}} \log\det e^{t Z_{a}}  \]
\[ = \left\langle \rho^{\{a\}} - \frac{1}{d_{a}} I_{a}, Z_{a} e^{t Z_{a}} \right\rangle|_{t=0} = \left\langle R_{a}^{-1} \left( \rho^{\{a\}} - \frac{1}{d_{a}} I_{a} \right) R_{a}^{-1}, Z_{a} \right\rangle_{R}   \]

Similarly we define the Hessian as
\[ \partial_{s=t=0} f(e^{tZ_{a} + sY_{b}}) = \langle \rho, \{ I_{\overline{a}} \otimes Z_{a}, I_{\overline{b}} \otimes Y_{b} \} \rangle   \]
\[ \implies (\nabla^{2} f(I))_{aa} = \langle R_{a}^{-1} \rho^{\{a\}} R_{a}^{-1}, \{Z, Y\} \rangle_{R}    \]
\[ \implies (\nabla^{2} f(I))_{ab} = \langle \rho^{\{a,b\}}, Z \otimes Y \rangle   \]
\end{proof}

\AR{Not sure how to define Hessian. I think I'd like the Hessian to be
\[  \sum_{a} E_{aa} \otimes (1 \pm \epsilon) I + \sum_{a \neq b} E_{ab} \otimes \pm \lambda  \]
for some small $\epsilon,\lambda$. Then the Hessian will be $1-\epsilon - (k-1) \lambda$-strongly convex. }



\begin{lemma} [Restatement of \cref{lem:convex-ball}]
Let $f$ be geodesically convex everywhere. All the below quantities are wrt metric $\langle \cdot, \cdot \rangle_{R}$. Assume $f$ and $\lambda$-strongly geodesically convex ball of radius $\kappa$ about $I$; further assume the geodesic gradient satisfies $\|\nabla f(I)\|_{R} = \eps < \lambda \kappa$. Then there is an optimizer within an $\eps/\lambda$-ball.
\end{lemma}
\begin{proof} [Proof of \cref{lem:convex-ball}]
The proof is exactly the same except the following:
\[ g'(0) = \langle \nabla f(I), Z \rangle_{R} \geq - \|\nabla f(I)\|_{R} \|Z\|_{R} \geq - \eps     \]
\end{proof}

\begin{remark}
Note the perturbation lemma then gives the following strategy. By Cole's lemma, we have that $c \|\nabla f(I)\|_{R} \geq \|\nabla f(I)\|_{op}$. If we can say the same thing for the optimizer $Z$, then it is enough for $\lambda \kappa \geq \Omega(1/c) > \eps$ and we can improve sample complexity to $nD > c \max_{a} d_{a}^{2}$.

A similar thing is true if we can show the above inequality for the gradient flow for $\log \max_{a} d_{a}$ time.
\end{remark}

\begin{lemma}
$\lambda$-strong convexity is a sufficient condition for fast convergence of the gradient flow:
\[ - \partial_{t=0} \|\nabla f(e^{-t\nabla f(I)})\|_{R}^{2} = -\partial_{t=0}^{2} f(e^{-t\nabla f(I)}) = \langle \nabla^{2} f, \nabla f \otimes \nabla f \rangle \geq \lambda \|\nabla f\|_{R}^{2}    \]
\AR{Not sure how to write the third term above, the inner product with Hessian}
\end{lemma}

\subsection{Old proof of \cref{thm:tensor-convexity}}


\begin{proof} [Proof of \cref{thm:tensor-convexity}]
Take any quadratic form of the Hessian for $\{Z_{a} \perp I_{a}\}$:
\[ \partial_{t=0}^{2} f(e^{t Z}) = \sum_{a} \langle Q^{a}, Z_{a}^{2} \rangle + \sum_{a \neq b} \langle Q^{ab}, Z_{a} \otimes Z_{b} \rangle   \]
\[ \geq \sum_{a} \lambda_{\min}(Q^{a}) \|Z_{a}\|_{F}^{2} - \sum_{a \neq b} \|Q^{ab} (I-P_{ab})\|_{op} \|Z_{a}\|_{F} \|Z_{b}\|_{F}    \]
Now we can use our high probability bounds derived above:
\[ \forall a: Q^{a} \succeq \frac{1-\epsilon}{d_{a}} I_{a}; \hspace{10mm}
\forall a \neq b: \|Q^{ab} (I-P_{ab})\|_{op} < \frac{\lambda}{\sqrt{d_{a} d_{b}}}   \]
\[ \implies \partial_{t=0}^{2} f(e^{t Z}) \geq \sum_{a} \frac{1-\eps}{d_{a}} \|Z_{a}\|_{F}^{2} - \sum_{a \neq b} \frac{\lambda}{\sqrt{d_{a} d_{b}}} \|Z_{a}\|_{F} \|Z_{b}\|_{F}  \]
\[ \geq \sum_{a} \frac{1-\eps+\lambda}{d_{a}} \|Z_{a}\|_{F}^{2} - \lambda \left( \sum_{a} \frac{1}{\sqrt{d_{a}}} \|Z_{a}\|_{F}   \right)^{2}   \]
\[ \geq \sum_{a} \frac{1-\eps+\lambda}{d_{a}} \|Z_{a}\|_{F}^{2} - k\lambda \sum_{a} \frac{1}{d_{a}} \|Z_{a}\|_{F}^{2} \]
\[ = (1-\eps-(k-1)\lambda) \|Z\|^{2}    \]
Choosing $\eps,\lambda$ small enough gives the theorem.
\end{proof}

\begin{proof}[Proof: \CF{Akshay's conceptual proof of \cref{thm:tensor-convexity}}]
We can in fact show that $\nabla^{2}$ is well-conditioned using the following:
\[ -\begin{pmatrix} 1 & 0 \\ 0 & 1 \end{pmatrix}
\preceq \begin{pmatrix} 0 & 1 \\ 1 & 0 \end{pmatrix}
\preceq \begin{pmatrix} 1 & 0 \\ 0 & 1 \end{pmatrix}
\]
%We can rewrite the Hessian using shorthand $\{M_{a}\}$ for the diagonal blocks and $\{M_{ab}\}$ for off-diagonal blocks: \CF{why not just write $\nabla^2_{ab}f$?} \CF{Also, the matrices $E_{aa} \otimes M_a$ are not all of the same size} 
\\ \AR{Ya $\nabla_{ab}$ notation is fine, just needed something that was a matrix of the right dimensions, so shorthand M was to avoid weird things with $\rho$}
\\ \AR{It's fine if they're of different sizes, we enumerate the basis of the whole space as $\cup_{a} e_{a} \otimes \{e_{i \in [d_{a}]}\}$ }\\
\CF{$E_{aa} \otimes \nabla^2_{aa}$ is $k d_a^2 \times k d_a^2$ dimensional. So how does this make sense? Maybe needs to be updated along the lines of the next proof.}
\[ \nabla^{2} f = \sum_{a} E_{aa} \otimes \nabla^{2}_{aa} + \sum_{a \neq b} E_{ab} \otimes \nabla^{2}_{ab}  \]
Now we can again use the high-probability bounds derived above: \TODO{actually cref them}
\begin{align}\nabla^{2}_{aa} \in \frac{1 \pm \eps}{d_{a}}; \hspace{5mm} \forall a \neq b: \|\nabla^{2}_{ab}\|_{op} \leq \frac{\lambda}{\sqrt{d_{a} d_{b}}} \label{eq:expansion-thing}  \end{align}
\[ \nabla^{2} \preceq \sum_{a} E_{aa} \otimes \left( \frac{1+\eps}{d_{a}} I_{a} \right) + \sum_{a < b} E_{aa} \otimes \left( \frac{\lambda}{d_{a}} I_{a} \right) + E_{bb} \otimes \left( \frac{\lambda}{d_{b}} I_{b} \right)    \]
\[ \preceq \sum_{a} E_{aa} \otimes \frac{1+\eps+(k-1)\lambda}{d_{a}} I_{a}  \]

The same sequence of inequalities can be reversed to show a lower bound. So in fact we can show the above bounds on blocks shows $1+O(\eps + k \lambda)$-condition number bound on the Hessian in norm $\|\cdot\|_{d}$. 
\end{proof}


\subsection{Steifel concentration bound}
\subsubsection{Gaussian Version}
We will eventually compare all our bounds to gaussian inputs, so we will analyze marginal error for gaussians first.

\begin{lemma}
Let $X_{1}, ..., X_{n}$ be iid from $N(0,I_{d})$, then we have the following with failure probability $\leq \exp(-d/2)$
\[ \left\| \sum_{i} X_{i} X_{i}^{*} - n I_{d} \right\|_{op} \leq     3 \sqrt{\frac{d}{n}}  \]
\end{lemma}
\begin{proof}
The proof goes by a simple net argument. For an arbitrary $\xi \in S^{d-1}$ consider the MGF:
\[ \log \E \exp t \langle \xi \xi^{*}, \sum_{i} X_{i} X_{i}^{*} \rangle = \sum_{i} \log \E \exp t \langle \xi, X_{i} \rangle^{2} = \frac{-n}{2} \log (1 - 2t)     \]
The last line is because $\langle \xi, X \rangle^{2}$ is distributed as a chi-squared with one degree of freedom. For small $|t| < \frac{1}{4}$, we can approximate this by the second order term:
\[ \frac{-n}{2} \log (1 - 2t) = \frac{n}{2} \sum_{k \geq 1} \frac{(2t)^{k}}{k} \leq n t + 2 n t^{2}    \]
If we define $Z_{\xi} := \langle \xi \xi^{*}, \sum_{i} X_{i} X_{i}^{*} - n I_{d} \rangle$, then we have shown this random variable is $(4n, 4)$-subexponential. Using standard results on sub-exponential variables (i.e. Bernstein bound), we get
\[ Pr \left[ |\langle \xi \xi^{*}, \sum_{i} X_{i} X_{i}^{*}  - n I_{d} \rangle | \geq c \sqrt{n}  \right] \leq \exp( - c^{2} n / 8)     \]
So we can simultaneously ask for this bound for $\exp(O(n))$ many unit vectors. To lift this to a result on operator norm, for a fixed instantiation of $X$ we consider
\[ \nu := \left\| \sum_{i} X_{i} X_{i}^{*} - n I_{d} \right\|_{op} = \sup_{\xi \in S^{d-1}} |\langle \xi \xi^{*}, \sum_{i} X_{i} X_{i}^{*} - n I_{d} \rangle|    \]
Let $\xi'$ be the optimizer, and let $N$ be our $\eta$-net over $S^{d}$, i.e. every element of $S^{d}$ is $\eta$-close to some element in $N$. In particular $\xi' = \xi + \delta$ where $\xi \in N, \|\delta\|_{2} \leq \eta$.
\[ \nu = |\langle (\xi') (\xi')^{*}, \sum_{i} X_{i} X_{i}^{*} - n I_{d} \rangle| = |\langle (\xi + \delta) (\xi + \delta)^{*}, \sum_{i} X_{i} X_{i}^{*} - n I_{d} \rangle|      \]
\[ \leq |\langle \xi \xi^{*}, \sum_{i} X_{i} X_{i}^{*} - n I_{d} \rangle| + (2 \|\delta\|_{2} + \|\delta\|_{2}^{2}) \nu    \]
Rearranging terms, we see that for $\eta < \frac{1}{5}$, the $\sup$ over $N$ gives a 2-approximation for the $\sup$ over $S^{d-1}$. Therefore we can choose $|N| \leq \exp( d \log 10)$:
\[ Pr \left[ \left\| \sum_{i} X_{i} X_{i}^{*} - n I_{d} \right\|_{op} \geq c \sqrt{n}  \right] \leq Pr \left[ \sup_{\xi \in N} Z_{\xi} \geq 2 c \sqrt{n}  \right] \leq |N| \exp( - c^{2} n / 2)    \]
So choosing $c = 3 \sqrt{d/n}$ gives the result.
\end{proof}

\subsubsection{Stiefel Version}

Note if $d_{a} > d_{b}$, the error of the $a$ marginal will be much larger than that of the $b$ marginal. So we could try to analyze what happens after one iteration of flip-flop where we normalize the $a$ to identity. We claim our input if our input is $X_{1}, ..., X_{n} \subseteq \R^{d_{a} \times d_{b}}$ iid from $N(0,I_{a \times b})$, and we normalize so that the rows of $X$ form an orthogonal basis for $\R^{d_{a}}$, our new input $Y$ will be uniform from the Stiefel manifold of bases for $d_{a}$ dimensional subspaces of $\R^{nd_{b}}$. Given this we can try to run a net argument to bound the $b$ marginal and compare it to the gaussian case above:
\[ \mu_{b}(Y) = \sum_{i} X_{i}^{*} \left( \sum_{j} X_{i} X_{i}^{*} \right)^{-1} X_{i} - \frac{d_{a}}{d_{b}} I_{b}      \]
The normalization for the Identity term comes from comparing traces. Now if we consider
\[ Y := \left[ \left( \sum_{i=1}^{n} X_{i} X_{i}^{*} \right)^{-1/2} X_{1}, ..., \left( \sum_{i=1}^{n} X_{i} X_{i}^{*} \right)^{-1/2} X_{n}     \right]   \]
as a matrix in $\R^{d_{a} \times nd_{b}}$, then by our claim above its rows come from orthonormal bases of a uniformly random $d_{a}$ dimensional subspace of $\R^{n d_{b}}$. Therefore we can rewrite the variable we will consider a net over:
\[ \langle \xi \xi^{*}, \sum_{i} X_{i}^{*} \left( \sum_{i} X_{i} X_{i}^{*}  \right)^{-1} X_{i} \rangle = \langle Y^{*} Y, \xi \xi^{*} \otimes I_{n}   \rangle     \]
By the discussion above, $Y^{*} Y$ is distributed as a uniformly random orthogonal projection on $\R^{n d_{b}}$ of rank $d_{a}$. Since $\xi \xi^{*} \otimes I_{n}$ is unitarily equivalent to any rank $n$ orthogonal projection on $\R^{n d_{b}}$, we are left with calculating the MGF for the "overlaps" of two projections. This has already been done for us in the links commented below: specifically Lemma III.5 in \url{https://arxiv.org/pdf/quant-ph/0407049.pdf}, which cites Lemma II.3 in \url{https://arxiv.org/pdf/quant-ph/0307104.pdf} (thanks Michael!).

\begin{theorem} [Lemma III.5 in \url{https://arxiv.org/pdf/quant-ph/0407049.pdf}]
Our ambient space is $\R^{N}$, and we consider a fixed $n$ dimensional orthogonal projector $Q$. If $P$ is a uniformly random orthogonal projector of rank $a$, then we have
\[ Pr [ \langle P, Q \rangle \not\in (1 \pm \eps) \frac{an}{N} ] \leq \exp( - \Omega( an \eps^{2} ) )     \]
\end{theorem}

So in our case we have ambient space $\R^{n d_{b}}$ and set $Q = \xi \xi^{*} \otimes I_{n}$ for any $\xi \in \R^{d_{b}}$. 


\subsubsection{Spreading}
Because I like the vocabulary used in \url{https://arxiv.org/abs/2006.14009}, I will rewrite the proof for completeness using ideas of "spreading". I claim no originality, this is merely a translation of terms to fit in this theory.

\CF{Perhaps we can give some references for where spreading comes up to justify using the terminology instead of just putting it out there.}

\begin{definition} [Def 2.1]
We say that random variable $Y \in \R^{d}$ is a spread of random variable $X \in \R^{d}$ if
there exists a coupling such that $\E[Y |X] = X$.
\end{definition}

Another way to say this is if we can sample $Y$ by first sampling $X$, then sampling a martingale.

\begin{lemma} [Lemma 2.2]
If $Y$ is a spread of $X$, then for any convex $\phi : \R^{d} \to \R$ we have (by Jensen's inequality)
\[ \E_{Y} \phi(Y) = \E_{X} \E [ \phi(Y) \mid X ] \geq \E_{X} \phi(\E[Y \mid X]) = \E_{X} \phi(X)      \]
\end{lemma}

With this we can already compare the MGF's for gaussians vs the sphere:
\begin{corollary}
If $v \sim S^{N-1}$ and $g \sim N(0,I_{N})$ then for any $X \succeq 0$ we have
\[ \E_{S^{N-1}} \exp \langle X, v v^{*} \rangle \leq \E_{N(0,I_{N})} \exp \langle X, g g^{*} \rangle     \]
\end{corollary}
\begin{proof}
We just have to notice $g g^{*} = \|g\|_{2}^{2} \frac{g g^{*}}{\|g\|_{2}^{2}}$, so there is a coupling where we first sample a random direction $v$, and then sample the magitude of the gaussian. The MGF comparison follows since $\phi(Z) := \exp \langle Z, X \rangle$ is convex for $X \succeq 0$.
\end{proof}

The above corollary is the content of Lemma 2.3 of \url{https://arxiv.org/pdf/quant-ph/0307104.pdf}, and the proof strategy is exactly the same, I just added the spreading vocabulary. Similarly, since we want to compare MGFs of higher rank projections, there is a trick in Lemma 3.5 of \url{https://arxiv.org/pdf/quant-ph/0407049.pdf} that we can copy here with slightly different notation.
\begin{definition}
As a weakening, for $K \subseteq \R^{d}$ we say $Y \in \R^{d}$ is a spread of $X \in \R^{d}$ wrt $K$ if
there exists a coupling of $X,Y$ such that
\[ \forall \xi \in K:  \langle \xi, \E[Y |X] \rangle = \langle \xi, X  \rangle   \]
Note for $K = \R^{d}$, the definitions are the same.
\end{definition}
\CF{I would state the next lemma like "$Tr_{n}[u u^{*}]$ is a spread of $V V^{*}$ wrt $L(\R^{d})$, therefore (insert inequality)". This better motivates the additional terminology of spreading.}

\begin{corollary}
Let our vector space be $\R^{d} \otimes \R^{n}$, and consider $V \in \R^{d \times a}$ distributed an a uniformly random rank $a < n$ isometry on $\R^{d}$, and consider $u$ a uniformly random unit vector on $\R^{d} \otimes \R^{n}$. Then for any $L(\R^{d}) \ni X \succeq 0$:
\[ \E \exp \langle X, \frac{1}{a} V V^{*} \rangle \leq \exp \langle X, Tr_{n}[u u^{*}] \rangle      \]
\end{corollary}
\begin{proof}
We first exhibit the coupling between $V,u$. Let $v_{1}, ..., v_{a}$ be the columns of $V$, then lift to
\[ V' := \{v_{1} \otimes e_{1}, ..., v_{a} \otimes e_{a} \} \subseteq \R^{d} \otimes \R^{n}     \]
\[ u := V' 1_{a} = \sum_{i} v_{i} \otimes e_{i} \in \R^{d} \otimes \R^{n}      \]
Choosing $\{e_{i}\}$ as any orthonormal vectors does not change the partial trace  \CF{use $\tr$ not $Tr$} $V V^{*} = Tr_{n} [V' V'^{*}] = Tr_{n}[ u u^{*} ]$. As $V$ runs over all possible bases, and $\{e_{i}\}$ runs over all possible bases, we recover $u$ uniformly random over the sphere in $\R^{d} \otimes \R^{n}$. Finally since the partial traces are equal, we get that $u u^{*}$ is a spread of $V' V'^{*}$ wrt $\{X \otimes I_{n} \mid X \in L(\R^{d}) \}$; equivalently $Tr_{n}[u u^{*}]$ is a spread of $V V^{*}$ wrt $L(\R^{d})$. The comparison of MGFs then follows by Jensen's inequality.
\end{proof}

So we have the following sequence of inequalities for MGFs:
\[ \E \exp \langle Y^{*} Y, \xi \xi^{*} \otimes I_{n} \rangle \]
\[ \leq \E_{u \sim S^{d_{a} \times d_{a} \times n d_{b}}} \exp d_{a} \langle u u^{*}, \xi \xi^{*} \otimes I_{n} \otimes I_{d_{a}} \rangle \]
\[ \leq \E _{g \sim N(0,I_{d_{a} \times d_{a} \times n d_{b}})} d_{a}  \langle g g^{*}, \xi \xi^{*} \otimes I_{n} \otimes I_{d_{a}} \rangle       \]
The desired bound then follows from the gaussian part. We use the correct normalization to be consistent with the rest of the paper below.

\begin{corollary}
If $X_{1}, ..., X_{n}$ are iid from $N(0, \frac{1}{n d_{a} d_{b}} I_{d_{a} \times d_{b}})$, then let $Y$ be the output after one iteration of flip-flop, so $Y$ is uniformly distributed as $\frac{1}{d_{a}}$ times the $d_{a}$-dimensional Stiefel manifold on $\R^{n d_{b}}$. By definition $\mu_{a}(Y) = 0$. We have the following bound on $\mu_{b}(Y)$ with failure probability $\leq \exp(- \Omega(d_{b}))$:
\[ \left\| \sum_{i} Y_{i}^{*} Y_{i} - \frac{1}{d_{b}} I_{b}    \right\|_{op} \leq O(1) \frac{1}{d_{b}} \sqrt{ \frac{d_{b}}{n d_{a}}}   \]
\end{corollary}

%-----------------------------------------------------------------------------
\subsubsection{Net and union bound}
%-----------------------------------------------------------------------------
\TODO{do the net bound like in the vershynin survey}





%If $\langle \cdot , \cdot \rangle_P$ is the metric at a point $P$, then the new metric $\langle \cdot, \cdot \rangle_{R,P}$ will be given by
%$$ \langle X, Y \rangle_{R,P} = \langle R^{1/2} X R^{1/2}, R^{1/2} Y R^{1/2} \rangle_{P}.$$

%At the identity the metric takes the form $\langle X, Y \rangle_R = \tr \sqrt{R} X  \sqrt{R} Y $. In particular the length of the geodesic $t\mapsto \sqrt{P}e^{Xt}\sqrt{P}$ for $t \in [0,1]$ is $\langle X, X \rangle_R$.

\bibliographystyle{alpha}
\bibliography{refs}

\end{document}
