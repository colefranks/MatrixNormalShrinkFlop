\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{amsthm,mathtools,braket}
\usepackage{amssymb,bm, hyperref}
\usepackage{xcolor}
\usepackage[capitalize]{cleveref}
\newtheorem{theorem}{Theorem}
\newtheorem{corollary}{Corollary}
\newtheorem{prop}{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corol}[theorem]{Corollary}
\newtheorem{fact}[theorem]{Fact}
\newtheorem{claim}[theorem]{Claim}
\newtheorem{remark}{Remark}
\newtheorem{definition}{Definition}
\newtheorem{example}{Example}

\DeclareMathOperator{\Lie}{Lie}
\DeclareMathOperator{\Lin}{L}
\DeclareMathOperator{\ope}{op}

\DeclarePairedDelimiter{\abs}{\lvert}{\rvert}
\DeclarePairedDelimiter{\norm}{\lVert}{\rVert}
\DeclarePairedDelimiter{\ip}{\langle}{\rangle}


\newcommand{\junk}[1]{}


\newcommand{\R}{{\mathbb{R}}}
\newcommand{\C}{{\mathbb{C}}}
\newcommand{\ot}{\otimes}
\newcommand{\mat}{\operatorname{Mat}}
\newcommand{\ch}{\operatorname{ch}}
\newcommand{\cut}{\operatorname{cut}}
\newcommand{\vol}{\operatorname{vol}}
\newcommand{\Q}{{\mathbb{Q}}}
\newcommand{\N}{{\mathbb{N}}}
\renewcommand{\vec}{\bm}
\newcommand{\Z}{{\mathbb{Z}}}
\renewcommand{\S}{\mathbb{S}}

\newcommand{\E}{\mathbb{E}}
\renewcommand{\Pr}{\mathbb{P}}

\newcommand\eps{\varepsilon}

\newcommand\FF{\mathcal{F}}
\newcommand\HH{\mathcal{H}}
\newcommand\GG{\mathcal{G}}
\newcommand\SL{\operatorname{SL}}
\newcommand\PD{\operatorname{PD}}
\newcommand\Herm{\operatorname{Herm}}
\newcommand\GL{\operatorname{GL}}
\newcommand\TT{\mathcal{T}}
\newcommand\PP{\mathcal{P}}
\newcommand\tr{\operatorname{Tr}}
\newcommand\rk{\operatorname{rk}}
\newcommand\RR{\mathcal{R}}
\newcommand\CC{\mathcal{C}}
\newcommand\BB{\mathcal{B}}
\newcommand\II{\mathcal{I}}
\renewcommand\AA{\mathcal{A}}
\newcommand{\MM}{\mathcal{M}}
\newcommand\AP{\mathcal{AP}}

\newcommand{\eqdef}{:=}
\newcommand{\maps}{\colon}
\newcommand{\ale}{\lesssim}
\newcommand{\age}{\gtrsim}
\newcommand{\CF}[1]{{\color{purple}[CF: #1]}}
\newcommand{\AR}[1]{{\color{green}[AR: #1]}}
\newcommand{\RMO}[1]{{\color{red}[RMO: #1]}}
\newcommand{\TODO}[1]{{\color{blue}[TODO: #1]}}


\title{Logarithmic sample complexity for dense matrix models}
\author{Cole Franks, Rafael Oliveira, Akshay Ramachandran}
\date{February 2020}

\begin{document}

\maketitle
\tableofcontents
\TODO{
\begin{enumerate}
\item Use $n$ instead of $T$ for number of samples.
\item 
\end{enumerate}

}


\section{Introduction}
\CF{background, history; remark that we automatically know flip-flop works but our strong convexity will give efficient algorithms in this setting, as opposed to just converging algorithms. } 
The covariance matrix $\Sigma$ of a Gaussian random variable $X$ on $\R^D$ is a fundamental object in statistics, and its estimation is a central task in data analysis. If the number of samples is comparable to the dimension, the sample covariance matrix approximates $\Sigma$ well in spectral norm. However, one can still often obtain useful estimates of $\Sigma$ with far fewer samples under the assumption that $\Sigma$ is structured. Performing this task under various structural assumptions such as sparsity, rank constraints, bandedness, and many more is a well-studied problem\CF{cite}. Here we consider the family of distributions, known as the \emph{tensor normal model}, with covariance matrices that are Kronecker products of some matrices of known dimensions $d_1, \dots, d_k$ with $d_1d_2\dots d_k = D$.

Perhaps the best-known case is the case $k = 2$, called the \emph{matrix normal model} or  \emph{matrix normal variate model} \CF{cite drton, etc}. This model applies when each sample can be viewed as a matrix and the covariance between entries can be modeled as a product of a inter-row factor and a inter-column factor, such as data indexed by both space and time in magnetoencephalography \CF{cite}. \CF{figure out some uses for the tensor normal models}. 

Much research has been devoted to estimating the covariance in matrix and tensor normal models. Authors have proven that the MLE exists uniquely under mild assumptions \CF{cite}, and there is an iterative algorithm (known as the \emph{flip-flop} algorithm) that converges quickly to the MLE in practice \CF{dutilleul}. Through recent connections noted in \CF{cite philipp et al} it follows that the flip-flop algorithm converges whenever the MLE exists. Estimators with good performance have been shown to exist under the assumption that the true covariance matrix is highly sparse. 

Nonetheless, to our knowledge there exist no nonasymptotic bounds on convergence of the MLE, or any other tractable estimator, in the dense case. Moreover, it has not been proven that the flip-flop algorithm converges \emph{quickly} to the MLE, or even that there is a polynomial time to compute the MLE or any other \CF{asymptotically consistent?} estimator. We make significant progress on both of these open questions. As an informal summary, we show the following:
\begin{enumerate}
\item The MLE for the matrix normal model is close to the covariance in spectral norm for a number of samples that is \CF{check}
$$O(\max \{d_1/d_2, d_2/d_1\} \log \max \{d_1/d_2, d_2/d_1\}),$$ which is tight up to logarithmic factors.
\item The MLE for the tensor normal model converges is close to the covariance in Frobenius norm for a number of samples that is $O(\max_{i \in [k]} D/d_i^3)$. This result is tight.
\item Under the same sample requirements as above, the flip-flop algorithm converges exponentially quickly to the MLE with high probability. As a corollary, there is an algorithm to compute the  MLE with polynomial expected runtime.
\end{enumerate}
It remains an open question to show tight bounds for convergence of the MLE in spectral norm for the tensor normal model. 
\subsection{Notation}
\CF{temporary}
\begin{itemize}
\item Number of samples $n$, dimensions $d_1, \dots, d_k$. Whatever works for product of these, etc. 
\item $X$ for the tensor random variable, $X_1, \dots, X_n$ for the samples, $\vec X = (X_1, \dots, X_n)$ for the tuple of samples. 
\item $f_{\vec X}$ for the function in \cref{dfn:function}. 
\item $\PD_d$ for $d \times d$ pd's, $\Herm_d$ for $d \times d$ Hermitians. I don't think stats folks use this, I just really like them. Other suggestions welcome.
\item I'm going to call $G = \oplus \GL_{d_i}, P = \oplus \PD_{d_i}, H = \oplus \Herm_{d_i}$. Explain somewhere how H is the tangent space.
\item $\nabla, \nabla^2$ seems fine for geodesic Hessians and gradients. We'll explicitly mention that this is different than the usual ones.
\item anything else we think of. 
\end{itemize}
\subsection{Results}

\CF{Enough background material to state \cref{thm:matrix-normal,thm:tensor-frobenius,thm:matrix-flipflop,thm:tensor-flipflop}; maybe make some of these into 2-parters, explanation of tightness of results.}

We use the following notions of distance. The first arises naturally because it is within a constant factor of the total variation distance (assuming both are small enough constants), and the second because bounds on it typically strengthen bounds on the first. 

\begin{definition}[Distance]
Define 
\begin{align}d_{F}(\Sigma_1, \Sigma_2) = \| I - \Sigma_2^{1/2} \Sigma_1^{-1} \Sigma_2^{1/2}\|_F\\
d_{op}(\Sigma_1, \Sigma_2) = \| I - \Sigma_2^{1/2} \Sigma_1^{-1} \Sigma_2^{1/2}\|_{op}
\end{align}
\end{definition}
We may now state precisely our results for the matrix and tensor normal models.
\begin{theorem}[Matrix normal model spectral error]\label{thm:matrix-normal} The MLE $(\hat{\Sigma}_1,\hat{\Sigma}_2) $ for $(\Sigma_1, \Sigma_2)$ from $n$ independent samples satisfies 
$$ d_{op}(\hat{\Sigma}_i, \Sigma_i) = O(\TODO{}) $$
for $i = \{1,2\}$ with probability $\TODO{}$.
\CF{presumably there will be some dimension-dependent factors}.
\end{theorem}

\begin{theorem}[Tensor normal Frobenius error]\label{thm:tensor-frobenius} The MLE $(\hat{\Sigma}_1, \dots, \hat{\Sigma}_k) $ for $(\Sigma_1, \dots \Sigma_k)$ from $n$ independent samples satisfies 
$$ d_{op}(\hat{\Sigma}_i, \Sigma_i) = O(\TODO{}) $$
for $i \in [k]$ with probability $\TODO{}$.
\CF{presumably there will be some dimension-dependent factors}.
\end{theorem}


\begin{theorem}[Matrix flip-flop]\label{thm:matrix-flipflop} If $(\hat{\Sigma}_1, \hat{\Sigma}_2) $ denotes the MLE estimator for for $(\Sigma_1, \Sigma_2)$, then provided $n \geq \TODO{}$ the flip-flop algorithm computes $(\underline{\Sigma}_1, \underline{\Sigma}_2)$ with 
$$ d_F(\underline{\Sigma}_i, \hat{\Sigma}_i) \leq \eps $$
for $i \in \{1,2\}$ in $O(\TODO{\log(1/\eps)})$ iterations with probability at least $\TODO{}$.
\end{theorem}

\CF{make the same changes to the other guys - make sure it's the right thing}.

\begin{theorem}[Tensor flip-flop]\label{thm:tensor-flipflop} If $\hat{\Sigma}$ denotes the MLE estimator for $\Sigma$, then provided $n \geq \TODO{}$ the flip-flop algorithm computes $\underline{\Sigma}$ with 
$$ d_F(\underline{\Sigma}, \hat{\Sigma}) \leq \eps $$
in $O(\TODO{\log(1/\eps)})$ iterations with probability at least $\TODO{}$.
\end{theorem}
One may wonder why in the above theorems we consider the distances for the individual tensor factors and not the covariance matrix itself, but tight bounds for the covariance matrix itself follow from the above bounds (apart from the logarithmic factor in the matrix normal case and a constant factor in general).


\section{Tensor normal models}
\CF{the precise definitions of MLE's}
\CF{Show that we can reduce to the identity case for all these models}
\CF{give high level overview of proofs complete enough to motivate the next two subsections} 
We now give a precise definition of the MLE in the tensor normal model, in which $X$ is sampled according to $N(0, \Sigma = \bigotimes_{a = 1}^{k} \Sigma_a)$ for $\Sigma_a$ a positive-semidefinite $d_a\times d_a$ matrix. Let $D = \prod_{a \in [k]} d_a$. The matrix normal model is the case $k = 2$.

First, recall the maximum-likelihood estimator for $\Sigma$ without any structure. If $X_1, \dots, X_n \in \R^D$ are the samples from $N(0, \Sigma)$, then the log-likelihood for $\Sigma$ is (up to an additive constant) given by the function 
$$L(\Sigma^{-1}) = \sum_{i = 1}^n \langle X_i,  \Sigma X_i\rangle - \frac{1}{D} \log\det \Sigma.$$
If we are only interested in estimating $\Sigma$ only up to a constant, we may instead maximize 
\begin{gather}\tilde{L}(\Sigma^{-1}) = \log \sum_{i = 1}^n \langle X_i,  \Sigma X_i\rangle - \frac{1}{D} \log\det \Sigma. \label{eq:projective-likelihood}\end{gather}
and the solutions will differ only by a constant. Both likelihoods are geodesically convex, as will be defined precisely in the next section, but only the latter has geodesically Lipschitz gradients, i.e. is geodesically smooth \CF{cite bfg+}. This suggests it is more amenable to geodesically convex optimization, our main tool in this work.
\begin{remark}[Projective space] One may think of \cref{eq:projective-likelihood} as the MLE for $\Sigma$ after recieving $[X_1, \dots, X_n]$, the equivalence class of the tuple of samples in projective space.
\end{remark}
Restricting $\tilde{L}$ to $\Sigma$ of the form $\bigotimes_{a = 1}^{k} \Sigma_a$, we are led to consider the minima of the following function. 
\begin{definition}[Projective likelihood function]\label{dfn:function}
Given $\vec{X} = (X_1, \dots, X_n)$, let the function $f_{\vec X}:\bigoplus_{i} \PD_{d_i} \to \R$ be given by 
\begin{gather}f_{\vec X}(\Sigma_1, \dots, \Sigma_n) = \log \sum_{i = 1}^n \left\langle X_i,  \bigotimes_{i = 1}^k \Sigma_i X_i\right\rangle - \sum_{i = 1}^k \frac{1}{d_i} \log\det \Sigma_i. \label{eq:projective-likelihood}\end{gather}
define $\widehat{\Sigma}$ to be the minimizer of $f_{\vec X}$ in $\bigoplus_{i} \PD_{d_i}^1$ if it exists and is unique.
\end{definition}
When $\vec X$ is clear from context we will drop it. 
The choice of the coefficient $1/d_i$ is a matter of convenience and, because renormalizing $\Sigma_i$ changes $\Sigma$ only by a constant factor, does does not change the optima.


\subsection{Sample complexity from goedesic convexity} Following \cite{FM20}, and in spirit also \CF{cite weisel}, we use geodesic convexity to prove \cref{thm:tensor-frobenius}. \cref{thm:matrix-normal}, proven later in \cref{sec:matrix-normal}, requires the stronger notion of \emph{quantum expansion}.


We first define our notion of convexity. The function $f_{\vec X}$ has a property called \emph{geodesic convexity} \CF{cite bfg+}. 
\begin{definition}[Geodesic convexity]\CF{define $P,G,H$ before here} A function $f:P \to \R$ is said to be geodesically convex if  
$$f(A^\dagger e^{t Z} A)$$ is convex in $t$ for all $Z \in H$ and $A\in G$. Say $f$ is $\lambda$-geodesically \emph{strongly convex} w.r.t norm $\|\cdot\|$ if $\partial^2_t f(A^\dagger e^{t Z} A) \geq \lambda \|Z\|^{2}$. 
\end{definition} 
With this definition in place, we are able to state the rather simple plan. 
\begin{enumerate}
\item \textbf{Reduce to identity: } Firstly, note that $\widehat{\Sigma}(\vec X) = \Sigma^{1/2} \hat{\Sigma}(\Sigma^{-1/2} \vec X) \Sigma^{1/2}$. As $\Sigma^{-1/2} \vec X$ is distributed as $n$ samples from $\Sigma^{-1/2}X$ which is a standard Gaussian, the distance 
$$ d(\hat{\Sigma}(\vec X), I)$$
for $X$ standard Gaussian is distributed exactly as $d(\hat{\Sigma}(\vec X), \Sigma)$ for $X$ with covariance matrix $\Sigma$. This means it is enough to bound $d(\hat{\Sigma}(\vec X), I)$ assuming $\vec X$ is a standard Gaussian.
\item \textbf{Show strong convexity: } Show that, with high probability, $f_{\vec X}$ is $\Omega(1)$-geodesically strongly convex near $I$ w.r.t. a specific norm $\|\cdot\|$.
\item \textbf{Bound the gradient: } Show that $\nabla f_{\vec X} (I) $ is small with high probability (w.r.t. the dual norm $\|\cdot\|_{*}$). 

\end{enumerate}
These together imply the sample complexity bounds - as in the Euclidean case, strong convexity in a suitably large ball about a point implies the optimizer cannot be far. Furthermore, like in the Euclidean case, if a function is strongly geodesically convex in a region and one can prove a \emph{descent lemma} for a sequence in the region, then it must converge exponentially quickly to an optimizer. It is well-known that various optimization algorithms applied to $f_{\vec X}$ satisfy such a descent lemma \CF{cite bfg+}.

We now proceed to state the lemma from geodesically convex optimization we will need to prove the \cref{thm:tensor-frobenius}.

%\begin{definition}[Geodesic gradient]\TODO{definition of geodesic gradient and Hessian}\end{definition}



\begin{lemma} Let $f$ is geodesically convex everywhere, and $\lambda$-geodesically strongly convex w.r.t. norm $\|\cdot\|$ on a ball of radius $\kappa$ about $I$; further assume the geodesic gradient satisfies $\|\nabla f(I)\|_{*} = \eps < \lambda \kappa$. Then there is an optimizer within a $\|\cdot\|$-ball of radius $\eps/\lambda$. 
\end{lemma}
\begin{proof}
Take geodesic $e^{t Z}$ towards the optimum with normalized speed $\|Z\| = 1$ so the optimizer is $e^{T Z}$. Let $g(t) := f(e^{tZ})$, which is $\lambda$-strongly convex in $t$. 
\[ g'(0) = \langle \nabla f(I), Z \rangle \geq - \|\nabla f(I)\|_{*} \|Z\| \geq - \eps   \]
\[ 0 = g'(T) = \int_{t=0}^{T} g''(t) dt + g'(0) \geq \lambda \min\{\kappa,T\} - \eps     \]
Now if $T \geq \kappa$ we have a contradiction as $\lambda \kappa - \epsilon > 0$. Therefore we must have $\lambda T < \lambda \kappa \leq \eps$, and the conclusion follows by rearranging. 
\end{proof}
Next, we state the two theorems that allow us to apply this lemma. We prove them in the next two sections.

\begin{lemma}\label{lemma:gradient-bound} Suppose the number of samples is at least $\max_{a} d_{a}^{2} / D \epsilon^{2} $. Then with failure probability $\leq \sum_{a} \exp(-\Omega(d_{a}))$, 
$$\forall a: \| d_{a} (\nabla f(I))_{a} \|_{op} \leq \epsilon $$
as a consequence, $\|\nabla f(I)\|_{*}^{2} := \sum_{a} d_{a} \|(\nabla f(I))_{a} \|_{F}^{2} \leq k \epsilon^{2}$. 
\end{lemma}

\begin{theorem}\label{thm:tensor-convexity}
Suppose the number of samples is at least $k^{2} \max_{a} d_{a}^{2}/D$. Then with failure probability $\leq 1/poly(D)$, $f_X$ is $\Omega(1)$-strongly convex w.r.t. norm $\|Z\|^{2} := \sum_{a} d_{a} \|Z_{a}\|_{F}^{2}$. 
\end{theorem}
These two theorems immediately yield \cref{thm:tensor-frobenius}. 
\begin{proof}[Proof of \cref{thm:tensor-frobenius}]
\CF{TODO}.

\end{proof}



\subsection{Gradients and Hessians}

We now proceed to compute the gradient and Hessian of $f$, and then to prove the bound \cref{lemma:gradient-bound} on the gradient using basic matrix concentration results. 



\begin{fact}
The geodesic gradient and Hessians of $f_{\vec X}$ are as follows:
\begin{align}
\nabla f_{\vec X}(I), \nabla^2 f_{\vec X}(I) = \TODO{ \text{put whatever bound we'll be using so this can be black-boxed.}}
\end{align}
\end{fact}
\CF{I put all the blood and guts inside a proof environment. What ever is outside proof environments should be pretty readable.}
\begin{proof}
We will calculate geodesic gradient and Hessian of the standard tensor representation for the two functions: \TODO{make the notation match the function $f_{\vec X}$. Maybe swap $F$ and $f$ what follows to be consistent.}. 
\[ f_{v}\{g_{a}\} := \|(\otimes_{a} g_{a}) \cdot v\|_{2}^{2} = \langle v v^{*},\otimes_{a} g_{a}^{*} g_{a} \rangle ; \hspace{10mm}  F_{v} := \log f_{v}  \]
\[ \partial_{t=0} f(I_{\overline{a}} \otimes e^{tZ}) = \langle v v^{*}, I_{\overline{a}} \otimes e^{2tZ} 2 Z \rangle |_{t=0} = \langle v v^{*}, I_{\overline{a}} \otimes Z \rangle   \]
\[ \partial_{s=0} \partial_{t=0} f(I_{\overline{ab}} \otimes e^{tZ} \otimes e^{sY}) = 2 \partial_{s=0} \langle (I_{\overline{b}} \otimes e^{sY}) v v^{*} (I_{\overline{b}} \otimes e^{sY})^{*}, I_{\overline{a}} \otimes Z \rangle \]
\[ = \langle v v^{*}, (I_{\overline{b}} \otimes Y) (I_{\overline{a}} \otimes Z) + (I_{\overline{a}} \otimes Z) (I_{\overline{b}} \otimes Y) \rangle
\]
For simplicity, in this part we denote $Q = v v^{*}, s := Tr[Q]$ and marginals $Q^{S} := Tr_{\overline{S}}[Q]$. We will only need $\{Q^{a}\},\{Q^{ab}\}$
\[ \langle (\nabla f)_{a}, Z \rangle = 2 \langle Q^{a} - \frac{s}{d_{a}} I_{a}, Z \rangle  \]
\[ \langle Z, (\nabla^{2} f)_{aa}, Z \rangle = 2 \langle Q^{a}, Z^{2} \rangle   \]
\[ \langle Y, (\nabla^{2} f)_{ab}, Z \rangle = 2 \langle Q^{ab}, Z \otimes Y \rangle   \]

Now to calculate the same for $F_{v}$, note $\nabla F = \frac{\nabla f}{f}$ and $\nabla^{2} F = \frac{\nabla^{2} f}{f} - \frac{(\nabla f)(\nabla f)^{*}}{f^{2}}$. For simplicity we'll denote $\tilde{v} := \frac{v}{\|v\|_{2}}$ and abuse notation by keeping $Q = \tilde{v} \tilde{v}^{*}$ and marginals likewise. 
\[ \langle (\nabla F)_{a}, Z \rangle = 2 \langle Q^{a} - \frac{1}{d_{a}} I_{a}, Z \rangle   \]
\[ \langle X, (\nabla^{2} f)_{aa}, X \rangle = 2 \langle Q^{a}, Z^{2} \rangle - 4 \langle Q^{a} - \frac{1}{d_{a}} I_{a}, Z \rangle^{2} \]
\[ \langle Y, (\nabla^{2} f)_{ab}, Z \rangle = 2 \langle Q^{ab}, Z \otimes Y \rangle  - 4 \langle Q^{a} - \frac{1}{d_{a}} I_{a}, Z \rangle \langle Q^{b} - \frac{1}{d_{b}} I_{a}, Y \rangle  \]

By the following Cauchy-Schwarz, in the regime we care about for unit vector $v$, the two functions will be effectively the same:
\[ \frac{1}{4}\partial_{t=0}^{2} (F(\otimes_{a} e^{tZ_{a}}) - f(\otimes_{a} e^{tZ_{a}})) = \left( \sum_{a} \langle \nabla_{a}, Z_{a} \rangle \right)^{2}   \]
\[ \leq \left( \sum_{a} \|\nabla_{a}\|_{F} \|Z_{a}\|_{F} \right)^{2} \leq \left( \sum_{a} d_{a} \|\nabla_{a}\|_{F}^{2} \right) \left( \sum_{a} \frac{\|Z_{a}\|_{F}^{2}}{d_{a}} \right)   \]
\[ \leq \left( \sum_{a} \|\nabla_{a}\|_{op} \|Z_{a}\|_{1} \right)^{2} \leq \left( \sum_{a} (d_{a} \|\nabla_{a}\|_{op})^{2} \right) \left( \sum_{a} \frac{\|Z_{a}\|_{F}^{2}}{d_{a}} \right)  \]

\end{proof}





 Recall again that in our setting $Q^{a}$ is the sum of $N := \frac{nD}{d_{a}}$ copies of $X X^{*}$ where $X \sim \mathcal{N}(0,\frac{1}{nD} I_{a})$. Note first the following fact which allows us to use these specialized inequalities

\begin{fact}
$\sum_{i=1}^{N} X_{i} X_{i}^{*} \equiv G G^{*} $ where $G := \{X_{1}, ..., X_{N}\}$. 
This means if we denote $\{\lambda_{1}, ..., \lambda_{d}\}$ the spectrum of $\sum_{i=1}^{N} X_{i} X_{i}^{*}$, this is the same as $\{s_{1}^{2}, ..., s_{d}^{2}\}$ where $s_{j} := s_{j}(G)$ the $j$-th singular value. By Taylor expansion of $\sqrt{1+x}$ we have:
\[ \lambda_{1},\lambda_{d}(GG^{*}) \in \frac{1}{d_{a}} \left( 1 \pm \frac{1}{\log d_{a}} \right) \iff s_{1},s_{d}(G) \in  \frac{1}{\sqrt{d_{a}}} \left( 1 \pm \frac{1}{\log d_{a}} \right)  \]
\end{fact}

% https://arxiv.org/pdf/1011.3027.pdf

\begin{corollary} [Corollary 5.35]\label{cor:vershynin}
Let $G_{d,N} \in \R^{d \times N}$ for $d < N$ have independent standard gaussian entries. Then for $t \geq 0$, the following occurs with $\leq 2 \exp(-t^{2}/2)$ failure probability:
\[ \sqrt{N} - \sqrt{d} - t \leq s_{d}(G) \leq s_{1}(G) \leq \sqrt{N} + \sqrt{d} + t  \]
\end{corollary}

\begin{corollary}
If $nD \gtrsim d_{a}^{2} \log^{2} d_{a}$ then $\|Q^{a} - \frac{1}{d_{a}} I_{a} \|_{op} \ll \frac{1}{d_{a} \log d_{a}}$ with failure probability $\leq \exp( - \Omega(d_{a}))$
\end{corollary}
\begin{proof}
We have the following with $\leq 2 \exp(-t^{2}/2)$ failure probability:
\[ s_{1},s_{d}\left( \frac{1}{\sqrt{nD}} G_{d,N} \right) \in \frac{1}{\sqrt{nD}} \left( \sqrt{\frac{nD}{d_{a}}} \pm (\sqrt{d_{a}} + t) \right) = \frac{1}{\sqrt{d_{a}}}\left( 1 \pm \frac{d_{a} + t \sqrt{d_{a}}}{\sqrt{nD}}  \right)  \]
Choosing $t \sim \sqrt{d_{a}}$ and $nD \gtrsim d_{a}^{2} \log^{2} d_{a}$ gives the required bound. 
\end{proof}
We may finally prove the bound on the gradient.
\begin{proof}[Proof of \cref{lemma:gradient-bound}]
\TODO{put guts here}.
\end{proof}

\subsection{Strong convexity}
\TODO{Pisier + perturbation bound}

\begin{theorem}
Let $P$ denote the projection onto $(\vec{I_{n}},\vec{I_{m}})$, then the following holds with $1/poly(mn)$ failure probability
\[ \|\sum_{i=1}^{N} \alpha_{i} Y_{i} \otimes Y_{i} (I - P) \|_{op} \leq O \left( \sqrt{\sum_{i} \alpha_{i}^{2}} \right) \left( \E \|Y\|_{op} \right)^{2} \]
\end{theorem}

\begin{corollary}
For $Y \in \R^{d_{a} \times d_{b}}$ where $Y_{ij} \sim \mathcal{N}(0,1)$, our Kraus operator is of the form $\frac{1}{\sqrt{TD}} Y$, and our off-diagonal operator is
\[ \sum_{k=1}^{TD/d_{a}d_{b}} \frac{1}{TD} Y_{k} \otimes Y_{k}   \]
By the above theorem, the operator norm on the subspace $\perp$ to $(I_{a},I_{b})$ is of the order $\frac{d_{a}+d_{b}}{\sqrt{d_{a} d_{b} TD}}$, so for constant expansion it suffices that $TD \gg \max_{a} d_{a}^{2}$
\end{corollary}

\begin{proof} [Proof of \cref{thm:tensor-convexity}]
Take any quadratic form of the Hessian for $\{Z_{a} \perp I_{a}\}$:
\[ \partial_{t=0}^{2} f(e^{t Z}) = \sum_{a} \langle Q^{a}, Z_{a}^{2} \rangle + \sum_{a \neq b} \langle Q^{ab}, Z_{a} \otimes Z_{b}    \]
\[ \geq \sum_{a} \lambda_{\min}(Q^{a}) \|Z_{a}\|_{F}^{2} - \sum_{a \neq b} \|Q^{ab} (I-P_{ab})\|_{op} \|Z_{a}\|_{F} \|Z_{b}\|_{F}    \]
Now we can use our high probability bounds derived above:
\[ \forall a: Q^{a} \succeq \frac{1-\epsilon}{d_{a}} I_{a}; \hspace{10mm} 
\forall a \neq b: \|Q^{ab} (I-P_{ab})\|_{op} < \frac{\lambda}{k \sqrt{d_{a} d_{b}}}   \]
\[ \implies \partial_{t=0}^{2} f(e^{t Z}) > \sum_{a} \frac{1-\eps - \lambda}{d_{a}} \|Z_{a}\|_{F}^{2}   \]
Choosing $\eps,\lambda$ small enough gives the theorem. 
\end{proof}

\section{Matrix normal models}\label{sec:matrix-normal}
\TODO{Proof of \cref{thm:matrix-normal}. Prove bounds on $d_{op}(\hat{\Sigma}, I)$ assuming $X$ standard normal.}
\subsection{Quantum expansion}
\TODO{Cheeger proof of expansion, also cite Pisier}

\section{Convergence of flip-flop algorithms}
\TODO{General descent-lemma + strong convexity folklore}
\subsection{Tensor flip-flop convergence}
\TODO{Proof of \cref{thm:tensor-flipflop}; vanilla strong convexity in a sublevel set stuff.}
\subsection{Matrix flip-flop convergence}
\TODO{Proof of \cref{thm:matrix-flipflop}}
\CF{prove that flip-flop works as soon as estimation in operator norm works. This goes by using KLR to show strong convexity holds in a ball about the optimizer}.









\section{Open problems}
\TODO{$d_{op}$} for tensors??








\appendix 


%One can calculate $$ H_{I, v}(X, X) = \langle w, \Pi(X)^2 w \rangle - \langle w, \Pi(X) w \rangle^2 $$ where $w = v/\|v\|$. We may calculate the Hessian $H_{P,v}$ using $H_{P, v} = H_{I, \sqrt{P}v}$. 


\section{Operator and tensor scaling}\label{sec:scaling}
\TODO{lower bound Hessian for operators and tensors for all different formats; we hope to get strong convexity with $\prod d_i /(d_1^2 + \dots + d_k^2)$ samples. I am concerned that a KLR19-style operator norm type theorem is needed to get $\tilde{O}$ of this, but we will do what we can with the Frobenius bounds for now; I'd expect to need at least $\max_i \sqrt{d_i}$ too many samples.}\\
\TODO{It would also be nice to have that tight example for the $\log$ in KLR19...}

We recall the moment map and Hessian calculations
\[ \partial_{t=0} f(e^{tX_{a}}) = \langle \nabla_{a}, X \rangle = \langle Q^{a} - sI_{a}, X \rangle   \]
\[ \partial_{t=0}^{2} f(e^{tX_{a}}) = \langle X, (\nabla^{2})_{aa} X \rangle = \langle Q^{a}, X^{2} \rangle  \]
\[ \partial_{s=0} \partial_{t=0} f(e^{tX_{a}} \otimes e^{sY_{b}}) = \langle Y, (\nabla^{2})_{ab} X \rangle = \langle Q^{ab}, X \otimes Y \rangle   \]

\section{Operator Scaling}
In this section we have $n$ samples of $X \sim N(0,\frac{1}{n} (\frac{1}{d_{1}} I_{1}) \otimes (\frac{1}{d_{2}} I_{2}))$. We will denote $D := d_{1} d_{2}$. In order to use the KLR analysis, we will show that the one-body marginals have low error in $\|\cdot\|_{op}$ and the whole operator is a sufficient expander at the start. 

\subsection{Bernstein Proof of $\|\mu\|_{op}$}
This is proven using matrix concentration

\begin{theorem} [Bernstein]
Consider independent $\{X_{k}\}$ such that $\E X_{k} = 0$ and $\lambda_{max}(X_{k}) \leq R$ almost surely. Further let the variance be $\sigma^{2} := \|\sum_{k} \E X_{k}^{2} \|_{op}$. 
\begin{eqnarray*} \Pr [ \lambda_{max} \left( \sum_{k} X_{k}  \right) \geq t ] & \leq & d \exp\left( - \frac{\Omega(t^{2})}{\sigma^{2} + t R} \right)
\\ & \leq & \begin{cases} 
d \exp ( - \Omega(t^{2}/\sigma^{2}) ) & \text{if $t \leq \sigma^{2}/R$ } 
\\ d \exp ( - \Omega(t/R) )           & \text{if $t \geq \sigma^{2}/R$}
\end{cases} 
\end{eqnarray*}
\end{theorem}

In our setting, $Q^{a}$ is comprised of $N := \frac{TD}{d_{a}}$ copies of a rank one $g g^{*}$ where each gaussian is $g \sim \mathcal{N}(0, N^{-1} \frac{1}{d_{a}} I_{a} ) = \mathcal{N}(0, \frac{1}{TD} I_{a}) $. We will drop subscripts for $d_{a}, I_{a}$ etc when they can be understood from context. Therefore we define $X := g g^{*} - \frac{1}{TD} I_{a}$ and note the following parameters:
\[ \lambda_{max}(X) = \|g\|_{2}^{2} - \frac{1}{TD} \hspace{10mm} \lambda_{min}(X) = - \frac{1}{TD}   \]
While $\|g\|_{2}$ is unbounded, we can threshold our distribution with a small loss in probability. Since we will be using $\chi^{2}$ distributions much from now on, we will do a quick exercise to prove our threshold bounds:

\begin{definition}
$\chi(\mu,d)$ denotes the $\chi^{2}$ distribution with mean $\mu$ and $d$ degrees of freedom. Explicitly $X \sim \chi(\mu,d) \implies X = \frac{\mu}{d} \sum_{i=1}^{d} g_{i}^{2}$
where $g \sim \mathcal{N}(0,1)$. 
\end{definition}

\begin{lemma}
For $X \sim \chi(\mu,d)$ we have the following (explicit and approximate) formula for the MGF, $\forall \theta < \left(O(\frac{\mu}{d}) \right)^{-1}$:
\begin{eqnarray*} \log \E \exp(\theta X) & = & - \frac{d}{2} \log \left(1 - 2 \theta \frac{\mu}{d} \right) 
\\ & \leq & \theta \mu + \theta^{2} \frac{O(\mu^{2})}{2 d}
\end{eqnarray*}
\end{lemma}

\begin{theorem} [Sub-exp variables]
The above MGF bound gives tail decay:
\[ \forall \theta < b^{-1}: \log \E \exp(\theta (X - \E X)) \leq \theta^{2} \frac{\sigma^{2}}{2} \]
\[ \implies  \Pr[X - \mu \geq t] \leq \begin{cases}
\exp( - \Omega(t^{2}/\sigma^{2}) ) & t \leq \sigma^{2}/b
\\ \exp( - \Omega(t/b) ) & t \geq \sigma^{2}/b
\end{cases}   \]
\end{theorem}

With these bounds in mind, note our variables $\|g\|_{2}^{2} \sim \chi(\frac{d_{a}}{TD},d_{a})$ so we have $\sigma^{2} = \frac{d}{(TD)^{2}}, b = \frac{1}{TD} \implies \sigma^{2}/b = \frac{d}{TD}$
\[ \Pr[ \exists k: \lambda_{max}(X_{k}) \geq M \sqrt{\log N}\frac{d}{TD} ] \leq \exp( - \Omega(M^{2}) )  \]
If we're happy with $1/poly$ failure probability we will take $M^{2} \sim \log D$, so in our matrix bound $R_{max} \leq \frac{d \log D}{TD}$

\[ \E X^{2} = \E (g g^{*})^{2} - \frac{1}{(TD)^{2}} I = \E \|g\|_{2}^{4} \hat{g} \hat{g}^{*} - \frac{1}{(TD)^{2}} \]
\[ = \frac{1}{(TD)^{2}} ( (3d + d(d-1)) \frac{1}{d} I - I ) = \frac{d+1}{(TD)^{2}} I   \]
Here $\hat{g} := g / \|g\|_{2}$ and the calculation is done by independence of $\|g\|_{2}, \hat{g}$. So we also have the variance parameter
\[ \sigma^{2} = N \|\E X^{2}\|_{op} = \frac{TD}{d} \frac{d+1}{(TD)^{2}} \sim \frac{1}{TD}  \]

\begin{corollary}
We have the following operator norm concentration
\[ \Pr[ \|Q^{a} - sI_{a}\|_{op} \geq t ] \leq d \exp \left( - \frac{\Omega(t^{2} TD)}{1 + t d_{a} \log D }  \right)  \]
Since we require $\|\cdot\|_{op}$ error $\ll \frac{1}{d_{a} \log D}$, if we are happy with $1/poly$ failure probability we require $TD \gg \max_{a} d_{a}^{2} \log^{3} D$. 
\end{corollary}

\begin{remark} Note I'm using $\min_{a} d_{a} < \max_{a} d_{a} < D$ in a couple places so the $\log$ term may be slightly sharpened. But the exponent is tight as we require $TD > \max_{a} d_{a}^{2}$ samples for existence/ uniqueness of the solution. 
\end{remark}


\subsection{Gaussian proof of $\|\mu\|_{op}$}
The above method of first thresholding the gaussians then using Bernstein-style concentration on a bounded random matrix feels a bit square-peg round-hole - y. Turns out there are better results specifically for the case of gaussian matrices. Recall again that in our setting $Q^{a}$ is the sum of $N := \frac{nD}{d_{a}}$ copies of $X X^{*}$ where $X \sim \mathcal{N}(0,\frac{1}{nD} I_{a})$. Note first the following fact which allows us to use these specialized inequalities

\begin{fact}
$\sum_{i=1}^{N} X_{i} X_{i}^{*} \equiv G G^{*} $ where $G := \{X_{1}, ..., X_{N}\}$. 
This means if we denote $\{\lambda_{1}, ..., \lambda_{d}\}$ the spectrum of $\sum_{i=1}^{N} X_{i} X_{i}^{*}$, this is the same as $\{s_{1}^{2}, ..., s_{d}^{2}\}$ where $s_{j} := s_{j}(G)$ the $j$-th singular value. By Taylor expansion of $\sqrt{1+x}$ we have:
\[ \lambda_{1},\lambda_{d}(GG^{*}) \in \frac{1}{d_{a}} \left( 1 \pm \frac{1}{\log d_{a}} \right) \iff s_{1},s_{d}(G) \in  \frac{1}{\sqrt{d_{a}}} \left( 1 \pm \frac{1}{\log d_{a}} \right)  \]
\end{fact}

% https://arxiv.org/pdf/1011.3027.pdf

\begin{corollary} [Corollary 5.35]\label{cor:vershynin}
Let $G_{d,N} \in \R^{d \times N}$ for $d < N$ have independent standard gaussian entries. Then for $t \geq 0$, the following occurs with $\leq 2 \exp(-t^{2}/2)$ failure probability:
\[ \sqrt{N} - \sqrt{d} - t \leq s_{d}(G) \leq s_{1}(G) \leq \sqrt{N} + \sqrt{d} + t  \]
\end{corollary}

\begin{corollary}
If $nD \gtrsim d_{a}^{2} \log^{2} d_{a}$ then $\|Q^{a} - \frac{1}{d_{a}} I_{a} \|_{op} \ll \frac{1}{d_{a} \log d_{a}}$ with failure probability $\leq \exp( - \Omega(d_{a}))$
\end{corollary}
\begin{proof}
We have the following with $\leq 2 \exp(-t^{2}/2)$ failure probability:
\[ s_{1},s_{d}\left( \frac{1}{\sqrt{nD}} G_{d,N} \right) \in \frac{1}{\sqrt{nD}} \left( \sqrt{\frac{nD}{d_{a}}} \pm (\sqrt{d_{a}} + t) \right) = \frac{1}{\sqrt{d_{a}}}\left( 1 \pm \frac{d_{a} + t \sqrt{d_{a}}}{\sqrt{nD}}  \right)  \]
Choosing $t \sim \sqrt{d_{a}}$ and $nD \gtrsim d_{a}^{2} \log^{2} d_{a}$ gives the required bound. 
\end{proof}


\subsection{Cheeger proof of expansion}
\CF{I am putting this in the operator language; perhaps it would be more direct in tensor language.}
Here we show that the Hessian at the identity is bounded below by a constant with inverse exponential failure probability. 
\begin{theorem}\label{thm:operator-cheeger}
There are absolute constants $c, C$ such that the following holds for all $d_1 \leq d_2$. If the entries of the $n$ Kraus operators of $\Phi$ are drawn from standard Gaussians, then $\Phi$ is $C\sqrt{d_2/n d_1}$-balanced and a $1 - c$ quantum expander with probability $e^{ - \Omega( nd_1)}$ provided $n \geq C\max\{(d_2/d_1) \log (d_2/d_1)\}$. 
\end{theorem}
\CF{I need to check the failure probabilities for being balanced again.}

To do this, we first define the Cheeger constant of an operator $\Phi:\mat(d_1) \to \mat(d_2)$. This is similar to a concept defined in \cite{H07}.
\begin{definition}
Let $\Phi : \mat(d_1) \to \mat(d_2)$ be a completely positive map. The Cheeger constant $\ch(\Phi)$ of the weighted bipartite graph associated to $B$ is given by 
$$\ch(\Phi):=\min_{\Pi_1, \Pi_2: \vol(\Pi_1, \Pi_2) \leq \tr \Phi} \phi(\Pi_1,\Pi_2)$$
where $\Pi_1: \C^{d_1} \to \C^{d_1}$ and $\Pi_1: \C^{d_2} \to \C^{d_2}$ are orthogonal projections and the \emph{conductance} $\phi$ of the cut $\Pi_1, \Pi_2$ is defined to be
$$\phi(\Pi_1,\Pi_2) := \frac{\cut(\Pi_1, \Pi_2)}{\vol(\Pi_1,\Pi_2)}$$
where 
%$$ \vol(\Pi_1,\Pi_2):= \sum_{i \in T, j \in [d_2]} b_{ij} + \sum_{i \in [d_1], j \in S} b_{ij}\textrm{ and } \cut(S, T):= \sum_{i \not\in T, j  \in S} b_{ij} + \sum_{i \in T, j \not\in S} b_{ij}.$$
$$ \vol(\Pi_1,\Pi_2):= 
\tr \Phi(\Pi_1) + \tr \Phi^*(\Pi_2)$$ 
and $$ \cut(\Pi_1, \Pi_2):= \tr \Pi_2 \Phi(I_{d_1} - \Pi_1) + \tr (I_{d_2} - \Pi_2) \Phi(\Pi_1).$$
\end{definition}

We now cite a slight generalization of \cite{FM20}. 
%Recall the function $$f^{\Phi}:X \mapsto \frac{d_1}{d_2} \log\det(\Phi(X)) - \log\det (X).$$

\begin{lemma} [\cite{FM20}, \cite{KLR19}]\label{lem:op-cheeger} There exist absolute constants $c, C$ such if $\eps < c \ch(\Phi)^2$ and $\Phi$ is $\eps$-balanced, then $\Phi$ is a 
$$ \max\left\{1/2, 1 -  \ch(\Phi)^2 + C \frac{\eps}{\ch(\Phi)^2} \right\}$$
quantum expander.
\end{lemma}

We proceed to bound the Cheeger constant of a random operator. The Cheeger constant of an operator is scale-invariant, so for convenience we let $\Phi$ have Kraus operators $A_1, \dots, A_n$, each drawn from $N(0,  I_{d_1} \ot I_{d_2}).$ Our main observation is the following. 
\begin{fact}\label{fact:chi} Let $\Pi_1:\C^{d_1} \to \C^{d_1}, \Pi_2: \C^{d_2} \to \C^{d_2}$ be orthogonal projections, of rank $r_1, r_2$, respectively. Then $\cut(\Pi_1, \Pi_2), \vol(\Pi_1, \Pi_2), \vol(I_{d_1}, I_{d_2})$ is jointly distributed as 
$$ X, X + 2Y, 2X + 2 Y + 2Z$$ where 
$X, Y, Z$ are independent $\chi^2$ random variables with $F_1:=n r_1(d_2 - r_2) + n r_2(d_1-r_1), F_2:= n r_1r_2, F_3:= n(d_1 - r_1)(d_2 - r_2)$ degrees of freedom, respectively. 
\end{fact} 
\begin{proof} As the distribution of $\Phi$ is invariant under the action of unitaries, the distribution of $\cut(\Pi_1, \Pi_2), \vol(\Pi_1, \Pi_2)$ depends only on the rank of $\Pi_1, \Pi_2$. Thus we may compute in the case that $\Pi_1, \Pi_2$ are coordinate projections, in which case one may verify the fact straightforwardly. 
\end{proof}


 We show a sufficient condition for the Cheeger constant being bounded away from $1$ that is amenable to the previous distributional description.
\begin{lemma}\label{lem:suff}
Let $r_1, r_2$ be the ranks of projections $\Pi_1: \C^{d_1} \to \C^{d_1}, \Pi_2: \C^{d_2} \to \C^{d_2}$, and let $F_1:= n r_1(d_2 - r_2) + n r_2(d_1-r_1)$ and $F_2:=n r_1r_2.$ If 
\begin{itemize}
\item for all $\Pi_1, \Pi_2$ such that $F_2 \geq (4/9) n d_1 d_2$ we have 
\begin{gather}\vol(\Pi_1, \Pi_2) \geq \frac{1}{2} \vol(I_{d_1}, I_{d_2}),\label{eq:vol}\end{gather} and
\item for all $\Pi_1, \Pi_2$ such that $F_2 < (4/9) n d_1 d_2$, we have 
\begin{gather} \vol(\Pi_1, \Pi_2) \leq (4/3)(F_1 + 2 F_2) \textrm{ and } \cut(\Pi_1, \Pi_2) \geq (2/3) F_1,\label{eq:cut} \end{gather}
\end{itemize}
then $\ch(\Phi) \geq 1/6$. 
\end{lemma}
\begin{proof} By the first assumption, it remains to show that $F_1/(F_1 + 2 F_2) \geq 1/3$ provided $F_2 < (4/9) n d_1 d_2$, or $r_1 r_2 < (4/9) d_1 d_2$. Indeed, 
\begin{align*}F_1/(F_1 + 2 F_2) &= \frac{r_1 d_2 + r_2 d_1 - 2 r_1 r_2}{r_1 d_2 + r_2 d_1}\\
 &= 1 -2 \sqrt\frac{ r_1 r_2}{d_1 d_2} \frac{1}{ \sqrt{ r_1 d_2/r_2 d_1} + \sqrt{r_2 d_1/ r_1 d_2}} \\
 &\geq 1 - \sqrt{4/9} = 1/3.
\end{align*}

In the last inequality we used that $a + a^{-1} \geq 2$ for all $a \in \R_+$ and that $r_1 r_2 < (4/9) d_1 d_2$. \end{proof}


Next we use this to show that for fixed $\Pi_1, \Pi_2$, with high probability the events in \cref{lem:suff} hold. 
\begin{lemma}\label{lem:probabilities}
Let $r_1, r_2$ be the ranks of projections $\Pi_1: \C^{d_1} \to \C^{d_1}, \Pi_2: \C^{d_2} \to \C^{d_2}$, and let $F_1:= n r_1(d_2 - r_2) + n r_2(d_1-r_1)$ and $F_2 = n r_1 r_2$. Then 
\begin{itemize}
\item if $F_2 \geq (4/9) n d_1 d_2$, then \cref{eq:vol} holds with probability at least $1 - e^{-\Omega( n d_1 d_2)}$. 
\item else, \cref{eq:cut} holds with probability at least $1 - e^{-\Omega( F_1)}$. 
\item Finally, $\vol(\Pi_1, \Pi_2) \geq \frac{1}{2}\tr \Phi(I_{d_1}) (d_1/d_2)$ with probability at least $1 - e^{- \Omega(n r_1 d_2 + n r_2 d_1)}$.
\end{itemize}
\end{lemma}


\begin{proof}
Recall from \cref{fact:chi} that, $\cut(\Pi_1, \Pi_2), \vol(\Pi_1, \Pi_2), \vol(I_{d_1}, I_{d_2})$ are jointly distributed as $X, X + 2Y, 2X + 2Y + 2Z$ for $X, Y, Z$ independent $\chi^2$ random variables with $F_1, F_2, F_3$ degrees of freedom, respectively. Thus it is enough to show that 
\begin{itemize}
\item If $nr_1 r_2 \geq (4/9) n d_1 d_2$, then with probability $1 - e^{- \Omega( n d_1 d_2)}$ we have $Y > Z$, and 
\item if $nr_1 r_2 \leq (2/3) n d_1 d_2$, then with probability $1 - e^{- \Omega(F_1)}$ we have $X \geq (2/3) F_1$ and $X + 2Y \leq (4/3) (F_1 + 2 F_2),$ 
\item and with probability $1 - e^{- \Omega(F_1 + 2 F_2)}$, $X + 2Y \geq (2/3) (F_1 + 2 F_2) = (2/3) n (r_1 d_2 + r_2 d_1)$ and $X + Y + Z \leq (4/3)(F_1 + F_2 + F_3) = (4/3)n d_1 d_2$. 
\end{itemize}
All three follow from standard results for concentration of $\chi^2$ random variables \cite{W19}. To prove the first item, first note that $F_1 + 2 F_2 \geq (4/3)(F_1 + F_2 + F_3)$, because 
\begin{align*}
(F_1 + 2 F_2)/( F_1 + F_2 + F_3) &= \frac{r_1}{d_1} + \frac{r_2}{d_2}\\
 &= \sqrt{ \frac{r_1 r_2}{d_1 d_2}}\left( \sqrt{ \frac{r_1 d_2}{r_2 d_1}} + \sqrt{ \frac{r_2 d_1}{r_1 d_2}}\right) \geq (2/3) \cdot 2 \geq 4/3.
\end{align*}
In particular, $F_2 \geq (2/3)(F_2 + F_3)$. Thus, with probability $1 - e^{- c F_2}$, $Y \geq (5/9) (F_2 + F_3)$ and $Y + Z \leq (10/9) (F _2 + F_3),$ so $Y > Z$ with probability $1 - e^{- c F_2} \geq 1 - e^{- c n d_1 d_2}$. The second and third items are straightforward. 
\end{proof}
Finally, we show using an epsilon net that the Cheeger constant is high for \emph{all} projections. 
\begin{lemma}[\cite{FM20}]\label{lem:net} There is a $\delta$-net $N$ of the rank $r$ orthogonal projections $\Pi: \C^d \to \C^d$ with $|N| = \exp(O(d r |\ln \delta|))$.
\end{lemma}
As a corollary, the number of pairs of projections $\Pi_1, \Pi_2$ of rank $r_1, r_2$ has a $\delta$-net of size on the order of $(r_1 d_1 + r_2 d_2) |\ln \delta|$. 

\begin{lemma}[A net suffices]\label{lem:net-suffices}
Suppose $\|\Pi'_1 -\Pi_2\|_F, \|\Pi'_2 - \Pi_2\|_F \leq \delta$. Then 
\begin{align*} |\cut(\Pi_1, \Pi_2) - \cut(\Pi'_1, \Pi'_2)| \leq4\delta \tr \Phi(I_{d_1})\\
\textrm{ and }|\vol(\Pi_1, \Pi_2) - \vol(\Pi'_1, \Pi'_2)| \leq 4\delta \tr \Phi(I_{d_1}).
\end{align*}
\end{lemma}
\begin{proof}
We first show the first inequality.
\begin{align*}|\cut(\Pi'_1, \Pi'_2) - \cut(\Pi_1, \Pi_2)| & \leq |\tr \Pi'_2 \Phi(I_{d_1} - \Pi'_1) - \tr \Pi_2 \Phi(I_{d_1} - \Pi_1)|\\
&  + |\tr (I_{d_2} - \Pi'_2) \Phi(\Pi'_2) - \tr (I_{d_2} - \Pi_2) \Phi(\Pi_2)|.
\end{align*}
We begin with the first term. 
\begin{align*}|\tr \Pi'_2 \Phi(I_{d_1} - \Pi'_1) - \tr \Pi_2 \Phi(I_{d_1} - \Pi_1)| &= |\tr (\Pi'_2 - \Pi_2) \Phi(I_{d_1} - \Pi'_1) + \tr \Pi_2 \Phi(\Pi_1 - \Pi'_1)|\\
&\leq \delta\| \Phi(I_{d_1} - \Pi'_1)\|_F + \delta\| \tr \Phi^*(\Pi_2)\|_F\\
& \leq 2 \delta \tr \Phi(I_{d_1}).
\end{align*}
The second term follows by symmetry. The proof of the second inequality is similar.
\end{proof}

\begin{lemma}[Applying union bound]\label{lem:union}
Let $d_1 < d_2$. Suppose $n \geq C \frac{d_2}{d_1} \log (d_1 d_2)$. Then with probability at least $1 - \exp( - \Omega(n d_1))$, we have $\ch(\Phi) = \Omega(1)$.
\end{lemma}
\begin{proof} Let $\delta \leq \CF{do}$. Let $N(r_1, r_2)$ be $\delta$-net for the pairs of projections of rank $r_1, r_2$, respectively, with $|N(r_1, r_2)| = e^{O((d_1r_1 + d_2 r_2) \log(1/\delta))}$, and $N = \bigcup_{r_1, r_2} N(r_1, r_2)$. By \cref{lem:net-suffices}, it is enough to show that with probability $\exp( - c n (d_1 + d_2))$ for $r_1, r_2$ not both zero we have
\begin{enumerate}
\item \cref{eq:vol} holds for every $\Pi_1,\Pi_2 \in N(r_1, r_2)$ when $r_1 r_2 \geq (4/9) d_1 d_2$,
\item  and \cref{eq:cut} for all $\Pi_1, \Pi_2 \in N(r_1, r_2)$ otherwise.
\item $\vol(\Pi_1, \Pi_2) \geq \tr \Phi(I) (d_1/d_2)$ for all $r_1, r_2$ not both zero.
\end{enumerate}
This completes the proof of the lemma, because every point is at most $\delta$ from an element of $N$ every element of $N$ has $\vol(\Pi_1, \Pi_2) \geq \tr \Phi(I)/{d_1 d_2}$, then by \cref{lem:net-suffices} the two sufficient conditions of \cref{lem:suff} hold. 

To show that the three conditions hold with the desired probability, we show that for fixed $r_1, r_2$, each holds with probability at least $1 - e^{n (r_1 d_2 + r_2 d_1)}$. The sum of $e^{-\Omega(n (r_1d_2 + r_2 d_1))}$ over all $0 \leq r_1 \leq d_1, 0 \leq r_2 \leq d_2$ apart from $r_1 = r_2 = 0$ is at most $e^{- \Omega( n d_1)}$, so the conditions hold for all $r_1, r_2$ with the desired probability. 

We first bound the failure probability for the first item. By \cref{lem:probabilities}, if $r_1 r_2 \geq (4/9) d_1 d_2$ then \cref{eq:vol} holds for every $\Pi \in N(r_1, r_2)$ with probability 
\begin{align*}
1 - |N|e^{- \Omega( n d_1 d_2) } = 1 - e^{-\Omega(n d_1 d_2)} \geq 1 - e^{ - \Omega(n (r_2d_1 + r_1d_2)}.
\end{align*}
Here we have used the lower bound on $n$. 

Next we bound the probability in the second case. As in the proof of \cref{lem:suff}, if $r_1 r_2 \leq (4/9) d_1 d_2$ then $F_1 \geq \frac{1}{2} (F_1 + 2 F_2)$. Hence $F_1/ (r_1 d_1 + r_2 d_2) \geq \frac{1}{2} (r_1 d_2 + r_2 d_1)/(r_1 d_1 + r_2 d_2) \geq \frac{1}{2} \frac{d_1}{d_2}$. Thus, by \cref{lem:probabilities} and the lower bound on $n$, \cref{eq:cut} holds for every $\Pi \in N(r_1, r_2)$ with probability $1 - e^{-\Omega(n (r_1d_2 + r_2 d_1)}.$ 

The third item holds with probability $1 - e^{-\Omega(n (r_1d_2 + r_2 d_1)}$ by \cref{lem:probabilities}. \end{proof}

\begin{proof}[Proof of \cref{thm:operator-cheeger}]
To prove \cref{thm:operator-cheeger}, apply \cref{lem:op-cheeger} with using \cref{cor:vershynin} to bound $\eps$ and \cref{lem:union} to bound $\ch(\Phi)$.
\end{proof}

\subsection{Pisier's proof of expansion}
We have shown above that the diagonal blocks $(\nabla^{2})_{aa} \approx \frac{1}{d_{a}} I_{a}$. Therefore to show strong convexity we would like to bound the off diagonal blocks 
\[ \forall X,Y:  \langle Q^{ab}, X \otimes Y \rangle \leq \lambda \frac{\|X\|_{F} \|Y\|_{F}}{\sqrt{d_{a} d_{b}}}   \]
Pisier's method of proof uses the trace method along with Gaussian concentration in Banach spaces. 

\begin{theorem}
We denote a random gaussian matrix $Y \in \R^{m \times n}$ (for $m < n$) with $Y_{ij} \sim \mathcal{N}(0,1)$, we have the following bound
\[ \E \|Y\|_{op} \leq \sqrt{m} + \sqrt{n} \]
\end{theorem}

% 

\begin{theorem} [Pisier]
For any Banach space $\sigma_{B}^{2} := \sup \{ \E \langle Y, \xi \rangle^{2} \mid \|\xi\|_{*} \leq 1 \} $
\[ \implies \Pr [ | \|Y\| - \E \|Y\| | \geq t ] \leq \exp \left( - \frac{\Omega(t^{2})}{\sigma^{2}} \right)   \]
\end{theorem}

\begin{corollary}
\[ (\E \|Y\|^{p})^{\frac{1}{p}} \leq \E \|Y\| + O \left( \sqrt{\frac{p}{\sigma^{2}}} \right)   \]
\end{corollary}

% Appendix - https://arxiv.org/abs/1209.2059
% Theorem 16.6 - https://arxiv.org/pdf/1101.4195.pdf 
\begin{theorem}
Let $P$ denote the projection onto $(\vec{I_{n}},\vec{I_{m}})$, then the following holds with $1/poly(mn)$ failure probability
\[ \|\sum_{i=1}^{N} \alpha_{i} Y_{i} \otimes Y_{i} (I - P) \|_{op} \leq O \left( \sqrt{\sum_{i} \alpha_{i}^{2}} \right) \left( \E \|Y\|_{op} \right)^{2} \]
\end{theorem}
\begin{proof}
We first begin by a standard symmetrization trick so that there are no products of variables
\[\E Y^{*} I_{m} Y = m I_{n} \implies \E Y \otimes Y (I-P) = 0\]
\[ \E_{Y} \|\sum_{i=1}^{N} \alpha_{i} Y_{i} \otimes Y_{i} (I - P)\|_{op} \leq \E_{Y,Z} \|\sum_{i=1}^{N} \alpha_{i} Y_{i} \otimes Y_{i} (I - P) - \sum_{i=1}^{N} \alpha_{i} Z_{i} \otimes Z_{i} (I - P) \|_{op}  \]
Now note $(Y,Z)$ has the same distribution as $(\frac{Y+Z}{\sqrt{2}},\frac{Y-Z}{\sqrt{2}})$
\[ = \frac{1}{2}\E_{Y,Z} \|\sum_{i=1}^{N} \alpha_{i} (Y_{i}+Z_{i}) \otimes (Y_{i}+Z_{i}) (I - P) - \sum_{i=1}^{N} \alpha_{i} (Y_{i}-Z_{i}) \otimes (Y_{i}-Z_{i}) (I - P) \|_{op}  \]
\[ = \E_{Y,Z} \|\sum_{i} \alpha_{i} Y_{i} \otimes Z_{i} (I-P) + \alpha_{i} Z_{i} \otimes Y_{i} (I-P)\|_{op}  \]
\[ \leq 2 \E_{Y,Z} \|\sum_{i} \alpha_{i} Y_{i} \otimes Z_{i}\|_{op}   \]
Note we've gotten rid of the projection $(I-P)$, but now the left and right Kraus operators are independent. Now we apply the trace method to this operator
\[ \E \|\sum_{i} \alpha_{i} Y_{i} \otimes Z_{i}\|_{2p}^{2p} \]
\[ = \E Tr [ \left( \sum_{ij} \alpha_{i} \alpha_{j} Y_{i}^{*} Y_{j} \otimes Z_{i}^{*} Z_{j} \right)^{p} ]  \]
\[ = \sum_{\vec{i},\vec{j} \in [N]^{p}} \alpha^{\vec{i}} \alpha^{\vec{j}} \E_{Y,Z} Tr [ Y_{i_{1}}^{*} Y_{j_{1}} ... Y_{i_{p}}^{*} Y_{j_{p}} \otimes Z_{i_{1}}^{*} Z_{j_{1}} ... Z_{i_{p}}^{*} Z_{j_{p}}  ]   \]
\[ = \sum_{\vec{i},\vec{j}} \alpha^{\vec{i}} \alpha^{\vec{j}} (\E_{Y} Tr [ Y_{i_{1}}^{*} Y_{j_{1}} ... Y_{i_{p}}^{*} Y_{j_{p}} ] ) (\E_{Z} Tr [ Z_{i_{1}}^{*} Z_{j_{1}} ... Z_{i_{p}}^{*} Z_{j_{p}} ] )  \]
Here we used independence of $(Y,Z)$. We eventually want to charge to a term $\E \|\sum_{i} \alpha_{i} Y_{i}\|_{op}$. To this end we notice that since any expectations of polynomials of gaussians with positive coefficients are nonnegative and positive iff the polynomial is even; therefore $\alpha^{\vec{i}} \alpha^{\vec{j}}$ is also positive for all non-vanishing terms. So we can upper bound the term individually by the nc-Holder inequality below
\[ \leq  \sum_{\vec{i},\vec{j}} \alpha^{\vec{i}} \alpha^{\vec{j}} (\E_{Y} Tr [ Y_{i_{1}}^{*} Y_{j_{1}} ... Y_{i_{p}}^{*} Y_{j_{p}} ] ) ( \E_{Z} \|Z_{i_{1}}^{*}\|_{2p} ... \|Z_{j_{p}}\|_{2p} )  \]
\[ = \E_{Y,Z} \left\|\sum_{i} \alpha_{i} \|Z_{i}\|_{2p} Y_{i} \right\|_{2p}^{2p}   \]
\[ = \E_{Z} \left( \sum_{i} \alpha_{i}^{2} \|Z_{i}\|_{2p}^{2} \right)^{p} \E_{Y} \|Y\|_{2p}^{2p}    \]
Finally we can bound $\|\cdot\|_{op}$ by the trace method. Assume wlog (by homogeneity) that $\|\alpha\|_{2} = 1$:
\[ \E_{Y,Z} \|\sum_{i} \alpha_{i} Y_{i} \otimes Z_{i} \|_{op}^{2p} \leq \E_{Y,Z} \|\sum_{i} \alpha_{i} Y_{i} \otimes Z_{i} \|_{2p}^{2p} \]
\[ \leq ( \E \|Y\|_{2p}^{2p} )^{2} \leq ( m \E \|Y\|_{op}^{2p} )^{2} \leq m^{2} (\E \|Y\|_{op} + O(\sqrt{\frac{2p}{m+n}}))^{4p} \]
So choosing $p \sim \log m$ gives the bound. 
\end{proof}

\begin{theorem}
For $p = 2^{m}$, we have the following generalization of Holder's inequality
\[ |Tr[\prod_{i} A_{i}]| \leq \prod_{i} \|A_{i}\|_{p}   \]
\end{theorem}
\begin{proof} [Tao notes]
Done by induction. Note if $A$ are Hermitian, there is a simple tensor trick which allows us to prove the above for all $p \in \mathbb{N}$. 
\end{proof}

\begin{corollary}
For $Y \in \R^{d_{a} \times d_{b}}$ where $Y_{ij} \sim \mathcal{N}(0,1)$, our Kraus operator is of the form $\frac{1}{\sqrt{TD}} Y$, and our off-diagonal operator is
\[ \sum_{k=1}^{TD/d_{a}d_{b}} \frac{1}{TD} Y_{k} \otimes Y_{k}   \]
By the above theorem, the operator norm on the subspace $\perp$ to $(I_{a},I_{b})$ is of the order $\frac{d_{a}+d_{b}}{\sqrt{d_{a} d_{b} TD}}$, so for constant expansion it suffices that $TD \gg \max_{a} d_{a}^{2}$
\end{corollary}


\section{Tensor Scaling}
We will maintain similar notation. We have $n$ samples of $X \sim \mathcal{N}(0,\frac{1}{n} \otimes_{a} \frac{1}{d_{a}} I_{a})$ with $D := \prod_{a} d_{a}$. We don't have a KLR style analysis at the moment, but strong convexity is enough by the FM analysis, and this can be proven by just controlling each bipartite piece. So the operator scaling analysis does give us very good bounds for $\|\mu\|_{op}$ and expansion with $nD \gg \max_{a} d_{a}^{2} \log^{c}(D)$. These bounds are not enough though, so in this section we will follow the FM analysis to give the requirements, then show the required strong convexity, and show how to maintain this under perturbation. 

\subsection{FM Analysis}
Recall that $\forall a: Q^{a} \approx \frac{1}{d_{a}} I_{a}$, so if we can show $\forall a \neq b: \langle Q^{ab}, Z \otimes Y \rangle \lesssim \frac{\|Z\|_{F} \|Y\|_{F}}{\sqrt{d_{a} d_{b}}}$ then we have strong convexity with $\langle Z, \nabla^{2} Z \rangle \gtrsim \sum_{a} \frac{\|Z_{a}\|_{F}^{2}}{d_{a}}$, i.e. the Hessian is strongly diagonally dominant. We will derive our requirements on strong convexity, perturbation bounds, and initial error. Assume we have the following strong convexity 
\[ \forall Z: \sum_{a} \frac{\|Z_{a}\|_{F}^{2}}{d_{a}} \leq \kappa^{2}: \langle Y, \nabla^{2}_{e^{Z}}, Y \rangle \geq \lambda \sum_{a} \frac{\|Y_{a}\|_{F}^{2}}{d_{a}}  \]

Choose $X$ to be the geodesic towards the optimum and $g(t) := f(e^{tZ})$ with the opt at $t=1$:
\[ g'(1) = \int_{0}^{1} g''(t) + g'(0) \geq \lambda \sum_{a} \frac{\|Z\|_{F}^{2}}{d_{a}} - |\langle \nabla_{a}, Z_{a} \rangle| \]
\[ \geq \sum_{a} \frac{\|Z_{a}\|_{F}}{\sqrt{d_{a}}} \left( \lambda \frac{\|Z_{a}\|_{F}}{\sqrt{d_{a}}} - \sqrt{d_{a}} \|\nabla_{a}\|_{F}  \right)  \]
\[ \geq \sqrt{\sum_{a} \frac{\|Z_{a}\|_{F}^{2}}{d_{a}}} \left( \lambda \sqrt{\sum_{a} \frac{\|Z_{a}\|_{F}^{2}}{d_{a}}} - \sqrt{\sum_{a} d_{a} \|\nabla_{a}\|_{F}^{2}} \right)  \]
This is $> 0$ if $\forall a: \lambda > d_{a} \frac{\|\nabla_{a}\|_{F}}{\|Z_{a}\|_{F}}$ or $\lambda^{2} > \frac{ \sum_{a} d_{a} \|\nabla_{a}\|_{F}^{2} }{\sum_{a} d_{a}^{-1} \|Z_{a}\|_{F}^{2}} $. 

Since standard perturbation bounds ($e^{Z} \approx I + Z$) only work for small $Z$, we will require
\[ \forall a: \|\nabla_{a}\|_{F} \ll \frac{1}{d_{a}} \]
\[ \forall a: \|Z_{a}\|_{F}^{2} \ll 1 \implies \langle Y, \nabla_{e^{Z}}^{2}, Y \rangle \geq \Omega(1) \sum_{a} \frac{\|Y_{a}\|_{F}^{2}}{d_{a}}   \]


\subsection{Moment Map}
For $g \sim \frac{1}{nD} \otimes_{a} I_{a}$, we want to bound $\|Q^{a} - sI_{a}\|_{F}$ using a net:
\[ \E \langle \sum_{t} g_{t} g_{t}^{*}, X_{a} \rangle = \sum_{i} x_{i} \chi(\frac{1}{d_{a}}, \frac{TD}{d_{a}}) = \langle \frac{1}{d_{a}} I_{a}, X \rangle = 0 \]
\[ \log \E \exp \theta \langle \sum_{t} g_{t} g_{t}^{*}, X_{a} \rangle = \log \E \exp \theta \sum_{i} x_{i} \chi(\frac{1}{d_{a}}, \frac{TD}{d_{a}})  \]
\[ = \sum_{i} \frac{-TD}{2 d_{a}} \log \left( 1 - 2 \theta \frac{x_{i}}{TD} \right)   \]
\[ \lesssim \theta^{2} \frac{\|X\|_{F}^{2}}{2 d_{a} TD} \hspace{10mm} \forall \theta < \left( \frac{\|X\|_{op}}{TD} \right)^{-1}  \]
\[ \implies \Pr[ \langle \sum_{t} g_{t} g_{t}^{*}, X_{a} \rangle \geq \epsilon \|X\|_{F} ] \leq 
\begin{cases} 
\exp( - \Omega(\epsilon^{2} TD d_{a}) ) & \epsilon  < \frac{\|X\|_{F}}{d_{a} \|X\|_{op}} 
\\ \exp ( - \Omega(\epsilon TD) \frac{\|X\|_{F}}{\|X\|_{op}} ) & \epsilon \geq \frac{\|X\|_{F}}{d_{a} \|X\|_{op}}
\end{cases}
\]
We will need the following settings of $\epsilon$ in future:
\[ \epsilon \approx \frac{1}{\sqrt{d_{a}}} \implies \Pr [ d_{a} \|Q^{a} - sI_{a}\|_{F}^{2} \gtrsim c ] \leq \exp(d_{a}^{2} \log d_{a} - c \frac{TD}{\sqrt{d_{a}}})  \]
For which we need $TD \gtrsim \max_{a} d_{a}^{5/2} \log d_{a}$.
\[ \epsilon \approx \frac{1}{d_{a}} \implies \Pr [ d_{a} \|Q^{a} - sI_{a}\|_{F}^{2} \gtrsim \frac{c}{d_{a}} ] \leq \exp(d_{a}^{2} \log d_{a} - c^{2} \frac{TD}{d_{a}})   \]
For which we need $TD \gtrsim \max_{a} d_{a}^{3} \log d_{a}$.

\begin{remark}
Note we lose out on the subgaussian part of the bound only when $\frac{d_{a} \|X\|_{op}^{2}}{\|X\|_{F}^{2}}$ is large. It is quite possible that for our setting, we can bound e.g. the condition number or stable rank of is small w.h.p. In particular if we can show the only relevant part of the net has $s \|X\|_{F}^{2} \geq d \|X\|_{op}$ then we only incur a $\sqrt{s}$ loss in required samples. 
\end{remark}

Actually there seems to be a simpler way to prove these statements using the $\|\cdot\|_{op}$ bounds derived earlier. 
\[ \|Q^{a} - sI_{a}\|_{op} \leq \frac{\sqrt{f(d)}}{d} \implies d_{a} \|Q^{a} - sI_{a}\|_{F}^{2} \leq f(d)  \]
So this means in the first case, we need $TD \gtrsim \max_{a} d_{a}^{2} \log D$ and the second case we need $TD \gtrsim \max_{a} d_{a}^{3} \log D$. But by this analysis we only get $1/poly$ failure probability. 


\subsection{Net proof of Expansion}
Recall again we have $Q := \sum_{i \in [n]} X_{i} X_{i}^{*}$ with i.i.d $X \sim \mathcal{N}(0,\frac{1}{n} \otimes_{a} \frac{1}{d_{a}} I_{a})$. For symmetric test matrices $Z,Y$ with eigenvalues $\{z_{i}\},\{y_{j}\}$ respectively:
\[ \E \langle Q, Z_{a} \otimes Y_{b} \rangle = \sum_{ij} z_{i} y_{j} \chi(\frac{1}{d_{a} d_{b}}, \frac{nD}{d_{a} d_{b}}) = \langle \frac{1}{d_{a}} I_{a}, Z \rangle \langle \frac{1}{d_{b}} I_{b}, Y \rangle = 0 \]

\[ \log \E \exp \theta \langle Q, Z_{a} \otimes Y_{b} \rangle = \log \E \exp \theta \sum_{ij} z_{i} y_{j} \chi(\frac{1}{d_{a} d_{b}}, \frac{nD}{d_{a} d_{b}})  \]
\[ = \sum_{ij} \frac{-nD}{2 d_{a} d_{b}} \log \left( 1 - 2 \theta \frac{z_{i} y_{j}}{nD} \right)   \]
\[ \lesssim \theta^{2} \frac{\|Z\|_{F}^{2} \|Y\|_{F}^{2}}{2 d_{a} d_{b} nD} \hspace{10mm} \forall \theta < \left( \frac{\|Z\|_{op} \|Y\|_{op}}{TD} \right)^{-1}  \]
\[ \Pr[ \langle Q, X \otimes Y \rangle \geq \lambda \frac{\|Z\|_{F} \|Y\|_{F}}{\sqrt{d_{a} d_{b}}} ] \leq 
\begin{cases} 
\exp( - \lambda^{2} nD ) \hspace{7mm} \lambda  < \frac{\|Z\|_{F} \|Y\|_{F}}{\sqrt{d_{a} d_{b}} \|Z\|_{op} \|Y\|_{op}} 
\\ \exp ( - \lambda nD \frac{\|Z\|_{F} \|Y\|_{F}}{ \sqrt{d_{a} d_{b}} \|Z\|_{op} \|Y\|_{op}} )
\end{cases}
\]
So for $\lambda \approx \frac{1}{k}$ we need $nD \gtrsim \max_{a} d_{a}^{3} \log D > \max_{a,b} \sqrt{d_{a} d_{b}} (d_{a}^{2} + d_{b}^{2}) \log D$

\begin{remark}
Note again we lose out when $\frac{d \|X\|_{op}^{2}}{\|X\|_{F}^{2}}$ is large. So we would like to show that w.h.p. the singular vectors of our bipartite operator have e.g. small condition number or large stable rank. Again if we can show the relevant part of the net has $s \|X\|_{F}^{2} \geq d \|X\|_{op}^{2}$ then we incur an $s$ factor loss in samples. This is reminiscent of the fact that eigenvectors for random matrices have many delocalization properties, so will look into that. 
\end{remark}

\subsection{Delocalization}
I have a few claims which, if true, would give another proof (along with Pisier's) of constant expansion at the start with $1/poly$ failure probability. Unfortunately to make this robust we would either need a robust form of delocalization, which I think is false in general, or exponential failure probability, which again may be false in general.

Recall $M^{ab}$ is the operator defined by the off-diagonal block of the Hessian. For every choice of bases (torus) $U,V$ we have a matrix $M^{ab}_{UV}$ where each entry is $\chi(\frac{1}{d_{a}d_{b}}, \frac{TD}{d_{a}d_{b}})$. In particular each entry is of exponential type. 

\begin{claim}
Let $U,V$ be the (random) basis for the optimizers of $\|M\|_{op}$. Then $(U,V)$ and $M_{UV}$ are distributionally independent. 
\end{claim}

\begin{theorem} [Informal]
For $M \in \R^{n \times n}$ populated by iid variables $X$ such that $\E X = 0, \E X^{2} = 1$ and $X$ is of exponential type, we have delocalization of eigenvectors with $1/poly$ failure probability:
\[ v \in S^{n-1}, \|v\|_{\infty} \gtrsim \frac{\log^{c} n}{\sqrt{n}} \implies \forall \lambda \in \C: \|Mv - \lambda v\|_{2} \gtrsim \frac{1}{\sqrt{n}}   \]
\end{theorem}

%https://arxiv.org/pdf/1306.2887.pdf
%https://arxiv.org/pdf/1306.3099.pdf
%https://arxiv.org/pdf/1601.03678.pdf

\begin{claim}
The above informal claim can be extended to singular vectors. 
\end{claim}

\begin{corollary}
Conditioned on the above delocalization event, our net only has to cover $\{X \mid d \|X\|_{op}^{2} \lesssim \log^{c}(d) \|X\|_{F}^{2}$, and so we only require $TD \gtrsim d_{a}^{2} \log^{c} D$.
\end{corollary}


\subsection{Robust proof of Expansion}
In this section we will show expansion under perturbations of the form $\otimes_{a} e^{\delta_{a}}$. Note if $\|\delta\|_{op} \ll 1$ then we can approximate $e^{\delta} - I \approx O(\delta)$. 
\[ \mu := \E \langle Q, e^{\delta_{a}} Z e^{\delta_{a}} \otimes e^{\delta_{b}} Y e^{\delta_{b}} \otimes_{c} e^{2\delta_{c}} \rangle = \langle \frac{e^{2\delta_{a}}}{d_{a}} , Z \rangle \langle \frac{e^{2\delta_{b}}}{d_{b}}, Y \rangle \prod_{c \neq a,b} Tr[\frac{e^{2\delta_{c}}}{d_{c}} ]  \]
\[ \log \E \exp \theta \langle Q, e^{\delta_{a}} Z e^{\delta_{a}} \otimes e^{\delta_{b}} Y e^{\delta_{b}} \otimes_{c \neq a,b} e^{2\delta_{c}} \rangle = \log \E \exp \theta \sum_{\vec{i}} x^{\vec{i}} \chi(\frac{1}{D},T)   \]
\[ \leq \theta \mu + \theta^{2} \frac{\|e^{\delta_{a}} Z e^{\delta_{a}}\|_{F}^{2} \|e^{\delta_{b}} Y e^{\delta_{b}}\|_{F}^{2} \prod_{c} \|e^{2\delta_{c}}\|_{F}^{2}}{2 TD^{2}} \]
\[ \forall \theta < \left( \frac{\|e^{\delta_{a}} Z e^{\delta_{a}}\|_{op} \|e^{\delta_{b}} Y e^{\delta_{b}}\|_{op} \prod_{c} \|e^{2\delta_{c}}\|_{op}}{TD} \right)^{-1}    \]

\begin{lemma}
For $\|\delta\|_{op} \ll 1$
\[ \frac{1}{d} Tr[e^{2\delta}] \leq 1 + O(\|\delta\|_{op})  \]
\[ \langle \frac{1}{d} I, e^{\delta} Z e^{\delta} - Z \rangle \leq \frac{O(\|\delta\|_{op})}{\sqrt{d}} \|Z\|_{F}   \]
\[ \|e^{\delta} Z e^{\delta}\|_{F}^{2} \leq (1 + O(\|\delta\|_{op})) \|Z\|_{F}^{2}   \]
\end{lemma}
\begin{proof}
Let $\delta := e^{\delta'} - I$ and note $\|\delta\|_{op} \approx \|\delta'\|_{op}$ for small $\delta'$. So we bound
\[ Tr[e^{2\delta'} - I] = \langle I, e^{2\delta'} - I \rangle \leq d \|2\delta + \delta^{2}\|_{op} \leq d O(\|\delta\|_{op})   \]
For the second line we use Cauchy Schwarz $\|Z\|_{1} \leq \sqrt{d} \|Z\|_{F}$
\[ \langle I, (I + \delta) Z (I + \delta') - Z \rangle = \langle 2 \delta + \delta^{2}, Z \rangle \leq O(\|\delta\|_{op}) \|X\|_{1} \leq O(\|\delta\|_{op}) \sqrt{d} \|Z\|_{F}  \]
The second line is similar but simpler using $\|AB\|_{F} \leq \|A\|_{op} \|B\|_{F}$
\end{proof}

So using the above lemma, we have bounds for $\langle Z, I_{a} \rangle = \langle Y, I_{b} \rangle = 0$:
\[ \mu \leq \frac{\|Z\|_{F} \|Y\|_{F}}{\sqrt{d_{a} d_{b}}} O(\|\delta_{a}\|_{op} \|\delta_{b}\|_{op}) \prod_{c} (1 + O(\|\delta_{c}\|_{op})  \]
\[ \log \E \exp \theta (... - \mu) \leq \theta^{2} \frac{\|Z\|_{F}^{2} \|Y\|_{F}^{2}}{2TD} \prod_{c} (1 + O(\|\delta_{c}\|_{op}))  \]
\[ \forall \theta < \left( \frac{\|Z\|_{op}\|Y\|_{op}}{TD} \right)^{-1} \prod_{c} (1 - O(\|\delta_{c}\|_{op}))  \]
So basically, as long as $\|\delta\|_{op} \ll 1$ everything is of the same order as in the unperturbed case, and therefore if we run a net on all parts simultaneously (of size $\exp(\sum_{a} d_{a}^{2})$) we get roughly the same probabilistic bounds as the start. 

This creates a bottleneck though as in general the inequality $\|X\|_{op} \leq \|X\|_{F}$ could be tight, so in order to guarantee a small enough perturbation we can only move in the ball $\|X\|_{F} \ll 1$. This is why we wrote out the conditions required for $\|\nabla_{a}\|_{F} \ll \frac{1}{d_{a}}$, as therefore we can assume $\|Z_{a}\|_{F} \ll 1$ and require $\lambda \approx 1$. 

\begin{remark}
Here is where we definitely would like a KLR style analysis to exploit the fact that we actually have robustness in $\|\cdot\|_{op}$. 
\end{remark}


\subsection{Deterministic Robust Proof of Expansion}

We follow Michael's write-up in our setting. We can assume we have sufficiently strong expansion initially and we would like to show that any scaling ($e^{Y} \cdot g$ with $\forall a: \|Y_{a}\|_{F} \ll 1$) only changes the Hessian by a small amount. 
\begin{fact}
Recall the formula for Hessian at $v$:
\[ \langle X, \nabla_{v}^{2}, X \rangle = \langle \frac{v v^{*}}{\|v\|_{2}^{2}}, \left( \sum_{a} X_{a} \otimes I_{\overline{a}}  \right)^{2} \rangle - \left( \sum_{a} \langle \frac{v v^{*}}{\|v\|_{2}^{2}}, X_{a} \otimes I_{\overline{a}} \rangle  \right)^{2}  \]
\end{fact}

Initially, we have expansion of the form
\[ \langle X, \nabla^{2}, X \rangle \geq (1-\epsilon) \sum_{a} \frac{\|X_{a}\|_{F}^{2}}{d_{a}} - \lambda \sum_{a \neq b} \frac{\|X_{a}\|_{F} \|X_{b}\|_{F}}{\sqrt{d_{a} d_{b}}} \geq (1-\epsilon') \sum_{a} \frac{\|X_{a}\|_{F}^{2}}{d_{a}}   \]
So for how large of a perturbation can we show that every quadratic form changes $<< \sum_{a} \frac{\|X_{a}\|_{F}^{2}}{d_{a}} =: \|X\|^{2}$?  

\begin{lemma}
\[ \|\sum_{a} X_{a} \otimes I_{\overline{a}}\|_{op} \leq \sum_{a} \|X_{a}\|_{op} \leq \sum_{a} \|X_{a}\|_{F} \leq \sqrt{ \left( \sum_{a} d_{a} \right) \left( \sum_{a} \frac{\|X_{a}\|_{F}^{2}}{d_{a}}  \right)} \]
All inequalities can be tight, say at $X_{a} = \sqrt{d_{a}} E_{11}$. In Michael's notation, for standard tensor rep 
\[ \sup_{X} \frac{\|\Pi(X)\|_{op}}{\sum_{a} \|X_{a}\|_{F}} = k  \text{  ;  } \hspace{7mm}
\sup_{X} \frac{\|\Pi(X)\|_{op}^{2}}{\|X\|^{2}} = \sum_{a} d_{a}  \]
\end{lemma}

\begin{theorem}
For $\|v\|_{2} = 1$ and $w := e^{Y} \cdot v / \|e^{Y} \cdot v\|_{2}$ with perturbation $\|\sum_{a} Y_{a} \otimes I_{\overline{a}}\|_{op} = \|\Pi(Y)\|_{op} \ll 1$: 
\[ |\langle X, \nabla_{w}^{2}, X \rangle - \langle X, \nabla_{v}^{2}, X \rangle| \leq O(\sum_{a} d_{a}) \|X\|^{2} \|\Pi(Y)\|_{op}   \]
\end{theorem}
\begin{proof}
Assume wlog $\sum_{a} \frac{\|X_{a}\|_{F}^{2}}{d_{a}} = 1$:
\[ |\langle X, \nabla_{v}^{2}, X \rangle - \langle X, \nabla_{w}^{2}, X \rangle| \]
\[ \leq |\langle w w^{*} - v v^{*}, \left( \sum_{a} X_{a} \right)^{2} \rangle| + |\langle w w^{*}, \sum_{a} X_{a} \rangle^{2} - \langle v v^{*}, \sum_{a} X_{a} \rangle^{2}| \]
\[ = |\langle w w^{*} - v v^{*}, \Pi(X)^{2} \rangle| + |\langle w w^{*} + v v^{*}, \Pi(X) \rangle \langle w w^{*} - v v^{*}, \Pi(X) \rangle|    \]
\[ \leq \|w w^{*} - v v^{*}\|_{1} \|\Pi(X)\|_{op}^{2} + 2 \|\Pi(X)\|_{op} \|w w^{*} - v v^{*}\|_{1} \|\Pi(X)\|_{op}  \]
\[ = 3 (\sum_{a} d_{a}) \|w w - v v^{*}\|_{1}   \]
Now we just have to bound perturbations of rank one matrices:
\[ \|w w^{*} - v v^{*}\|_{1} \leq \sqrt{2} \|w w^{*} - v v^{*}\|_{F} \leq 2 \sqrt{2} \|w - v\|_{2}  \]
Finally we can just bound the action of $Y$:
\[ \|w - v\|_{2} \leq \|\frac{e^{Y} \cdot v}{\|e^{Y} \cdot v\|_{2}} - e^Y \cdot v\|_{2} + \|e^{Y} \cdot v - v\|_{2}  \]
\[ \leq |1 - \|e^{Y} \cdot v\|_{2}| \|w\|_{2} + \|e^{Y} - I\|_{op} \|v\|_{2} \leq 2 \|e^{Y} - I\|_{op} \leq O(1) \|\Pi(Y)\|_{op} \]
\end{proof}

\begin{claim}
For unit vectors $v,w: \|v v^{*} - w w^{*}\|_{F}^{2} \leq 2 \|v - w\|_{2}^{2}$
\end{claim}
\begin{proof}
Assume wlog $v = e_{1}, w = (x,y) \in \R^{2}$ and note
\[ v v^{*} - w w^{*} = \begin{pmatrix} 1 - x^{2} & -xy \\ -xy & -y^{2} \end{pmatrix}  \]
has $Tr = 1 - x^{2} - y^{2} = 0$ and $\det = (1-x^{2})(-y^{2}) - x^{2} y^{2} = - y^{2}$; so the eigenvalues are $\pm y$. 
\[ \implies \|v v^{*} - w w^{*}\|_{F}^{2} = 2 y^{2} = 2(1-x^{2}) \]
\[ \|v - w\|_{2}^{2} = (1-x)^{2} + y^{2} = 2(1-x) \]
Finally note $1 - x^{2}$ is concave and $2(1-x)$ is a tangent line at $x=1$. 
\end{proof}

Recall the sufficient condition for $\lambda = \Omega(1)$-expansion
\[ \sum_{a} d_{a} \|\nabla_{a}\|_{F}^{2} \lesssim \sum_{a} d_{a}^{-1} \|Y_{a}\|_{F}^{2}  \]
So we require $\langle X, \nabla_{e^{Y}}, X \rangle \gtrsim \|X\|^{2}$. We can get this bound for all $\|\Pi(Y)\|_{op} \ll (\sum_{a} d_{a})^{-1}$. Therefore we can get a proper perturbation bound for $\forall a: \|Y_{a}\|_{F} \ll (\sum_{a} d_{a})^{-1}$ which implies $\|Y\|^{2} \ll (\max_{a} d_{a})^{-3}$, so we require $\forall a: \|\nabla_{a}\|_{F} \ll (\max_{a} d_{a})^{-2}$. This is worse than the requirement from the larger net argument above. 

\CF{cole agrees - needs $D > d_a^5!$}

\subsection{Better Deterministic Robustness}
\begin{theorem}
For perturbation $v \to \otimes_{a} e^{\delta_{a}} \cdot v =: w$ where $\forall a: \|\delta_{a}\|_{op} \ll 1$, and let $\{\sigma_{1}^{ab}, \sigma_{2}^{ab}\}$ be the $\|\cdot\|_{F} \to \|\cdot\|_{F}$ norm and norm $\perp$ to $(I,I)$ for each bipartite part respectively:
\[ \forall a,b: \sigma_{2}^{ab}(w w^{*}) - \sigma_{2}^{ab}(v v^{*}) \leq O \left( \sum_{a} \|\delta_{a}\|_{op}  \right) \sigma_{1}^{ab}(v v^{*})   \]
\end{theorem}
\begin{proof}
We first argue the change of $\sigma^{ab}$ is small under perturbations where $\forall c \neq a,b: \delta_{c} = 0$. Let $M^{ab}$ be the matrix versions of the bipartite operators:
\[ \langle vec(Y), M_{v}^{ab}(vec(Z)) \rangle := \langle v v^{*}, I_{\overline{ab}} \otimes Z \otimes Y \rangle \] 
\[ \langle vec(Y), M_{w}^{ab}(vec(Z)) \rangle := \langle w w^{*}, I_{\overline{ab}} \otimes Z \otimes Y \rangle\]
\[ \implies M_{w} = (e^{\delta_{b}} \otimes e^{\delta_{b}}) M_{v} (e^{\delta_{a}} \otimes e^{\delta_{a}})   \]
\[ \implies \|M_{w} - M_{v}\|_{op} \leq O(\|\delta_{a}\|_{op} + \|\delta_{b}\|_{op}) \|M_{v}\|_{op}   \]
where in the last step we used $\delta \ll 1$. 

The more difficult part of the argument to see (at least for me) is how $\sigma^{ab}$ changes if some other part $c \neq a,b$ is changed, i.e. $\forall d \neq c: \delta_{d} = 0$. First we define $\delta := e^{2 \delta_{c}} - I$, and test vectors $Z,Y$:
\[ \langle w w^{*} - v v^{*}, I_{\overline{ab}} \otimes Z \otimes Y \rangle = \langle v v^{*}, \delta \otimes Z \otimes Y \rangle  = \langle Z \otimes Y, V^{*} \delta V \rangle \]
Here $V \in \R^{d_{c} \times d_{a}d_{b}}$ is the matricized version of $v$, i.e. the $k$-th element of $ij$-th column is $(V_{ij})_{k} := v_{ijk}$. Now in order to use our operator norm bounds, we need to deal with cancelations, so we split into positive and negative parts $Z := Z_{+} - Z_{-}, Y := Y_{+} - Y_{-}$:
\[ |\langle Z \otimes Y, V^{*} \delta V \rangle| \leq |\langle Z_{\pm} \otimes Y_{\pm}, V^{*} \delta V \rangle |  \]
Now we analyze one of these terms (by abuse of notation $Z, Y \succ 0$):
\[ \leq \langle Z \otimes Y, V^{*} |\delta| V \rangle \leq \|\delta\|_{op} \langle Z \otimes Y, V^{*} V \rangle = \|\delta\|_{op} \langle v v^{*}, I_{\overline{ab}} \otimes Z \otimes Y \rangle   \]
Each of these terms we can bound by $\sigma^{ab}_{1} \|Z\|_{F} \|Y\|_{F}$. (Note we can save a $2$-factor on these four terms since they are decompositions of $Z,Y$). So by iterating this argument over all $c$, we get the desired bound. 
\end{proof}

\subsection{Conclusion}
At the end of the day we require $nD \gtrsim \max_{a} d_{a}^{3} \log D$ to get small enough $\nabla$ and robust expansion. Note this is $d \log D$ away from the existence/ uniqueness threshold for the solution. 

\subsection{Ideas for KLR style analysis}
\begin{enumerate}
    \item The delocalization type arguments may be helpful since we know the movement is spread out. 
    \item Follow gradient flow and show we have strong convergence for $\Omega(\log \max_{a} d_{a})$ time, after which the operator norm of the gradient can never be bigger than the initial one. Therefore if we have the first point, it is enough to cover the $O(1)$-size operator norm ball. 
    \item Even if we get that perturbations come from $O(1)$-size operator norm ball, without some improved delocalization (or something) we would still need an extra $d$ factor. 
    \item One important lemma in KLR is that the operator norm of moment map is roughly monotone decreasing over all time (where the fudge is of the order of the change in objective function $\|v\|_{2}^{2}$ over time). This is not true for $k=3$. But we have some extra structure e.g.: the error of a row and column are positively correlated. 
    \item Let $\epsilon(t) := \max_{a} d_{a} \|\nabla_{a}(t)\|_{op}$. Maybe we can show 
    \[ T_{\epsilon} := \inf \{ t \mid \epsilon(t) > 100 t \epsilon(0) \}  \]
    is large enough. In particular by the argument above $T_{\epsilon}$ cannot be larger than $\log d$. Maybe there's some contradiction whereby we can always increase $T_{\epsilon}$. 
\end{enumerate}

Recall the gradient flow is 
\[ \partial_{t} v(t) = \sum_{a} I_{\overline{a}} \otimes X_{a} \cdot v(t)   \]
where $X_{a} := d_{a} \mu_{a}(t)$. Then we have
\[ \partial_{t} \sum_{a} d_{a} \|\mu_{a}(t)\|_{F}^{2} = \sum_{a} d_{a}^{2} \langle Q^{a}, \mu_{a}^{2} \rangle + \sum_{a \neq b} d_{a} d_{b} \langle Q^{ab}, \mu_{a} \otimes \mu_{b} \rangle  \]
\[ \geq (1-\epsilon) \sum_{a} d_{a}^{2} \frac{\|\mu_{a}\|_{F}^{2}}{d_{a}} - \lambda \sum_{a \neq b} d_{a} d_{b} \frac{\|\mu_{a}\|_{F} \|\mu_{b}\|_{F}}{\sqrt{d_{a} d_{b}}}   \]
\[ \geq (1-\epsilon') \sum_{a} d_{a} \|\mu_{a}\|_{F}^{2}   \]



\subsection{Experiments}
\begin{enumerate}
    \item Check $\|\mu(t)\|_{op}$ is (close to) monotone decreasing
    \item Check $\int_{0}^{T} \|\mu(t)\|_{op} \gg \|\int_{0}^{T} \mu(t)\|_{op}$. I'm assumming this last term is roughly the operator norm of the scaling, but can check that too: what is the operator norm of log of the scaling at time $T$?
    \item Plot $d \|\mu(t)\|_{op}^{2} / \|\mu(t)\|_{F}^{2}$. Should be $\Tilde{O}(1)$ for some amount of time. 
\end{enumerate}


\section{Noise}
\TODO{make sure things work under some error in the data}







%If $\langle \cdot , \cdot \rangle_P$ is the metric at a point $P$, then the new metric $\langle \cdot, \cdot \rangle_{R,P}$ will be given by 
%$$ \langle X, Y \rangle_{R,P} = \langle R^{1/2} X R^{1/2}, R^{1/2} Y R^{1/2} \rangle_{P}.$$

%At the identity the metric takes the form $\langle X, Y \rangle_R = \tr \sqrt{R} X  \sqrt{R} Y $. In particular the length of the geodesic $t\mapsto \sqrt{P}e^{Xt}\sqrt{P}$ for $t \in [0,1]$ is $\langle X, X \rangle_R$. 

\bibliographystyle{alpha}
\bibliography{refs}

\end{document}
