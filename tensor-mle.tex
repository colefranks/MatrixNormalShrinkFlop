\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{amsthm,mathtools,braket}
\usepackage{amssymb,bm, hyperref}
\usepackage{xcolor, float}
\usepackage[capitalize]{cleveref}

\newtheorem{theorem}{Theorem}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{prop}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corol}[theorem]{Corollary}
\newtheorem{fact}[theorem]{Fact}
\newtheorem{claim}[theorem]{Claim}
\newtheorem{remark}{Remark}
\newtheorem{definition}{Definition}
\newtheorem{example}{Example}
\floatstyle{boxed}\newfloat{Algorithm}{ht}{alg}

\DeclareMathOperator{\Lie}{Lie}
\DeclareMathOperator{\Lin}{L}
\DeclareMathOperator{\ope}{op}
\DeclareMathOperator{\poly}{poly}

\DeclarePairedDelimiter{\abs}{\lvert}{\rvert}
\DeclarePairedDelimiter{\norm}{\lVert}{\rVert}
\DeclarePairedDelimiter{\ip}{\langle}{\rangle}


\newcommand{\junk}[1]{}


\newcommand{\R}{{\mathbb{R}}}
\newcommand{\C}{{\mathbb{C}}}
\newcommand{\ot}{\otimes}
\newcommand{\mat}{\operatorname{Mat}}
\newcommand{\op}{\operatorname{op}}
\newcommand{\ch}{\operatorname{ch}}
\newcommand{\cut}{\operatorname{cut}}
\newcommand{\vol}{\operatorname{vol}}
\newcommand{\Q}{{\mathbb{Q}}}
\newcommand{\N}{{\mathbb{N}}}
\renewcommand{\vec}{\bm}
\newcommand{\Z}{{\mathbb{Z}}}
\renewcommand{\S}{\mathbb{S}}
\newcommand{\otheta}{\overline{\Theta}}
\newcommand{\htheta}{\hat{\Theta}}
\newcommand{\oZ}{\overline{Z}}
%\renewcommand{\Pr}{\operatorname{Pr}}

\newcommand{\E}{\mathbb{E}}
%\renewcommand{\Pr}{\mathbb{P}}

\newcommand\eps{\varepsilon}

\newcommand\cN{\mathcal{N}}
\newcommand\FF{\mathcal{F}}
\newcommand\HH{\mathcal{H}}
\newcommand\GG{\mathcal{G}}
\newcommand\SL{\operatorname{SL}}
\newcommand\PD{\operatorname{P}}
\newcommand\Herm{\operatorname{Herm}}
\newcommand\Sym{\mathcal{S}}
\newcommand\smallSym{S}
\newcommand\SPD{\mathcal{P}}

\newcommand\samp{x}

\newcommand\rv{X}

\newcommand\GL{\operatorname{GL}}
\newcommand\TT{\mathcal{T}}
\newcommand\PP{\mathcal{P}}
\newcommand\tr{\operatorname{Tr}}
\newcommand\rk{\operatorname{rk}}
\newcommand\RR{\mathcal{R}}
\newcommand\CC{\mathcal{C}}
\newcommand\BB{\mathcal{B}}
\newcommand\II{\mathcal{I}}
\renewcommand\AA{\mathcal{A}}
\newcommand{\MM}{\mathcal{M}}
\newcommand\AP{\mathcal{AP}}

\newcommand{\eqdef}{:=}
\newcommand{\maps}{\colon}
\newcommand{\ale}{\lesssim}
\newcommand{\age}{\gtrsim}

\newcommand{\CF}[1]{{\color{purple}[CF: #1]}}
\newcommand{\AR}[1]{{\color{orange}[AR: #1]}}
\newcommand{\RMO}[1]{{\color{red}[RMO: #1]}}
\newcommand{\MW}[1]{{\color{red}[MW: #1]}}
\newcommand{\TODO}[1]{{\color{blue}[TODO: #1]}}

\crefname{Algorithm}{Algorithm}{Algorithms}


\title{Optimal sample complexity for matrix and tensor normal models via geodesic convexity}
\author{Cole Franks, Rafael Oliveira, Akshay Ramachandran, Michael Walter}
\date{February 2020}

\begin{document}

\maketitle
\tableofcontents
\TODO{
\begin{enumerate}
%\item Use $n$ instead of $T$ for number of samples.
\item Replace the failure probability $1/poly(D)$ by an actual expression.
\item Fix the dimension issue in \cref{thm:tensor-convexity}.
\item Small annoying thing - we assume determinant 1. This can be handled easily, but needs to be done.
%\item When not random, use lower case $x$.
\item Comment that the geodesic distance is some variation on the Fischer-Rao distance.
\end{enumerate}
Polishing:
\begin{enumerate}
\item Akshay's note on simplifying the proof of \cref{thm:tensor-convexity}: \AR{Can rephrase explicitly in terms of $\nabla^{2} F$ and rank-one term. Then \ref{prop:gradient-bound} gives $1-\eps$ lower bound on each diagonal block of $\nabla^{2} F$, \ref{offdiagPisier} gives $\lambda$ bound on each offdiagonal block of $\nabla^{2} F$, and \ref{prop:gradient-bound} once again gives $k \eps^{2}$ bound on whole rank-one term.  }
\item \AR{Can improve the strong convexity stuff by a factor of two by using the projection to traceless on both sides of the inner product}
\item \CF{for the readers' sake some justification is needed for why perturbing $\samp$ this is the same as considering the Hessian of our function at another point in $\SPD$. We'll probably have to discuss this earlier in the paper when we mention all the different perspectives for scaling, but for now a little reminder would help.}
\item Define $B^\infty_c$ to be all $\Theta$ such that $\sum \|\log \Theta_a\|_{op} \leq c$. May be able to avoid this since everything is internal to proofs.
\end{enumerate}

}


\section{Introduction}
\CF{second try}
Covariance matrix estimation is an important task in statistics, machine learning, and the empirical sciences. We consider covariance estimation for matrix-variate and tensor-variate data, that is, when individual data points are matrices or tensors. Matrix-variate data arises naturally in numerous applications like gene microarrays, spatio-temporal data, and brain imaging. However the dimensionality of these problems is frequently much higher than the number of samples, making estimation information-theoretically impossible without structural assumptions even in the Gaussian setting. 

To remedy this issue, matrix-variate data is commonly assumed to follow the \emph{matrix normal distribution} \CF{cite dutilleul 99, werner 08}. That is, the matrix follows a multivariate Gaussian distribution and the covariance between any two entries in the matrix is a product of an inter-row factor and an inter-column factor. In spatio-temporal statistics this is referred to as a separable covariance structure. Formally, if a matrix normal random variable $X$ takes values in the $d_1\times d_2$ matrices, then its covariance matrix $\Sigma$ is a $d_1d_2\times d_1 d_2$ matrix that is the Kronecker product $\Sigma_1 \ot \Sigma_2$ of two positive-semidefinite matrices $\Sigma_1$ and $\Sigma_2$ of dimension $d_1\times d_1$ and $d_2\times d_2$, respectively. This naturally extends to the tensor normal model, the multivariate Gaussians on $k$-dimensional arrays with covariance matrix equal to the Kronecker product of $k$ many positive semidefinite matrices $\Sigma_1, \dots, \Sigma_k$. In this paper we consider the estimation of $\Sigma_1, \dots, \Sigma_k$ from $n$ samples of a matrix or tensor normal random variable $X$. 

Much research has been devoted to estimating the covariance matrix for the matrix and tensor normal models, but gaps in rigorous understanding remain. Dutilleul \CF{cite} and later Werner et. al \CF{cite} proposed an iterative algorithm, known as the flip-flop algorithm, to compute the maximum likelihood estimator (MLE). In the same work Werner et. al. also showed that the MLE is consistent and asymptotically normal, and showed the same for the estimator obtained by terminating the flip-flop after three steps. Here we will be interested in non-asymptotic rates; standard estimation of the covariance matrix $\Sigma$ by the sample covariance matrix yields a mean-squared Frobenius norm error of $(d_1 d_2)^2/n$ assuming $n \geq C d_1 d_2$. The matrix normal model, however, has $\Theta(d_1^2 + d_2^2)$ parameters so it should be possible to do much better. Assuming that the covariance factors have constant condition number and that $n$ is at least $\tilde{\Omega}(\max\{d_1,d_2\})$, in \CF{tsilig} it was shown that the three-step flip-flop estimator has mean-squared Frobenius error of $O((d_1^2 + d_2^2)/n)$ for the full matrix $\Sigma$; they did not state a bound for the individual factors $\Sigma_1,\Sigma_2$. The same authors showed tighter rates which hold even for $n\ll d_i$ were shown for a penalized estimator under the additional assumption that the precision matrices $\Sigma_i^{-1}$ are sparse. In the extremely undersampled regime, Zhou demonstrated a single-step penalized estimator that converges even for a single matrix $(n=1)$ when the precision matrices have constant condition number, are highly sparse, and have bounded $\ell_1$ norm off the diagonal. Simply setting $\Sigma_2 = I_{d_2}$ or $\Sigma_1 = I_{d_1}$, in which case the matrix normal model reduces to standard covariance estimation with $d_1n$ (resp. $d_2 n$) samples, shows the necessity of additional assumptions like sparsity or well-conditionedness if $n < \max\{d_1/d_2, d_2/d_1\}$. Allen and Tibshirani also considered penalized estimators for the purpose of missing data imputation \CF{cite allen}. For the tensor normal model, a natural generalization of the flip-flop algorithm has been proposed to compute the MLE \CF{cite dutilleul} but is not known to converge. Assuming bounded constant condition number of the true covariances and knowledge of initializers within constant Frobenius distance of the true precision matrices, the authors of \CF{sun} propose an estimator with tight rates. In both the matrix and tensor case, no estimator for the Kronecker factors has been proven to have tight rates without additional assumptions on the factors' structure.


\CF{The sparse folks should not feel bad because it's considered that estimating sparse GGMs without condition number bounds or irrepresentability conditions is NP hard.}


%Despite advances in understanding other estimators, 

Even characterizing the existence of the MLE for the matrix and tensor normal model has remained elusive until recently. It was recently noted that the matrix normal and tensor MLEs are equivalent to algebraic problems about a group action called the \emph{left-right action} \CF{cite philipp} and the \emph{tensor action}, respectively. In the computer science literature these two problems are called \emph{tensor} and \emph{operator scaling}, respectively. It was independently pointed out in \CF{cite tyler M} that the Tyler's M estimator for elliptical distributions (which arises as the matrix normal problem under the additional promise that $\Sigma_2$ is diagonal) is a special case of operator scaling. Using the connection to the left-right action, the existence of the matrix normal MLE in terms of $d_1,d_2,n$ was recently characterized in full \CF{cite visu} and was extended to the tensor tensor normal model in \CF{cite michael, visu}. In the context of operator scaling, Gurvits showed much earlier that the flip-flop algorithm converges to the matrix normal MLE whenever it exists \CF{cite gurv}, and later it was shown that the number of flip-flop steps to obtain precision $\eps$ for the tensor and matrix normal model is polynomial in the input size and $1/\eps$ \CF{cite ggow, cite burgisser}. 
%In the context of tensor scaling, it was shown earlier that the flip-flop algorithm converges to the tensor MLE whenever it exists \CF{cite tensor scaling}. 

%In the current literature on matrix normal and tensor models, typically the estimators are assessed using Frobenius or spectral norm between the estimated parameter and the truth. However, neither of these metrics bound statistical distances of interest such as the total variation or Kullback-Leibler divergence between the true distribution and that corresponding to the estimated parameter, or the Fischer-Rao distance. These quantities are affinely invariant, meaning that for any invertible matrix $g$ we have $d(\Sigma, \Sigma') = d(g \Sigma g^T, g \Sigma' g^T)$. \CF{not exactly sure how to write this, but I want it to say that we get the right rates with no assumptions and we use more appropriate metrics. Amusingly, to get from these metrics TO the less useful metrics or vice versa, one needs the condition number assumptions. Also, it appears that the reason that the better metrics aren't being used is that the estimators didn't have the equivariance property.}

\subsection{Our contribution}

We take a geodesically convex optimization approach to provide bounds to estimate the precision matrices without any assumptions on their structure. For the matrix normal model our rates are tight in every regime up to logarithmic factors, and for the tensor normal model our rates our tight if there are enough samples.

In the current literature on matrix normal and tensor models, typically the estimators are assessed using Frobenius or spectral norm between the estimated parameter and the truth. Neither of these metrics bound statistical dissimilarity measures of interest such as the total variation or Kullback-Leibler divergence between the true distribution and that corresponding to the estimated parameter, or the Fisher-Rao distance. To this end, we consider the following dissimilarity measures:
\begin{definition}[Mahalanobis distance]
For $A,B$ positive-definite $d\times d$ matrices, define
% \begin{align}d_{F}(\Sigma_1; \Sigma_2) = \| I - \Sigma_2^{1/2} \Sigma_1^{-1} \Sigma_2^{1/2}\|_F\\
% d_{op}(\Sigma_1; \Sigma_2) = \| I - \Sigma_2^{1/2} \Sigma_1^{-1} \Sigma_2^{1/2}\|_{op}
% \end{align}
% and
\begin{align}d_{F}(A; B) = \| I - B^{-1/2} A B^{-1/2}\|_F\\
d_{op}(A; B) = \| I - B^{-1/2} A B^{-1/2}\|_{op}
\end{align}
\end{definition}
If $\Theta_1, \Theta_2$ are the precision matrices of centered Gaussians, $d_F$ is sometimes called the Mahalanobis distance between them. The Mahalanobis distance is a natural dissimilarity measure on Gaussians because when any of the Mahalanobis, total variation, square of the KL-divergence, or Fisher-Rao distances is at most a small constant then they are on the same order \CF{check, cite} \cite{barsov1987}. Considering $d_{op}$ yields even stronger results, and both can be related to the usual norms by $\|A - B\|_F \leq \|B\|_{op} d_F(A,B)$ and $\|A  - B\|_{op} \leq \|B\|_{op} d_{op}(A,B)$. Our theoretical contribution is as follows:
\begin{enumerate}
\item For the matrix normal model, if $n \geq C (\max\{d_i^2\}/d_1 d_2) \log^2 d_1$, then the MLE for the precision matrices $\Theta_1, \Theta_2$ has MSE $O( \max d_i^2/ n \prod d_i )$ for in $d_{op}$; as a corollary the MSE in $d_F$ is $O( \max d_i^3/ n \prod d_i )$.
\item For the tensor normal model, if $n \geq C \max\{d_i^3\}/ \prod d_i$, then the MLE for the precision matrices $\Theta_1, \dots, \Theta_k$ has MSE $O(\max d_i^3/ n \prod d_i)$ in $d_F$.
\item Under the same sample requirements as above in each case, the flip-flop algorithm converges exponentially quickly to the MLE with high probability. As a corollary, there is an algorithm to compute the MLE with expected runtime polynomial in the input size and $\log(1/\eps)$. 
\end{enumerate}
\CF{can we say better for each individual matrix? Another thought is that if one of the matrices is too big, we can still try to get the other ones by marginalizing the bad one out)}

\CF{mention $d_{TV}$ again}

The first result is tight up to logarithmic factors, and the second result is tight. Without the restriction on $n$ in the first result, no estimator can converge in spectral norm as the parameters grow, so we consider the restriction mild. Likewise for the second result no estimator can converge in Frobenius norm without the restriction on $n$. 
%The first result is tight up to logarithmic factors, and the second is tight provided the number of samples is large enough for it to apply. Without the restriction on $n$ in our second result, no estimator can converge in Frobenius norm, so we consider it mild. 

%Clearly one requires $n \geq C d_i^2/\prod d_i$ for any estimator to converge to the $i^th$ marginal even in spectral norm; the restriction in our section result asks that $n$ is a $d_i$ factor larger than this. 

For interesting cases of the tensor normal model such as $d\times d \times d$ tensors we just require that $n$ is at least a large constant. For the matrix normal model, our first result removes the added constraint $n \geq C \max\{d_1,d_2\}$ in \CF{tsilig}. For the tensor normal model we leave extending our MSE bounds for $d_{op}$ to $k \geq 3$ as an open problem. 

\CF{I think I'd like to do a better job at gradually defining notation so its clear that these are the right numbers.}

To handle the undersampled case, we also introduce a regularized estimator that is much simpler to compute than the penalized regularizers introduced in \CF{tsilig, sun, zhou}, and empirically has comparable and sometimes better performance to existing regularizers. Our regularizer has a Bayesian interpretation as coming from a Wishart prior for the covariance, and is closer in spirit to the shrinkage estimators considered by \CF{cite Weisel, etc}. \CF{expand up on this; put it in the bulleted list also?}





%These distances are invariant under changes of basis on $\R^d$, which is desirable as 

%Our bounds on $d_{op}$ will strengthen those on $d_F$.

%In some regimes we are able to obtain stronger results by considering $d_{op}$. 
%if the Mahalanobis distance is at most a small constant then it is on the order of the total variation distance $\widehat{\Sigma}$ \cite{barsov1987estimates} and the Fisher-Rao distance.

% We consider the second quantity $d_{op}$ because we are often able to show that $d_{op}$ is much smaller than our estimates. 





%\CF{The more recent stuff; philipp, visu, etc.}\\

\CF{then some discussion of methods, numerics}

\subsection{Notation}
\CF{some aspects of this seem awfully specific to the tensor normal model and maybe could wait until after we define it, or simply merge the two, i.e. "Notation and model"}
The letter $n$ will denote a number of samples, and $d_1\leq \dots \leq d_k$ will denote sorted dimensions, and we set $D:=\prod_{i = 1}^k d_i$. Let $\PD_d$ denote the positive definite $d\times d$ matrices with unit determinant, and $\PD_d^1$ the subset of $\PD$ with unit determinant. Let $\smallSym_d$ denote the vector space of $d\times d$ symmetric matrices, and $\smallSym^0_d$ the subspace of traceless matrices in $\smallSym_d$, i.e. the tangent space of $\PD_d^1$. Let~$\SL_d$ denote the group of $d\times d$ matrices with unit determinant.
%Then, $A^T e^Z A \in \PD_d^1$ for any $A \in \SL_d$ and $Z\in\Sym_d^0$.
% Any matrix in $\PD_d^1$ can be written as the matrix exponential of a matrix in $\Sym_d^0$.
Let 
$$\SL = \bigoplus_{a=1}^k \SL_{d_a}, \SPD = \bigoplus_{a = 1}^k \PD_{d_a}^1, \Sym = \bigoplus_{a = 1}^k \smallSym_{d_a}^0.$$ For a $k$-tuple of $A = (A_1, \dots, A_k)$ of matrices, $\|A\|_F^2:=\sum_{i = 1}^k \|A_i\|_F^2$. 
 We denote by $AB=(A_1B_1,\dots,A_kB_k)$ and $e^Z=(e^{Z_1},\dots,e^{Z_k})$ the componentwise product and matrix exponential, respectively, of matrix tuples $A, B \in \SL$ and $Z\in\Sym$. $I$ will denote an identity matrix, and $I_{a}$ a $d_a\times d_a$ identity matrix. $\langle \cdot, \cdot \rangle$ denotes the standard inner product. $C, c$ denote large (resp. small) absolute constants that change line to line.


\CF{further notation to be introduced; delete as is done}
\begin{itemize}
%\item Number of samples $n$, dimensions $d_1\leq \dots \leq d_k$. $D$ for product of these. 
%\item $X$ for the tensor random variable, $\samp_1, \dots, \samp_n$ for each , $\samp = (\samp_1, \dots, \samp_n)$ for the random tuple of samples. $\rho = \samp \samp^T/\|\samp\|_F^2$. Lower case $x$ for samples.

\item $\rv$ for the random tuple $(\rv_1, \dots, \rv_n)$, $x$ for the tuple of samples $(x_1, \dots, x_n)$ when no longer random. Think of $\rv$ as $D \times n$ matrix, $\rho = \rv\rv^T,xx^T$ etc. When $x_i$ is a matrix, which unfortunately does happen sometimes, we'll use $x_i^\dagger$ for the matrix transpose (open to suggestions on this one).
%\item $\braket{\cdot,\cdot}_{\vec d}$ denote modified Hilbert-Schmidt inner products\MW{sadly the corresponding norms look like $\ell_p$ norms},
\item $f_{\rv}$ for the function in \cref{dfn:function}, mostly drop $\rv$. $\langle \cdot, \cdot \rangle$ is the $\ell_2$ inner product of vectors

%\item $\smallSym_d$ for $d \times d$ real symmetric (meh), $\PD_d$ for $d \times d$ real positive definite, $\smallSym_d^0$ for traceless symmetric, $\PD_d^1$ for $\det=1$ positive definite? 
\item $\Theta$ for big tensor product pd concentration matrix, $\Theta_a$ for individual pd's.
%\item I'm going to call $\SL = \oplus \SL_{d_i}, \SPD = \oplus \PD_{d_i}^1, \Sym = \oplus \smallSym_{d_i}^0$. Explain somewhere how $\Sym$ is the tangent space of $\SPD$. 
%\MW{Suppressing the $^1$ and $^0$ is a bit confusing I think. Maybe $\operatorname{SPD}$ for $\SPD$ with $\det=1$? I still feel that $\Sym$ looks somewhat horrible (with or without subscript, but I am not sure what would be better).}
%\item $G = \prod_{a=1}^k \SL_{d_a}$, $P = \prod_{a=1}^k \PD_{d_a}^1$, and $S = \bigoplus_{a=1}^k \Sym_{d_a}^0$.

\item $\nabla, \nabla^2$ for Riemannian Hessians and gradients, $\nabla_a f$, $\nabla^2_{ab} f$ for components. $\nabla f$ means at the identity. $\|\nabla^2_{ab}f\|_{op}:=\|\nabla^2_{ab}f\|_{F\to F}$.
%\item $C, c$ large (resp. small) constants that change line to line.
\item $\rho^{(a)}$, $\rho^{(ab)}$ for marginals.
%\item $I$ for an identity matrix, $I_a$ for the $d_a \times d_a$ identity matrix
\item $\norm{\cdot}_p$ for the $\ell_p$ norm of vectors and the Schatten-$p$ norm of operators
\end{itemize}




%By simply setting $\Sigma_2 = I_{d_2}$, it is easy to see that one cannot obtain better MSE rates than $d_1^2/d_2 n$ for estimating $\Sigma_1$ in Frobenius norm. 

%=============================================================================
\section{The tensor and matrix normal models}
%=============================================================================


We consider the tensor normal model, in which $X_1, \dots, X_n \in \R^D$ are drawn i.i.d. from the random variable $X$ distributed according to the normal distribution $\cN(0, \Sigma)$, where the covariance matrix $\Sigma = \bigotimes_{a = 1}^{k} \Sigma_a \in \PD_D$ is the Kronecker product of positive definite $d_a\times d_a$ matrices $\Sigma_a$ for $a\in [k]$. Our goal is to estimate the $k$ Kronecker factors $\Sigma_1, \dots, \Sigma_k$ given access to the random tuple $X_1, \dots, X_n$. \CF{maybe the following discussion belongs in the intro}. 

One may think of the random variable $X$ as taking values in the set of $d_1 \times \dots \times d_k$ arrays of real numbers. There are $k$ natural ways to ``flatten" $X$ to a matrix: for example, we may think of it as a $d_1 \times d_2d_3...d_k$ matrix whose column indexed by $(i_2,\dots, i_k)$ is the vector in $\R^{d_1}$ with $i_1^{th}$ entry equal to $X_{i_1, \dots, i_k}$. In an analogous way we may flatten it to a $d_2 \times d_1d_3...d_k$ matrix, and so on. In the tensor normal model, the $d_2d_3\dots d_k$ many columns are each distributed as a Gaussian random vector with covariance proportional to $\Sigma_1$. Similarly the columns of the $d_2 \times d_1d_3...d_k$ flattening has columns proportional to $\Sigma_2$, and so on. As such, the columns of the $a^{th}$ flattening can be used to estimate $\Sigma_a$ up to a scalar. However, doing so na\"ively (e.g. using the sample covariance matrix of the columns) can result in an estimator with very high variance. This is because the columns of the flattenings are not independent. They may be so highly correlated that they effectively constitute only one random sample rather than $d_2\dots d_k$ many. The MLE decorrelates the columns to obtain rates like those one would obtain if the columns were independent.



The MLE is easier to state in terms of the precision matrices, the inverses of the covariance matrices. Let $\Theta$ denote the precision matrix $\Sigma^{-1}$, i.e., $\Theta = \bigotimes_{a=1}^k \Theta_a$ where $\Theta_a = \Sigma_a^{-1}$. Given a tuple $\samp$ of samples  $\samp_1,\dots,\samp_n\in\R^D$, up to an additive constant the log likelihood is $
  \ell_{\samp}(\Theta)
% = \frac n2 \log \det(\Theta/2\pi) - \sum_{i=1}^n \frac12 \braket{\samp_i, \Theta \samp_i} ~
  = \frac n2 \log \det \Theta - \frac12 \sum_{i=1}^n \braket{\samp_i, \Theta \samp_i}$, which we can write 
\begin{align*}
  \ell_{\samp}(\Theta_1, \dots, \Theta_k)
% = \frac n2 \log \det(\Theta/2\pi) - \sum_{i=1}^n \frac12 \braket{\samp_i, \Theta \samp_i} ~
  = \frac {n D} 2 \sum_{a = 1}^k \frac{1}{d_a} \log \det \Theta_a  - \frac12 \sum_{i=1}^n \braket{\samp_i, \left( \textstyle \bigotimes_{a=1}^k \Theta_a \right) \samp_i},
\end{align*}
One notes that $\Theta_a$ are not identifiable; we assume $\Theta_a$ have the same determinant in order to ensure identifiability. The maximum likelihood estimator (MLE) for $\Theta_a$ is given by 
$$\widehat{\Theta}_1, \dots, \widehat{\Theta}_k := \underset{|\Theta_1| = |\Theta_2|= \dots = | \Theta_d|}{ \arg\max} \ell_x(\Theta_1, \dots, \Theta_k).$$



The flip-flop algorithm has been put forth in \CF{cite dutilleul} to compute the MLE. For simplicity we describe it for the matrix normal model $(k = 2)$, so that $\samp_1, \dots, \samp_n$ can be viewed as $d_1\times d_2$ matrices. Initialize $\overline{\Theta}_a = I_a$, and choose a tolerance $\eps > 0$. 
\begin{enumerate}
%\item Normalize $\samp \leftarrow \samp/\|\samp\|$. 
\item Set $\overline{\Theta}_1 \leftarrow (\frac{1}{n d_2} \sum_{i = 1}^n \samp_i \overline{\Theta}_2 \samp_i^T)^{-1}.$ 
\item Set $\Upsilon = \frac{1}{n d_1} \sum_{i = 1}^n \samp_i^T \overline{\Theta}_1 \samp_i$.  If $d_F( \Upsilon; \overline{\Theta}_2) > \eps,$ set $\overline{\Theta}_2 \leftarrow \Upsilon^{-1}$ and return to Step 1; else output $\widehat{\Theta}_1, \widehat{\Theta}_2$.
\end{enumerate}

%View the sample $\samp \in \R^{n D}$ as a $d_1\times \dots \times d_k \times n$ array. 

%\begin{enumerate}\item \item Normalize $\samp \leftarrow \samp/\|\samp\|$.  \item Compute the $k$ different flattenings $M_1,\dots, M_k$ of $\samp$ described above; set $M_1 \leftarrow M_1 \sqrt{\Theta_2 \ot \dots \ot \Theta_k}$ (and analogously for $M_1, \dots, \end{enumerate}

%however it is identifiable if we assume  $\Theta_a$ have the same determinant. 



\subsection{Results}




We may now state precisely our result for the tensor normal models. \CF{We should think a bit about the best parametrization of these bounds. I think they only match the fantasy bounds for the largest marginal.}
\begin{theorem}[Tensor normal Frobenius error]\label{thm:tensor-frobenius} Suppose $\eps \leq c/\sqrt{d_k}$ and the number of samples satisfies $n \geq C k d_k^3/D\eps^2$. The MLE $(\widehat{\Theta}_1, \dots, \widehat{\Theta}_k) $ for $(\Theta_1, \dots \Theta_k)$ from $n$ independent samples of the tensor normal model satisfies
$$ d_{F}(\widehat{\Theta}_a, \Theta_a) = O\left( \sqrt{\frac{d_a d_k} {n D/d_k}}\eps\right) $$
for $a \in [k]$ with probability at least 
$$1 - k^2 \left(\sqrt{nD} / kd_1 \right)^{ - \Omega(d_1)} - O(k e^{ - \eps^2 d_k / k}).$$
\end{theorem}
 In the case of matrix normal models $(k=2)$, we obtain the following stronger result.

\begin{theorem}[Matrix normal model spectral error]\label{thm:matrix-normal} Suppose $d_1 \leq d_2$,
$$n \geq C (d_2/d_1) \max\{\log (d_2/d_1),  \log^2(d_1) \eps^{-2}\}$$ for $C$ a large enough constant and $\eps < c$ for a small enough constant $c$. The MLE $(\widehat{\Theta}_1,\widehat{\Theta}_2) $ for $(\Theta_1, \Theta_2)$ from $n$ independent samples of the matrix normal model satisfies
$$ d_{op}(\widehat{\Theta}_a, \Theta_a) = O\left(\eps \sqrt{\frac{d_2}{n d_1}} \log d_1\right) $$
for $a \in \{1,2\}$ with probability $1 - O(e^{ - \Omega( d_2 \eps^2/\log^2 d_1)})$.
\end{theorem}

Our next results show that in either case the flip-flop algorithm can find the MLEs with high probability. \TODO{change flip flops to thetas}

\begin{theorem}[Tensor flip-flop]\label{thm:tensor-flipflop} If $\hat{\Sigma}$ denotes the MLE estimator for $\Sigma$, then provided $n \geq \TODO{}$ the flip-flop algorithm computes $\underline{\Sigma}$ with
$$ d_F(\underline{\Sigma}, \hat{\Sigma}) \leq \eps $$
in $O(\TODO{\log(1/\eps)})$ iterations with probability at least $\TODO{}$.
\end{theorem}

\begin{theorem}[Matrix flip-flop]\label{thm:matrix-flipflop} If $(\hat{\Theta}_1, \hat{\Theta}_2) $ denotes the MLE estimator for $(\Theta_1, \Theta_2)$, then provided $n \geq \TODO{}$ the flip-flop algorithm computes $(\underline{\Theta}_1, \underline{\Theta}_2)$ with
$$ d_F(\underline{\Theta}_i, \hat{\Theta}_i) \leq \eps $$
for $i \in \{1,2\}$ in $O(\TODO{\log(1/\eps)})$ iterations with probability at least $\TODO{}$.
\end{theorem}

One may wonder why in the above theorems we consider the distances for the individual tensor factors and not the covariance matrix itself, but tight bounds for the covariance matrix itself follow from the above bounds (apart from the logarithmic factor in the matrix normal case and a constant factor in general).




\section{Geodesic convexity and sample complexity}\label{subsec:outline}

%-----------------------------------------------------------------------------
%\subsection{Sample complexity from geodesic convexity}\label{subsec:outline}
%-----------------------------------------------------------------------------

Following \cite{FM20}, and in spirit also \CF{cite weisel}, we use geodesic convexity to prove \cref{thm:tensor-frobenius}.
\Cref{thm:matrix-normal}, proven later in \cref{sec:matrix-normal}, requires the stronger notion of \emph{quantum expansion}. Though the likelihood function is geodesically convex, we find it somewhat easier to work with a different potential function $f_{\samp}$ to be defined in the next section. We now review the notion of geodesic convexity. 



%\CF{technically $\Theta(t)$ is ill-defined} Then, the curve $\Theta(t) = A^T e^{tZ} A = (A_1^T e^{t Z_1} A_1, \dots, A_k^T e^{t Z_k} A_k)$ is a geodesic or `straight line' through $\Theta = A^T A$ with direction~$Z$ for a natural Riemannian metric on~$P$~\cite{bhatia2009positive}.


\begin{definition}[Geodesic convexity]\label{dfn:g-convexity}
 Let $\Theta \in \SPD$. The \emph{exponential map} $\exp_\Theta:\Sym \to \SPD$ at $\Theta$ is defined by 
$$ \exp_\Theta(Z) = \Theta^{1/2} e^{\sqrt{\vec d}Z} \Theta^{1/2},$$
where $\sqrt{\vec d} Z$ denotes the componentwise product $(\sqrt{d_1} Z_1, \dots, \sqrt{d_k}Z_k)$.
The \emph{geodesics} through $\Theta$ are the curves $t\mapsto \exp_\Theta(tZ)$ for $t \in \R$. These curves are in fact geodesics, or ``straight lines,'' through $\Theta$ for a natural Riemannian metric on~$\SPD$~\cite{bhatia2009positive}. \CF{fisher-rao}
% The length of the curve given by restricting $\gamma$ to $[0,1]$, or $\gamma|_{[0,1]}$, is defined to be $\|Z\|_F$. 


A function $f\colon \SPD \to \R$ is said to be \emph{geodesically convex} if the function 
$$\R \to \R, \quad t \mapsto f (\exp_\Theta( tZ))$$ is convex for every $\Theta \in \SPD, Z \in \Sym$, i.e. the function $f \circ \gamma$ is convex for every geodesic $\gamma$. Equivalently, assuming $f$ is twice differentiable, $\partial^2_t f(\exp_\Theta(tZ)) \geq 0.$ 

Further, say $f$ is \emph{$\lambda$-strongly geodesically convex} at a point $\Theta \in \SPD$ if, for some (equivalently, for any) $A \in \SL$ with $\Theta = A^T A$ and for any $Z \in \Sym$, we have
\begin{align*}
  \partial^2_{t=0} f(\exp_\Theta(tZ)) \geq \lambda \norm{Z}^2.
\end{align*}
% and \emph{$L$-geodesically smooth} with respect to the norm $\norm\cdot$ if
% \begin{align*}
%   \partial^2_t f(A^T e^{t Z} A) \leq L \norm{Z}^2 \qquad (\forall Z\in S).
% \end{align*}
%Here we use the norm $\norm{\cdot}_{\vec d}$ induced by the rescaled Hilbert-Schmidt inner product $\braket{X, Y}_{\vec d} = \sum_{a=1}^k \frac1{d_a} \tr(X_a Y_a)$ for $X, Y \in \Sym$.
\end{definition}

The function $f_{\samp}$ defined in \cref{dfn:function} is geodesically convex~\cite{burgisser2019towards}. 
%\CF{see the commented out motivation. Where to put it?}
%\MW{Write $\norm Z_F^2 = \sum_i \norm{Z_i}_F^2$???}
%One might wonder why we define $\exp_\Theta$ using the componentwise multiplication by $\sqrt{\vec d}$. In fact, this is equal to the exponential map for the Riemannian metric $\braket{X, Y}_{\vec d} = \sum_{a=1}^k \frac1{d_a} \tr(X_a Y_a)$. Oddly enough, defining the gradient and Hessian according to this metric, rather than the usual Hilbert-Schmidt inner product, improves the ratio between the norm of the gradient of $f_{\samp}$ and its strong geodesic convexity parameter - a ratio which controls the quality of our statistical estimate. 
%RMO: Is this oddity simply because of the structure of the g-convex function that we are studying, the fact that it is a function over $\otimes \Theta_i$ and therefore we will get these $d_i$'s naturally? (todo for myself: do these calculations and see)
We also note that $f$ has a certain useful transformation property or ``equivariance." For all $\Theta \in \SPD$, $A \in \SL$, and $\samp=(\samp_1,\dots,\samp_n)$, we have that
\begin{align}\label{eq:equivariance}
  f_{A^{-1} \samp}(A^T \Theta A) = f_{\samp}(\Theta),
\end{align}
where we write $A^{-1} \samp = (A^{-1} \samp_1,\cdots,A^{-1} \samp_n)$.
With these definitions in place, we are able to state the rather straightforward plan.

\begin{enumerate}
\item\label{it:reduce} \textbf{Reduce to identity:}
Firstly, \cref{eq:equivariance} implies that $\widehat\Theta_{\Theta^{-1/2} \samp} = \Theta^{1/2} \widehat\Theta_{\samp} \Theta^{1/2}$ assuming either minimizer exists and is unique.
Therefore,
\begin{align*}
  d_F(\widehat\Theta_{\Theta^{-1/2} \samp}; \Theta)
= \norm{ I - \Theta^{-1/2} \widehat\Theta_{\Theta^{-1/2} \samp} \Theta^{-1/2} }_F
% = \norm{ I - \widehat\Theta_{\Theta^{1/2} \samp} }_F
= d_F(\widehat\Theta_{\samp}; I)
\end{align*}
and likewise for the distance $d_{\op}$.
Now if $\rv$ is distributed as $n$ independent samples from a standard Gaussian, $\Theta^{-1/2} \rv$ is distributed as $n$ independent samples from $\cN(0, \Theta^{-1})$.
This shows that to prove \cref{thm:tensor-frobenius} it is enough to consider the case that $\Theta = I$ and $\rv$ is standard Gaussian.
\item\label{it:grad} \textbf{Bound the gradient:}
Show that the gradient $\nabla f_{\rv}(I)$ (defined below) is small with high probability.
\item\label{it:convexity} \textbf{Show strong convexity:}
Show that, with high probability, $f_{\rv}$ is $\Omega(1)$-strongly geodesically convex near $I$.
\end{enumerate}

These together imply the desired sample complexity bounds -- as in the Euclidean case, strong convexity in a suitably large ball about a point implies the optimizer cannot be far.
Furthermore, like in the Euclidean case, if a function is strongly geodesically convex in a region and one can prove a \emph{descent lemma} for a sequence in the region, then it must converge exponentially quickly to an optimizer.
It is well-known that various optimization algorithms applied to $f_{\samp}$ satisfy such a descent lemma~\cite{burgisser2019towards}.

To make this discussion more concrete, we now define the gradient formally and state the lemma that we will use to relate the gradient and strong convexity to the distance to the optimizer as in the plan above.

\begin{definition}[Riemannian gradient]
For $\Theta \in \SPD$, the \emph{Riemannian gradient}~$\nabla f(\Theta)$ is the unique element in $\Sym$ such that
\begin{align*}
  \braket{\nabla f(\Theta), Z} = \partial_{t=0} f(\exp_{\Theta}(Zt))
\end{align*}
for all $Z\in \Sym$.
We often abbreviate $\nabla f = \nabla f(I)$ and write $\nabla f = (\nabla_1 f, \dots, \nabla_k f)$.
\end{definition}

\noindent
\CF{highlight or put in notation}Now say that $B_\kappa(\Theta) = \{ \exp_{\Theta}(Z) : \norm Z \leq \kappa \}$ is the ball of radius $\kappa>0$ about~$\Theta$.

\begin{lemma}\label{lem:convex-ball}
Let $f\colon \SPD\to \R$ be geodesically convex everywhere, and $\lambda$-strongly geodesically convex in a geodesic ball $B_\kappa$ of radius~$\kappa>0$ about~$I$.
Further assume the Riemannian gradient satisfies $\norm{\nabla f(I)} \leq \eps < \lambda \kappa / 2$.
Then the sublevel set $\{\Theta:f(\Theta) \leq f(I)\}$ is contained in the ball $B_{\eps/\lambda} \subset B_\kappa$, and $f$ has a unique minimizer $\Theta^*$ which is contained in $B_{\eps/\lambda}$, and 
$$ f(\Theta^*) \geq f(I) - \eps^2/2 \lambda.$$
\end{lemma}

%\RMO{We also need to add here that the sublevel set of $f(I)$ is contained in $B_{\kappa}(I)$, which we prove in the middle of the lemma anyways. Is that okay?}

\begin{proof}
We first show that $f$ has a minimum.
Consider $g(t) := f(\exp_I(tZ))$, where $Z\in \Sym$ is an arbitrary vector of unit norm~$\norm Z = 1$.
Then, using the assumption on the gradient,
\begin{align}\label{eq:grad bound}
  g'(0)
= \partial_{t=0} f(\exp_{I}(tZ))
= \braket{\nabla f(I), Z}
\geq -\norm{\nabla f(I)} \, \norm Z
\geq -\eps.
\end{align}
Since $f$ is $\lambda$-strongly geodesically convex on $B_\kappa(I)$, we have $g''(t) \geq \lambda$ for all $\abs t\leq\kappa$. It follows that 
for all $0 \leq t \leq \kappa$ we have $g(t) \geq g(0) - \eps t + \lambda t^2/2$; plugging in $t = \kappa$ yields 
\begin{align*}
  g(\kappa)
%\geq g(0) + g'(0) \kappa + \lambda \frac{\kappa^2}2
\geq g(0) - \eps \kappa + \lambda \frac{\kappa^2}2
= g(0) + \left( \frac{\lambda\kappa}2 - \eps \right) \kappa
> g(0)
\end{align*}
Since $g$ is convex due to the geodesic convexity of $f$, it follows that, for any~$t \geq \kappa$,
\begin{align*}
  g(0) < g(\kappa) \leq \left( 1-\frac{\kappa}t \right) g(0) + \frac{\kappa}t g(t),
\end{align*}
hence
\begin{align*}
  f(I) = g(0) < g(t) = f(\exp_{I}(tZ)).
\end{align*}
It follows that the sublevel set of~$f(I)$ is contained in the ball of radius~$\kappa$, which is a compact set.
In particular, $f$ has a minimizer $\Theta^*$ in this ball. Moreover, the quantity $ g(0) - \eps t + \lambda t^2/2$ takes a minimum at $t = \eps/ \lambda$, and so $g(t) \geq g(0) - \eps^2/2\lambda$ for all $t \leq \kappa$. By definition of $g$, this tells us that $f(\Theta^*)$ is at least $f(I) - \eps^2/2\lambda$.

Next, we prove that any minimizer of $f$ is necessarily contained in the (smaller) ball of radius~$\eps/\lambda$.
To see this, take an arbitrary minimizer and write it in the form $\exp_I(TZ)$, where $Z\in \Sym$ is a unit vector and $T>0$.
As before, we consider the function $g(t) = f(\exp_I(tZ))$.
Then, using \cref{eq:grad bound}, the convexity of~$g(t)$ for all $t\in\R$ and the strong convexity of $g(t)$ for $\abs t \leq \kappa$, we have
\begin{align*}
  0 = g'(T) = \int_0^T g''(t) \, dt + g'(0) \geq \lambda \min(T, \kappa) - \eps.
\end{align*}
If $T>\kappa$ then we have a contradiction as $\lambda\kappa - \eps > \lambda\kappa/2 - \eps > 0$.
Therefore, we must have $T\leq\kappa$ and hence $\lambda T - \eps \leq 0$, so $T \leq \eps/\lambda$.
Thus we have proved that any minimizer of $f$ is contained in the ball of radius $\eps/\lambda$.

%\CF{just track down some ref, or proceed as in commented text.} 
%To see that the minimimizer $\Theta$ is unique, we observe that geodesic strong convexity implies strong star convexity of the function $f \circ \exp_{\Theta}$, which has a unique global minimizer at $0$ if and only if $\Theta$ is the unique minimizer of $f$. It is easy to see that function that is strongly star convex in a neighborhood of the origin is uniquely minimized at the origin.

We still need to show that the minimizer is unique; that this follows from strong convexity is convex optimization ``folklore,'' but we include a proof nonetheless. Indeed, suppose that $\Theta^*$ is a minimizer and let $Z\in \Sym$ be arbitrary.
Consider $h(t) := f(\exp_{\Theta^*}(tZ))$.
Then the function $h(t)$ is convex, has a minimum at $t=0$, and satisfies $h''(0) > 0$, since $f$ is $\lambda$-strongly geodesically convex at $\Theta^*$ by what we showed above.
It follows that $h(t) > h(0)$ for any $t\neq0$.
Since $Z$ was arbitrary, this shows that $f(\Theta') > f(\Theta^*)$ for any $\Theta'\neq\Theta^*$.
\end{proof}


%-----------------------------------------------------------------------------
\subsection{Our potential function}
%-----------------------------------------------------------------------------



It is more convenient to maximize a different function than the log-likelihood $\ell_{\samp}$. Recall that for a centered normal distribution~$\cN(0,\Sigma)$ with $D\times D$ covariance matrix~$\Sigma$, the log-likelihood function for given samples $\samp_1,\dots,\samp_n\in\R^D$ is (up to an additive constant)
\begin{align*}
  \ell_{\samp}(\Theta)
% = \frac n2 \log \det(\Theta/2\pi) - \sum_{i=1}^n \frac12 \braket{\samp_i, \Theta \samp_i} ~
  = \frac n2 \log \det \Theta - \frac12 \sum_{i=1}^n \braket{\samp_i, \Theta \samp_i},
\end{align*}
where $\Theta$ denotes the precision matrix $\Sigma^{-1}$. \CF{skip this, go straight to the scale-invariant for tensors?}

We instead maximize the scale-invariant function
\begin{align}\label{eq:tilde ell}
  \tilde\ell_{\samp}(\Theta) = \frac1D\log\det \Theta - \log \sum_{i=1}^n \braket{\samp_i, \Theta \samp_i}.
\end{align}
Indeed, the two functions have the same maximizers up to rescaling.%
\footnote{To see this, write $\Theta = \lambda \Theta'$ for $\lambda>0$ and $\Theta'\in\PD_D^1$ and maximize over~$\lambda$.
The result is $\lambda = n D / \sum_{i=1}^n \braket{\samp_i, \Theta \samp_i}$, hence $\sup_{\Theta\in\PD_D} \ell_{\samp}(\Theta) = \sup_{\Theta'\in\PD_D^1} \frac {nD}2 \left( \log(nD) - 1 \right) + \frac{nD}2 \tilde\ell_{\samp}(\Theta')$.}
One may think of \cref{eq:tilde ell} as proportional, up to an additive constant, to the log-likelihood of $\Sigma$ after receiving $[\samp_1, \dots, \samp_n]$, the equivalence class of the tuple of samples in projective space.

Both log-likelihoods are geodesically concave, as will be defined precisely in the next section, but only the latter function has geodesically Lipschitz gradients, i.e., is geodesically smooth~\cite{burgisser2019towards}.
This suggests it is more amenable to geodesically convex optimization, our main tool in this work.

The tensor normal model for dimensions~$\vec d=(d_1,\dots,d_k)$ \CF{make unbold?} consists of the normal distributions~$\cN(0,\Sigma)$, where $D = \prod_{a=1}^k d_a$ and the covariance matrix is a Kronecker product $\Sigma = \bigotimes_{a=1}^k \Sigma_a$ of positive definite $d_a\times d_a$-matrices $\Sigma_a$.
Let $\Theta_a = \Sigma_a^{-1}$, so that the precision matrix is given by $\Theta = \bigotimes_{a=1}^k \Theta_a$.
Then the same argument as above, restricted to precision matrices of this form, shows that to compute the MLE we may maximize the function
\begin{align*}
  \tilde\ell_{\samp}(\Theta_1,\dots,\Theta_k) = \sum_{i=1}^k \frac1{d_a} \log \det \Theta_a - \log \sum_{i=1}^n \braket{\samp_i, \left( \textstyle \bigotimes_{a=1}^k \Theta_a \right) \samp_i},
\end{align*}
or equivalently minimize its negation.
Since this function is also invariant under rescaling the~$\Theta_a$, we may restrict to $\PD_{d_a}^1$, the $d_a\times d_a$ positive definite matrices with unit determinant.
Thus we are led to minimizing the following function:

\begin{definition}[Objective function]\label{dfn:function}
Given $n$ samples $\rv = (\samp_1, \dots, \samp_n)$ in $\R^D$, define the function $f_{\samp}\colon \SPD \to \R$ by
\begin{align}\label{eq:projective-likelihood}
  f_{\samp}(\Theta_1,\dots,\Theta_k) = \log \sum_{i=1}^n \braket{\samp_i, \left( \textstyle \bigotimes_{a=1}^k \Theta_a \right) \samp_i}.
\end{align}
Let $(\widehat{\Theta}_{\samp,1}, \dots, \widehat{\Theta}_{\samp,k})$ denote the minimizer of $f_{\samp}$ if it exists and is unique.
\end{definition}

\noindent
As explained above, $\widehat{\Theta}_{\samp} = \widehat{\Theta}_{\samp,1} \ot \cdots \ot \widehat{\Theta}_{\samp,k}$ is directly related to the maximum likelihood estimator for the tensor normal model (namely, by an known \CF{careful; is not automatically known}) overall multiplicative factor).
We will often identify $\Theta$ with the tuple $(\Theta_1,\dots,\Theta_k)$, and similarly for $\widehat{\Theta}_{\samp}$.
When $\samp$ is clear from context we will drop it.

%-----------------------------------------------------------------------------
\subsection{Bounding the gradient}
%-----------------------------------------------------------------------------
As suggested in the plan from the previous section, we now compute the gradient of $f_{\samp}$ and bound it using basic matrix concentration results.

To calculate the gradient, we need a definition from linear algebra.

\begin{definition}[Partial trace]\label{def:single marginal}
For $\rho \in \PD_D$ and $a\in[k]$, let $\rho^{(a)} \in \PD_{d_a}$ denote the \emph{partial trace} defined by the condition that
\begin{align*}
  \tr \rho^{(a)} Z = \tr \rho Z_a,
\end{align*}
for all $d_a \times d_a$ matrices $Z$, where $Z_a = (I_1 \ot \cdots \ot I_{a-1} \ot Z \ot I_{a+1} \ot \cdots \ot I_k)$.
Here and in the following we write $I_a$ for the $d_a\times d_a$ identity matrix. \CF{can I suggest $Z_{(a)}$ instead to avoid the confusion with components below?}
\end{definition}

\MW{Explain how to compute in terms of $\samp$. Ref above \cref{cor:vershynin}.}

\begin{lemma}[Riemannian gradient]\label{lem:gradient}
Let $\rho = \sum_{i=1}^n \samp_i \samp_i^T / \norm{\samp}_2^2$.
Then the components of the Riemannian gradient $\nabla f_{\samp}$ at the identity are given by
\begin{align*}
  \nabla_a f_{\samp} = \sqrt{d_a} \rho^{(a)} - I_a/\sqrt{d_a} \qquad \text{ for } a \in [k].
\end{align*}
\end{lemma}
\begin{proof}
  Let $F_{\samp}(\Theta) = \tr \rho \, \Theta$.
  Then, for all $a\in[k]$, we have
  \begin{align*}
   \langle \nabla_a F_{\samp} , Z \rangle
  % = \partial_{t=0} F_{\samp}(I_1,\dots,I_{a-1}, e^{tZ}, I_{a+1},\dots,I_n)
  = \partial_{t=0} \tr \rho \, e^{t \sqrt{d_a} Z_a}
  = \sqrt{d_a} \tr \rho Z_a
  = \sqrt{d_a} \tr \rho^{(a)} Z
  \end{align*}
  for all traceless symmetric $d_a \times d_a$ matrices $Z$.
  Since $\nabla_a F_{\samp}$ is traceless and symmetric by definition, this implies
  \begin{align*}
    \nabla_a F_{\samp}
  = \sqrt{d_a} \rho^{(a)} - \tr(\rho^{(a)}) I_a /\sqrt{d_a}
  = \sqrt{d_a} \rho^{(a)} - I_a/\sqrt{d_a}.
  \end{align*}
  Finally, note that $f_{\samp}(\Theta) = \log F_{\samp}(\Theta) + \log \norm{\samp}_2^2$, so
  \begin{align*}
    \nabla_a f_{\samp}
  = \frac{\nabla_a F_{\samp}}{F_{\samp}(I)}
  = \nabla_a F_{\samp}
  = \sqrt{d_a} \rho^{(a)} - I_a/\sqrt{d_a},
  \end{align*}
  using that $F_{\samp}(I) = \tr \rho = 1$.
\end{proof}

Having calculated the gradient, we are ready to state our bound:
%\MW{Please check updated proof. Can we assume $\eps<1$ and simply take~$C=16$?}
\MW{Make pretty by manipulating $C$}
\begin{prop}\label{prop:gradient-bound}
Let $\rv = (\rv_1,\dots,\rv_n)$ be independent standard Gaussian random variables in $\R^D$, where $D=d_1\cdots{}d_k$.
Let $\eps>0$ and $C = 16^2\max\{\eps^2,1\}$.
Suppose that, for all $a \in [k]$,
\begin{align*}
  N_a := \frac{n D}{d_a} \geq C \frac{d_a}{\eps^2}.
\end{align*}
Then, with probability at least $1 - 2 \sum_{a=1}^k e^{-N_a \eps^2/2C}$ we have that
\begin{align*}
  \norm{\nabla_a f_{\rv}}_{\op} \leq \eps/\sqrt{d_a}
\end{align*}
for all $a\in[k]$, and hence $\norm{\nabla f_{\rv}}^2 \leq k \eps^2$.
\end{prop}

To prove this we will need a standard result in matrix concentration.
Recall that $\nabla_a f_{\samp} = \sqrt{d_a} \rho^{(a)} - I_a/\sqrt{d_a}$ by \cref{lem:gradient}.
It is not hard to see that $\rho^{(a)}$ is distributed as $Y Y^T / \norm{Y}_F^2$, where $Y$ is a $d_a \times N_a$ matrix with independent Gaussian entries.
The following result bounds the singular values of such random matrices.

\begin{theorem}[Corollary 5.35 of \cite{vershynin2010introduction}]\label{cor:vershynin}
Let $Y \in \R^{d \times N}$ have independent standard Gaussian entries.
Then, for $t \geq 0$, the following occurs with probability at least $1 - 2 \exp(-t^{2}/2)$:
\begin{align*}
  \sqrt{N} - \sqrt{d} - t \leq s_{\min}(Y) \leq s_{\max}(Y) \leq \sqrt{N} + \sqrt{d} + t,
\end{align*}
where $s_{\min}$ and $s_{\max}$ denote the smallest and largest singular value, respectively (we allow for singular values that are zero).
\end{theorem}

\begin{proof}[Proof of \cref{prop:gradient-bound}]
We first consider a fixed $a\in[k]$ and later take a union bound.
Let $d = d_a$ and $N = N_a = n D/d_a$.
Again, $\nabla_a f_{\rv} = \sqrt{d_a} \rho^{(a)} - I_a/\sqrt{d_a}$ where $\rho^{(a)}$ is distributed as $Y Y^T/\|Y\|_F^2$, where $Y$ is a $d \times N$ matrix with independent Gaussian entries.
By \cref{cor:vershynin}, we have the following with $\leq 2 \exp(-t^2/2)$ failure probability:
\begin{align*}
  \sqrt{N} \left( 1 -  \frac{\sqrt{d} + t }{\sqrt{N}}  \right)\leq s_{\min}(Y) \leq s_{\max}(Y) \leq  \sqrt{N} \left( 1 + \frac{\sqrt{d} + t }{\sqrt{N}} \right).
\end{align*}
Let $t = \sqrt{N} \eps / \sqrt{C}$.
Because $d \leq N \eps^2 / C$, this shows that all singular values are in $\sqrt{N} \left( 1 \pm 2\eps/\sqrt{C} \right)$.
It follows that the eigenvalues of $YY^T$ are in $N \left( 1 \pm 8\eps/\sqrt{C} \right)$.
Since there are at most $d \leq N$ nonzero eigenvalues, this implies in turn that the eigenvalues of $Y Y^T/\|Y\|_F^2$ are in
\begin{align*}
\frac {\left( 1 \pm 8\eps/\sqrt{C} \right)} {d \left( 1 \pm 8\eps/\sqrt{C} \right)} \subseteq \frac1d \left( 1 \pm 16\eps/\sqrt{C} \right) \subseteq \frac1d \left( 1 \pm \eps \right),
\end{align*}
Thus,
$\norm{\nabla_a f_{\rv}}_{\op} \leq \eps/\sqrt{d_a}$
with failure probability at most $2 \exp(-N \eps^2/2C)$.
Applying the union bound over $[k]$ completes the proof of the proposition.
% Next, observe that $\|Y\|_F^2$ is a $\chi^2$-distribution with $Nd$ degrees of freedom, and thus satisfies
% \begin{align*}
%   \norm Y_F^2 - Nd \leq tNd
% \end{align*}
% with failure probability at most $e^{-Ndt^2/18}$ for $t \in [0,3]$~\cite[(2.44)]{W19}.
% WAS: $\| Y\|_F^2 - Nd \leq t N d$ with probability $1 - 2e^{- Nd t^2/8}$ for $t \leq 1$ \cite{W19}.
% Thus
  % $$\lambda_1(\rho^{\{a\}}), \lambda_2(\rho^{\{a\}}) \in  \cN(1 \pm c \cdot \eps)/ Nd(1 \pm c \cdot \eps) \in \frac{1}{d}(1 \pm \eps)$$
% with probability $1 - 2 e^{- c N \eps^2}- 2 e^{- c Nd \eps^2} = 1 - 4 e^{- N\eps^2},$ provided $c$ is small enough.
\end{proof}

It will also be convenient to have bounds on the norm of $X$; because $\|X\|_F^2$ is simply a sum of $\chi$-square random variables, the next proposition follows from standard concentration bounds (e.g. \cite{W19}).
\begin{prop}[Norm of $X$]\label{prp:xnorm}
Let $\rv = (\rv_1,\dots,\rv_n)$ be independent standard Gaussian random variables in $\R^D$, where $D=d_1\cdots{}d_k$. For $t \leq c$,
$$ (1 - t) nD \leq \|X\|_F^2 \leq (1 + t)nD$$ with probability $2e^{-t^2 nD/8}$.
\end{prop}

% Both $\|Y Y^\dagger - N I\|_{op} \leq  c N/\log d_a$ and $\| \samp\|_F^2 - Nd_a \leq .5 N d_a$ hold with probability \CF{blah}$. Now  $$ \|\rho^{\{a\}} - I_a/d_a\|_{op} \leq \|\rho^{\{a\}} - N I_{d_a}/ \|\samp\|_F\|_{op} + $$


%Provided $c$ is small enough we then have
 %$$\|Y Y^\dagger - N I\|_{op} \leq  c N/\log d_a.$$


% Next, observe that $\|\samp\|_F^2$ is a $\chi$-squared distribution with $Nd_a$ degrees of freedom, and thus satisfies $\| \samp\|_F^2 - Nd_a \leq .5 N d_a$ with probability $1 - 2e^{- c N d_a}$. Both $\|Y Y^\dagger - N I\|_{op} \leq  c N/\log d_a$ and $\| \samp\|_F^2 - Nd_a \leq .5 N d_a$ hold with probability \CF{blah}$. Now  $$ \|\rho^{\{a\}} - I_a/d_a\|_{op} \leq \|\rho^{\{a\}} - N I_{d_a}/ \|\samp\|_F\|_{op} + $$



%-----------------------------------------------------------------------------
\subsection{Strong convexity}
%-----------------------------------------------------------------------------
%\CF{will move this to the appropriate place; rafael you can cite this for section 4 and just eventually put in the right probability bound.}




We now prove our main strong convexity result, \cref{thm:ball-convexity}, in order to carry out step~\ref{it:convexity} of the plan from \cref{subsec:outline}. The theorem states that, with high probability, $f_X$ is strongly convex for $Z$ sufficiently close to the identity.

%\begin{theorem}\label{thm:ball-convexity}There is $c> 0$ such that the following holds: with probability at least $1 - k^2 \left(\frac {\sqrt{nD}}{kd_1}\right)^{ - \Omega(d_1)}$, the function $f_{\rv}$ is $1/2$-strongly convex at all points $Z\in \PD$ with $\sum_{i = 1}^n \|\log Z_a\|_{op} < c/k.$\end{theorem}


\begin{theorem}\label{thm:ball-convexity} There are constants $C,c>0$ such that the following holds. For $n \geq C k^2 d_1^2/D$, with probability at least $1 - k^2 \left(\frac {\sqrt{nD}}{kd_1}\right)^{ - \Omega(d_1)}$, $f$ is $1/2$ strongly convex at any point $Z \in \SPD$ such that $\sum_a \|\log Z_a\|_{op} \leq c$ for all $a \in [k]$. \CF{define the ell-infty ball}
\end{theorem}


%$O\left( \sum_a e^{ - \Omega(nD/d_a)}\right)\leq k e^{- \Omega(nD/d_1)}$

Just like in the Euclidean case, the Hessian is convenient to characterize strong convexity, and we begin by defining it formally in the Riemannian context. \cref{thm:ball-convexity} follows straightforwardly from a bound on the convexity \emph{at} the origin,  \cref{thm:tensor-convexity} , and a bound on how the Hessian changes away from the origin, \cref{convexRobustness}. We assemble these results to prove \cref{thm:ball-convexity} at the end of this subsection. %\CF{Shall I actually prove theorem 9? yeah, I think so.}



%We next show strong convexity \emph{at} the origin, and finally show strong convexity \emph{near} the origin.

\begin{definition}[Riemannian Hessian]
For $\Theta \in \SPD$, the \emph{Riemannian Hessian}~$\nabla^2 f(\Theta)$ is the unique linear operator on $\Sym$ such that
\begin{align*}
  \braket{Y, \nabla^2 f(\Theta) Z} = \partial_{s=0} \partial_{t=0} f(\exp_{\Theta}(sY + tZ))
\end{align*}
for all $Y, Z\in \Sym$.
We often abbreviate $\nabla^2 f = \nabla^2 f(I)$.
As a block matrix,
\begin{align*}
  \nabla^2 f = \begin{bmatrix}
  \nabla_{11}^2 f & \dots & \nabla_{1k}^2 f \\
  \vdots & \ddots & \vdots \\
  \nabla_{k1}^2 f & \dots & \nabla_{kk}^2 f \\
  \end{bmatrix},
\end{align*}
where $\nabla_{ab}^2f$ is an operator from $\smallSym_{d_b}^0$ to $\smallSym_{d_a}^0$.
\end{definition}

We note that the Riemannian Hessian is symmetric with respect to the inner product~$\braket{\cdot,\cdot}$ on $\Sym$.
Furthermore, $\braket{Z, \nabla^2 f(\Theta) Z} = \partial^2_{t=0} f(\exp_{\Theta}(tZ))$ for all $Z\in \Sym$.
Thus, $f$~is geodesically convex if and only if the Riemannian Hessian is positive semidefinite, that is, $\nabla^2 f(\Theta) \succeq 0$.
Similarly, $f$ is $\lambda$-strongly geodesically convex at $\Theta$ if and only if $\nabla^2 f(\Theta) \succeq \lambda I$, i.e., the Hessian is positive definite with eigenvalues larger than or equal to~$\lambda$.
%\MW{Here, $\succeq$ denotes the Loewner order with respect to $\braket{\cdot,\cdot}_{\vec d}$.}

Similarly as for the gradient, we can compute the components of the Hessian in terms of a partial trace, as in the following definition and lemma.

\begin{definition}[Partial trace for two-body marginals]
For $\rho \in \PD_D$ and $a \neq b\in[k]$, let $\rho^{(ab)} \in \PD_{d_a \times d_b}$ denote the \emph{partial trace} defined by the condition that
\begin{align*}
  \tr \rho^{(ab)} (Y \ot Z) = \tr \rho Y_a Z_b,
\end{align*}
for all $d_a \times d_a$ matrices $Y$ and $d_b \times d_b$ matrices $Z$, where, as before, we write~$Y_a = (I_1 \ot \cdots \ot I_{a-1} \ot Y \ot I_{a+1} \ot \cdots \ot I_k)$ and similarly for $Z_b$. %\CF{does partial trace really need to be defined twice?}
\end{definition}

\noindent
This definition is consistent with \cref{def:single marginal} in the sense that $\tr \rho^{(ab)} (Y \ot I) = \tr \rho^{(a)} Y$ and $\tr \rho^{(ab)} (I \ot Z) = \tr \rho^{(b)} Z$ for all $\rho$, $Y$, $Z$.

\begin{lemma}[Riemannian Hessian]\label{lem:hessian}
Let $\rho = \sum_{i=1}^n \samp_i \samp_i^T / \norm{\samp}_2^2$.
Then the Riemannian Hessian $\nabla^2 f_{\samp}$ at the identity is given by \CF{fix $Y_a$ here}
\begin{align*}
 \frac{1}{d_a} \langle Y,  \left( \nabla^2_{aa} f_{\samp} \right) Y \rangle
&= \left( \tr \rho^{(a)} Y^2 - \bigl(\tr Y \rho^{(a)}\bigr)^2 \right) \\
  \frac1{\sqrt{d_a d_b}} \langle Y,  \left( \nabla^2_{ab} f_{\samp} \right) Z \rangle
&= \tr \rho^{(ab)} \left( Y \ot Z \right) - \bigl(\tr Y \rho^{(a)}\bigr)\bigl(\tr Z \rho^{(b)}\bigr) 
\end{align*}
for all $a\neq b\in[k]$ and traceless symmetric $d_a\times d_a$ matrices $Y$, $d_b\times d_b$ matrices~$Z$.
\end{lemma}
\begin{proof}
  Let $F_{\samp}(\Theta) = \tr \rho \, \Theta$ as in the proof of \cref{lem:gradient}.
  Then,
  \begin{align*}
   \langle Y,  \left( \nabla^2_{aa} F_{\samp} \right) Y \rangle
  &= \partial_{s=0} \partial_{t=0} \tr \rho \, e^{(s+t)\sqrt{d_a}  Y_a} \\
  &= \partial_{s=0} \partial_{t=0} \tr \rho^{(a)} e^{(s+t)  \sqrt{d_a}Y}
  = d_a \tr \rho^{(a)} Y^2,
  \end{align*}
  while for $a\neq b$ we have
  \begin{align*}
    \langle Y,  \left( \nabla^2_{ab} F_{\samp} \right) Z \rangle
  &= \partial_{s=0} \partial_{t=0} \tr \rho \, e^{s \sqrt{d_a}Y_a + t \sqrt{d_b} Z_b} \\
  &= \partial_{s=0} \partial_{t=0} \tr \rho^{(ab)} \left( e^{s \sqrt{d_a}  Y} \ot e^{t \sqrt{d_b} Z} \right)\\
  &= \sqrt{d_a d_b} \tr \rho^{(ab)} \left( Y \ot Z \right).
  \end{align*}
  %Since $f_{\samp}(\Theta) = \log F_{\samp}(\Theta) + \log \norm{\samp}_2^2$, we have
  Since $f_{\samp}(\Theta) = \log F_{\samp}(\Theta)$ \CF{deleted a term; please see commented text to check}, we have
  \begin{align*}
    \braket{Y, (\nabla^2 f_{\samp}) Z}
  &= \frac{\braket{Y, (\nabla^2 F_{\samp}) Z}}{F_{\samp}(I)} - \frac{\braket{Y, \nabla F_{\samp}} \braket{\nabla F_{\samp}, Z}}{F_{\samp}(I)^2} \\
  &= \frac{\braket{Y, (\nabla^2 F_{\samp}) Z}}{F_{\samp}(I)} - \braket{Y, \nabla f_{\samp}} \braket{\nabla f_{\samp}, Z}.
  \end{align*}
  % \begin{align*}
  %   \nabla^2 f_{\samp}
  % = \frac {\nabla^2 F_{\samp}} {F_{\samp}(I)} - \frac{\nabla F_{\samp} \braket{\nabla F_{\samp}, \cdot}_{\vec d}}{F_{\samp}(I)^2}
  % = \frac {\nabla^2 F_{\samp}} {F_{\samp}(I)} - \nabla f_{\samp} \braket{\nabla f_{\samp}, \cdot}_{\vec d},
  % \end{align*}
  % where $\dagger$ denotes the adjoint with respect to the inner product $\braket{\cdot,\cdot}_{\vec d}$.
  Since $F_{\samp}(I)=1$, plugging in the above formulas and using \cref{lem:gradient} for the gradient completes the proof.
  % Indeed,
  % \begin{align*}
  %   \frac1{d_a} \tr Y (\nabla^2_{aa} f_{\samp}) Y
  % = \frac1{d_a} \tr Y (\nabla^2_{aa} F_{\samp}) Y - \frac1{d_a} \tr Y (\nabla_a f_{\samp}) \frac1{d_a} \tr Y (\nabla_a f_{\samp})
  % = \tr \rho^{(a)} Y^2 - (\tr Y \rho^{(a)})^2
  % \end{align*}
  % and
  % \begin{align*}
  %   \frac1{d_a} \tr Y (\nabla^2_{ab} f_{\samp}) Z
  % = \frac1{d_a} \tr Y (\nabla^2_{ab} F_{\samp}) Z - \frac1{d_a} \tr Y (\nabla_a f_{\samp}) \frac1{d_b} \tr Z (\nabla_b f_{\samp})
  % = \tr \rho^{(ab)} \left( Y \ot Z \right) - (\tr Y \rho^{(a)})(\tr Z \rho^{(b)})
  % \end{align*}
\end{proof}

%\MW{Explain that this looks like $\frac1{\norm \samp_2^2} \sum_i \samp_i \ot \samp_i - \dots$ if we think of \dots}
%\CF{most important part of hessian is this $\rho^{ab}$ part ( a channel), discuss that there are three ways to look at it: partial trace, channel (cpm), and natural representation. Cite Pisier in channel form. In appendix prove by translating to channel}







We now state our strong convexity result at the identity:

\begin{theorem}\label{thm:tensor-convexity-old}
Let $\rv = (\rv_1,\dots,\rv_n)$ be independent standard Gaussian random variables in $\R^D$, where the number of samples is $N \geq k^2 \max_{a=1}^k d_a^2 / D$. Then, with probability at least $1 - 1/{\color{red}\poly(D)}$ \TODO{propagate from \cref{cor:off-diag-hess}} , 
$$ (1 - (k - 1) \lambda - k \eps^2 - \eps) I \preceq 
 \nabla^2 f_{\rv} \preceq (1 + (k - 1) \lambda + k \eps^2 + \eps) I.$$
$$ \|\nabla^{2} f_{\rv} - I\|_{op} \leq (k - 1) \lambda + k \eps^2 + \eps $$
 In particular, $f_{\rv}$ is $\Omega(1)$ strongly convex with probability \TODO{propagate from \cref{cor:off-diag-hess}} . 
\end{theorem}

\begin{theorem}[\CF{new theorem}]\label{thm:tensor-convexity} There is an absolute constant $C$ such that the following holds. Let $\rv = (\rv_1,\dots,\rv_n)$ be independent standard Gaussian random variables in $\R^D$, where the number of samples $n$ is at least $C k^2 d_1^2/D$. Then, with probability at least $1 - k^2 \left(\frac {\sqrt{nD}}{kd_1}\right)^{ - \Omega(d_1)}$ we have
%$$ I/2 \preceq  \nabla^2 f_{\rv} \preceq 2I.$$
$$ \|\nabla^{2} f_{\rv} - I\|_{op} \leq 1/4.$$
In particular, $f_{\rv}$ is $3/4$-strongly convex at the identity.
\end{theorem}



The most interesting part of the Hessian is the quadratic form $Y,Z \mapsto \tr \rho^{(ab)} \left( Y \ot Z \right) = \langle Y, \tr_b \rho^{(ab)} \left(I \ot Z\right) \rangle$ appearing in the off-diagonal blocks. For a general state $\rho \in \PD(d_1, d_2)$, the linear map $T:\mat(d_2) \to \mat(d_1)$ corresponding to the bilinear form above, or
$$T: Z \mapsto \tr_2 \rho \left(I \ot Z\right),$$
 is known as a \emph{quantum channel}. If $\rho = \sum_{i = 1}^N x_i x_i^T$ for $x_i \in \R^{d_1 \ot d_2}$, then we have the following two equivalent ways to write $T$:
$$ T(Z) = \sum_{i = 1}^N x_i Z x_i^\dagger = \left(\sum_{i = 1}^N x_i \ot x_i\right) Z.$$ 
Here $x_i$ are viewed as $d_1 \times d_2$ matrices and $x_i^\dagger$ is the matrix transpose. In our proof of strong convexity, we will show that for $\rho = \rho^{(a,b)}$ this quantum channel has a spectral gap, or rather that $T$ has a small operator norm on the traceless symmetric matrices. 

\CF{define quantum expanders using akshay's $\|\|_0$ norm?}

Quantum channels with this spectral gap condition are called \emph{quantum expanders}, and play an important role in quantum information theory and quantum computation. Analogously to random graphs, it is known that random quantum channels, namely those obtained by choosing $x_i$ from various distributions, yield good quantum expanders. Pisier studied the case when $x_i$ are chosen from a standard Gaussian, which applies in our setting. He showed the following result. 

%We show that, restricted to the traceless matrices, this quadratic form is small. More precisely, we bound the operator norm of the map $T: Z \mapsto \tr_2 \rho^{(ab)} \left(I \ot Z\right)$ restricted to the traceless symmetric matrices. Such maps are called \emph{quantum channels}, and we will revisit their properties later \CF{where? or earlier?}. The operator norm condition we desire for $T_{\rho}$ is equivalent to $T_{\rho}$ being a good \emph{quantum expander} - this led Pisier to bound the operator norm of $T_{\rho}$ for random $\rho$. \CF{one-sentence motivation of quantum expanders?}

\begin{theorem}\label{thm:hess-pisier} Let $X = (X_1, \dots, X_N)$ be a tuple of standard Gaussians on $(\R^{d_1}\ot \R^{d_2})^n$. Then $\forall t \geq 1$ with failure probability at most $t^{-\Omega(d_{1} + d_{2})}$, the random quantum channel 
$$T: Z \mapsto \frac{1}{nD}\sum_{i = 1}^N X_i Z X_i^\dagger$$
has operator norm 
$O \left( t^{2} \frac{d_{1}+d_{2}}{\sqrt{d_{1} d_{2} nD}} \right)$ restricted to the traceless symmetric matrices. \end{theorem}
Pisier's actual result is slightly different; \cref{thm:hess-pisier} is essentially Theorem~16.6 in~\cite{pisier2012grothendieck}, together with a standard symmetrization trick (e.g., Proof of Lemma~4.1 in~\cite{P14}). We present the details in an appendix \CF{cref it}. We now apply it to bound the off diagonal terms in the Hessian.

%\begin{corollary}\label{cor:off-diag-hess} For traceless symmetric matrices $Y \in \Sym^0_{d_a}$, $Z \in \Sym^0_{d_a}$, we have 
%$O \left( \frac{d_{a}+d_{b}}{\sqrt{d_{a} d_{b} nD}} \right)$  

%$$ \langle Y,  \left( \nabla^2_{ab} f_{\samp} \right) Z \rangle
%= O (\lambda)
%$$
%\TODO{Akshay, please make the rhs $O(\lambda)$ and then adjust the failure probability accordingly}
%with probability $1 - \exp( - \Omega \left( (d_{a} + d_{b}) \log \frac{ \lambda \sqrt{n D} }{d_{a} + d_{b}}  \right) ) \geq 1 - \exp( - \Omega(d_{a} + d_{b}) )$ for $\lambda^{2} n D \gg \max_{a} d_{a}^{2}$. 
%\AR{In particular we get $\|\nabla^{2}_{ab} f\|_{op} \approx \frac{d_{a} + d_{b}}{\sqrt{n D}}$ with failure probability $\leq \exp(-\Omega(d_{a} + d_{b}))$; as well as $\|\nabla^{2}_{ab} f\|_{op} \approx \frac{d_{a} + d_{b}}{\sqrt{D}}$ with failure probability $\leq \exp(-\Omega(d_{a} + d_{b}) \log n)$ }
%\end{corollary}


\begin{corollary}\label{cor:off-diag-hess} Let $a,b \in [k]$ with $a < b$. For traceless symmetric matrices $Y \in \smallSym^0_{d_a}$, $Z \in \smallSym^0_{d_b}$, we have 
%$O \left( \frac{d_{a}+d_{b}}{\sqrt{d_{a} d_{b} nD}} \right)$  

$$ \langle Y,  \left( \nabla^2_{ab} f_{\rv} \right) Z \rangle
= O \left(s \|Y\|_F\|Z\|_F\right)
$$
%\TODO{Akshay, please make the rhs $O(\lambda)$ and then adjust the failure probability accordingly}
with probability %$1 - t^{\Omega (d_{a} + d_{b})} n^{- (d_a + d_b) \log \frac{\sqrt{D} }{d_{a} + d_{b}}}$.
$1 - \left(\frac {s \sqrt{nD}}{2d_b}\right)^{ - \Omega(d_a)} - e^{- \Omega(nD)}$.
% for $s \geq d_b /\sqrt{n D}.$

In particular, for any constant $c>0$ there is $C>0$ such that for $n \geq C k^2 d_1^2/D$ we have $\|\nabla^2_{ab} f_{\rv}\|_0 \leq c/k$ with probability $1 - k^2 \left(\frac {\sqrt{nD}}{kd_1}\right)^{ - \Omega(d_1)}$ for all $a,b \in [k]$. 


\end{corollary}






%\CF{akshay defines a notation $\|T\|_0$ for restricting operators to the traceless symmetrics. Maybe we should adopt it here also}
\begin{proof}
We proceed to bound the operator norm of $\nabla^2_{ab} f: \smallSym^0_{d_b} \to \smallSym^0_{d_a}$ for $a \neq b$. Recall from \cref{lem:hessian} that the gradient is defined by
\begin{align}\frac1{\sqrt{d_a d_b}} \langle Y,  \left( \nabla^2_{ab} f_{\rv} \right) Z \rangle
= \tr \rho^{(ab)} \left( Y \ot Z \right) - \bigl(\tr Y \rho^{(a)}\bigr)\bigl(\tr Z \rho^{(b)}\bigr).\label{eq:grad-off-diag}\end{align}
As a linear map, define the first term to be $T:Z \mapsto \sqrt{d_a d_b} \tr_b \rho^{(a,b)} (I \ot Z)$. The state $\rho^{(a,b)} = \tr_{\overline{\{a,b\}}} \rho$ is given by $\rho^{{a,b}} =  X X^\dagger/\|X\|^2$ where $X$ is viewed as an $n D/{d_a d_b} \times d_a d_b$ matrix. Equivalently, $\rho^{(a,b)}$ is distributed as 
$$\frac{1}{\sum_{i} \|Y_i\|^2} Y_i Y_i^T$$ where $Y_i$ are $n D/{d_a d_b}$ i.i.d standard Gaussians on $\R^{d_a} \ot \R^{d_b}$. By \cref{thm:hess-pisier} with $2t^2 = \frac {s \sqrt{nD}}{d_a + d_b}$, the operator norm of $T$ restricted to $\smallSym^0_{d_b}$ is $O(s n D /2 \sum_i \|Y_i\|^2 )$ 
with probability $(s \sqrt{nD}/(d_a + d_b))^{ - \Omega(d_a + d_b)} \leq (s \sqrt{nD} /d_b)^{ - \Omega(d_a)}$. The denominator is $\Theta(nD)$ with failure probability $e^{ -\Omega(nD)} $ by \cref{prp:xnorm} so finally we have that the operator norm of $T$ restricted to $\smallSym^0_{d_b}$ is $O \left( s/2\right)$.
%By \cref{eq:expansion-thing} (\CF{which needs to actually be proved}), with probability $\CF{good}$, the map $Z \to \tr_2 \rho^{(a,b)} (I \ot Z)$ has operator norm at most $\lambda/\sqrt{d_a d_b}$ restricted to traceless $Z$. 
The second term of \cref{eq:grad-off-diag} defines the quadratic form 
$$Y,Z \mapsto \sqrt{d_a d_b} (\tr Y \rho^{(a)})\bigl(\tr Z \rho^{(b)}\bigr).$$ 
As we have already shown in \cref{prop:gradient-bound}, we have $\norm{\nabla_a f_{\rv}}_{\op} = \|\sqrt{d_a} \rho^{(a)} - I_a/\sqrt{d_a}\|_{op} \leq \eps/\sqrt{d_a}$ with failure probability at most $e^{-D n \eps^2/2Cd_a}$ , and likewise for $b$. We have$$\sqrt{d_a d_b} |(\tr Y  \rho^{(a)})\bigl(\tr Z \rho^{(b)}\bigr)| \leq \left( \frac{\eps \|Y\|_{1}}{\sqrt{d_{a}}} \right) \left( \frac{\eps \|Z\|_{1}}{\sqrt{d_{b}}} \right) \leq \eps^2 \|Z\|_F\|Y\|_F.$$
Setting $\eps^2 =  s/2$, the above holds with failure probability at most $2e^{- Dn s /4Cd_b}  \leq (s \sqrt{nD} /d_b)^{ - \Omega(d_a)}.$ Taken together, the bounds for the first and second terms of \cref{eq:grad-off-diag} complete the proof. %$2 e^{ - \sqrt{D}d_2 t^2/C\sqrt{n} d_1 } \ll t^{-\Omega(d_{a} + d_{b})}.$ 
%Taken together, $\|\nabla^2_{ab} f\|_{op} \leq (\lambda + \eps^2)$ as a map $\Sym^0_{d_b} \to \Sym^0_{d_a}$.
\end{proof}
We now have the bounds we need to prove \cref{thm:tensor-convexity}.

\begin{proof}[Proof of \cref{thm:tensor-convexity}] 
We use the inequality for block matrices 
$$\begin{bmatrix} 0 & K \\ K^{*} & 0 \end{bmatrix} \succeq - \begin{bmatrix} A & 0 \\ 0 & B \end{bmatrix}$$ for any $A,B \succeq 0$ such that $\|A^{-1/2} K B^{-1/2}\|_{op} \leq 1,$ which may be proved by computing Schur complements. We will sequentially apply this inequality to the block matrix $\nabla^2 f$ until we are left with a diagonal matrix. 

%To implement this,  
%\CF{cite the new corollary}
We first apply the inequality from \cref{cor:off-diag-hess} to the upper-left two by two block, or $a = 1$ and $b = 2$. By our bound on $\|\nabla^2_{ab} f\|_{op}$, we may take 
   $A = (\lambda + \eps^2) I_{d_1}$, $B = (\lambda + \eps^2)I_{d_2}$ to find that 
   \begin{align*}
\nabla^2 f \succeq \begin{bmatrix}
 \nabla^2_{11} f - (\lambda + \eps^2) I_{d_1} & 0 & \hdots &  \nabla^2_{1k} f \\
0 & \nabla^2_{22} f - (\lambda + \eps^2) I_{d_2} & \ddots & \vdots \\
\vdots & \ddots & \ddots & \vdots\\
\nabla^2_{k1}f & \hdots &  \hdots & \nabla^2_{kk} f \\
  \end{bmatrix}.\end{align*}
 Applying this for all $a < b$ we find that 
 $$\nabla^2 f  \succeq \bigoplus_{a \in [k]} \nabla^2_{aa} f - (k - 1) (\lambda + \eps^2) I_{d_a}.$$ 
The last thing we need is a lower bound on $\nabla^2_{aa} f.$ This operator is again a sum of two terms, the latter of which we have already analyzed. The former defines the quadratic form $Y \mapsto d_a \tr \rho^{(a)} Y^2$ on $\smallSym^0_{d_a}$, and using the bound $\|\rho^{(a)} - I_{d_a}/{d_a}\|_{op} \leq \eps/d_a$ one checks that $\tr \rho^{(a)} Y^2 \in \frac{1 \pm \eps }{d_a} \tr Y^2 = \frac{1 \pm \eps }{d_a} \|Y\|_{F}^2. $ Combining this bound with the bound on the second term, we have $\nabla^2_{aa} f \succeq (1 -  \eps  - \eps^2 ) I_{d_a}$. Plugging this into the direct sum, we have $\nabla^2 f \succeq (1  - \eps - \eps^2  - (k - 1) (\lambda + \eps^2)) I$. Reversing the inequalities in all the steps also yields $\nabla^2 f \preceq (1  + \eps + \eps^2  + (k - 1) (\lambda + \eps^2))$.
\end{proof}


%$$\bigl(\tr Y \rho^{(a)}\bigr)\bigl(\tr Z \rho^{(b)}\bigr)$$



We now show our second strong convexity result, namely that if our function is strongly convex at the origin then it is also strongly convex in an operator norm ball about the origin. We'll use an easy fact relating the exponential map and the operator norm.

\begin{fact} For all symmetric $d\times d$ matrices $\delta $ such that $ \|\delta\|_{op} \leq \frac{1}{2}$, we have 
$$ \|e^{\delta} - I\|_{op} \leq \|\delta\|_{op} + \|\delta\|_{op}^{2} \leq 2 \|\delta\|_{op}.$$ 
\end{fact}

We will follow the structure presented in the last proof %by letting our input at the identity be represented by $x$ \CF{input at identity? use of "input" is in general imprecise} 
by showing each block of the Hessian only changes a small amount under perturbation $x' := e^{\delta} x$ for $\delta \in \Sym$. In particular we will give bounds on each block under each component-wise perturbation $x' := (e^{\delta_{b}})_{b} x$. Recall (c.f. \cref{lem:hessian}) that the $a^{th}$ diagonal block of the Hessian depends only on $\rho^a_{\samp}$. This motivates the next two lemmas quantifying the change of $\rho^{a}
_{\samp}$ under perturbations. 

%\CF{Akshay: $\sum_{a} \|\delta_{a}\|_{op} = \|\delta\|_{op}.$}
%\CF{say stronger condition}
\begin{theorem} \label{convexRobustness}
There is a constant $c>0$ such that the following holds. If $f_{\samp}$ is $\alpha$-strongly convex at $I$; and for every $a \in [k]$ we have $\|d_{a} \rho_{\samp}^{(a)} - Tr[\rho_{\samp}] I_{a}\|_{op} = \|\sqrt{d_{a}} \nabla_{a} f_{\samp}\|_{op} \leq c$; and $\sum_{a} \|\delta_{a}\|_{op} \leq c$: then $f$ at $e^{\delta} := \otimes_{a} e^{\delta_{a}}$ is $\geq (\alpha - k C \|\delta\|_{op})$-strongly convex for some universal constant $C$. 
\end{theorem}

Recall the definition of a quadratic form of the Hessian:
\[ \langle Z, (\nabla^{2} f) Z \rangle = \langle Z, (\nabla^{2} F) Z \rangle + \langle Z, (\nabla^{2} f - \nabla^{2} F) Z \rangle     \]
The second term is rank one, so the quadratic form is:
\[ \langle Z, (\nabla^{2} f - \nabla^{2} F) Z \rangle = \left( \sum_{a} \sqrt{d_{a}} \langle \rho^{(a)}, Z_{a} \rangle   \right)^{2}       \]
For the first term, we follow the same proof structure as $\ref{thm:tensor-convexity}$, and analyze diagonal and off-diagonal blocks separately:
\[ \langle Z_{a}, \nabla^{2}_{aa} F Z_{a} \rangle = \langle d_{a} \rho^{(a)}, Z_{a}^{2} \rangle       \]
\[ \langle Z_{a}, \nabla^{2}_{ba} Z_{b} \rangle = \langle \sqrt{d_{a} d_{b}} \rho^{(ab)}, Z_{a} \otimes Z_{b} \rangle   \]
Theorem $\ref{thm:tensor-convexity}$ gives good bounds on the Hessian of $\samp$; so in order to bound the Hessian at perturbation $\samp' := \prod_{a} (e^{\delta_{a}})_{a} \samp$, it is enough to bound the difference. By the above discussion then we will bound the difference of each of the terms (diagonal $\nabla^{2}_{aa} F$, off-diagonal $\nabla^{2}_{ba} F$, and rank one $(\nabla^{2} f - \nabla^{2} F)$) under each component-wise perturbation. Note all three terms involve only $\{\rho^{(a)}\}, \{\rho^{(ab)}\}$, so we prove perturbations on marginals in the following lemmas.  

\begin{lemma} \label{atoaaRobustness}
For input $\samp \in \R^{nD}$ and perturbation $\delta \in \mat(d_{a})$ such that $\|\delta\|_{op} \leq \frac{1}{20}$, if we denote $\samp' := (e^{\delta})_{a} \samp$ then
\[ \|\rho_{\samp'}^{a} - \rho_{\samp}^{a}\|_{op} \leq 4.5 \|\delta\|_{op} \|\rho_{\samp}^{a}\|_{op}   . \]
\end{lemma}
\begin{proof} By definition, $\|\rho_{\samp'}^{(a)} - \rho_{\samp}^{(a)}\|_{op} = \sup_{\|Z\|_{1} \leq 1} \langle Z_{a}, \rho_{\samp'} - \rho_{\samp} \rangle $. 


Choose $\eta$ such that  $\|(e^{\delta})_{a} \samp\|_{2}^{-1} = \|\samp\|_{2}^{-1} (1 + \eta)$; note that $|\eta| = O(\|\delta\|_{op})$ provided $c$ is small enough. Letting $\delta' := (1+\eta)e^{\delta} - I_{a}$. Assuming without loss of generality that $\|Z\|_{1} = 1$, we have 
\[ | \langle Z_{a}, (I+\delta')_a \rho_{\samp} (I+\delta')_a - \rho_{\samp} \rangle | \]
\[ \leq (2\|\delta'\|_{op} + \|\delta'\|_{op}^{2}) \|\rho^{(a)}\|_{op} \|Z\|_{1}    \]
from which the lemma follows since 
\[ \|\delta\|_{op} \leq .05 \implies \|\delta'\|_{op} \leq |\eta| + (1+|\eta|)(\|\delta\|_{op} + \|\delta\|_{op}^{2}) \leq  2.1 \|\delta\|_{op} \]
\end{proof}

%\CF{ I think we should combine these lemmas into a single one with two items.}\AR{The proofs are different, and I like the similar structure for diagonal/off-diagonal blocks. It may clutter the statements more to combine. }
\begin{lemma} \label{btoaaRobustness}
For input $\samp \in \R^{nD}$ and perturbation $\delta \in \mat(d_{b})$ such that $\|\delta\|_{op} \leq \frac{1}{20}$, if we denote $\samp' := (e^{\delta})_{a} \samp$ then for $b \neq a$:
\[ \|\rho_{\samp'}^{a} - \rho_{\samp}^{a}\|_{op} \leq 9.5 \|\delta\|_{op} \|\rho_{\samp}^{a}\|_{op}      \]
\end{lemma}
\begin{proof} 
Choose $\eta$ such that $\|(e^{\delta})_{b} \samp\|_{2}^{-2} = (1+\eta) \|\samp\|_{2}^{-2}$; let $\delta' := (1+\eta)e^{2\delta} - I$. 
\[ \|\delta\|_{op} \leq .1 \implies |\eta| \leq 2\|\delta\|_{op} + 4\|\delta\|_{op}^{2}, \hspace{3mm} \|\delta'\|_{op} \leq (2+|\eta|)(2\|\delta\|_{op} + 4\|\delta\|_{op}^{2}) \leq 9.24 \|\delta\|_{op} \]
We assume for now $Z \succeq 0$. 
\begin{align*} | \langle Z_{a}, (1+\eta) (e^{\delta})_{b} \rho_{\samp} (e^{\delta})_{b}^{*} - \rho_{\samp} \rangle| 
& = | \langle Z_{a} \otimes \delta'_{b}, \rho_{\samp} \rangle   |  \\ 
&\leq \langle Z \otimes |\delta'|, \rho_{\samp}^{(ab)} \rangle   
\leq \|\delta'\|_{op} \langle Z, \rho_{\samp}^{(a)} \rangle    
\end{align*}
Here in the first inequality we used that $\rho_{\samp} \succeq 0, Z \succeq 0$; and the last inequality was by definition of marginals. 
In general we decompose $Z = Z_{+} - Z_{-}$ and use the above to show
\[ |\langle Z, \rho_{\samp'}^{(a)} - \rho_{\samp}^{(a)} \rangle| \leq \|\delta'\|_{op} (\|Z_{+}\|_{1} + \|Z_{-}\|_{1}) \|\rho_{\samp}^{(a)}\|_{op}     \]
The lemma follows by noting $\|Z\|_{1} = \|Z_{+}\|_{1} + \|Z_{-}\|_{1}$ and $\|\delta\|_{op} \leq c$:
\[ \|\delta'\|_{op} = \|(1+\eta) e^{2 \delta} - I\|_{op} \leq |\eta|(1 + 2 \|\delta\|_{op}) + 2\|\delta\|_{op} \leq O(\|\delta\|_{op})   \qedhere  \]
\end{proof}

This is already enough to prove a bound on the diagonal terms $\{\nabla^{2}_{aa} F\}$ and rank one term $(\nabla^{2} f - \nabla^{2} F)$. 

\begin{corollary} \label{diagRobustness}
For input $\samp \in \R^{nD}$ such that $\|d_{a} \rho_{\samp}^{(a)}\|_{op} \leq 1 + \frac{1}{20}$; and perturbation $\delta := \sum_{b} (\delta_{b} \in \mat(d_{b}))_{b}$ such that $\|\delta\|_{op} = \sum_{b} \|\delta_{b}\|_{op} \leq \frac{1}{20}$; if we denote $\samp' := e^{\delta} \samp$, then we have:
\[ \|\nabla^{2}_{aa} F(e^{2\delta}) - \nabla^{2}_{aa} F(I)\|_{op} \leq 11 \|\delta\|_{op}     \]
\end{corollary}
%\CF{technically $f(e^{\delta})$ corresponds to $e^{\delta/2} \samp$}
\begin{proof}
Recall from the discussion after \ref{convexRobustness} that $\langle Y, \nabla^{2}_{aa} F_{\samp} Y \rangle = \langle d_{a} \rho_{\samp}^{(a)}, Y^{2} \rangle$. We treat the perturbation as the composition of $k$ perturbations; 
\[ \samp_{(0)}:=\samp \to \samp_{(1)}:= (e^{\delta_{1}})_1 \samp_{(0)} \to ... \to \samp_{(k)}:=(e^{\delta_{k}})_{k} \samp_{(k-1)} = \samp'  \]
We can use $\ref{atoaaRobustness}$ to handle $e^{\delta_{a}}$ and $\ref{btoaaRobustness}$ for the rest:
\begin{align*}
 |\langle \rho_{\samp'}^{(a)} - \rho_{\samp}^{(a)}, Y^{2} \rangle| 
 &\leq \sum_{j=1}^{k} |\langle \rho_{\samp_{(j)}}^{(a)} - \rho_{\samp_{(j-1)}}^{(a)}, Y^{2} \rangle| \underset{\ref{atoaaRobustness},\;\ref{btoaaRobustness}}{\leq} \sum_{j=1}^{k}  9.5 \|\delta_{j}\|_{op} \|\rho_{\samp_{(j-1)}}^{(a)}\|_{op} \|Y^{2}\|_{1} \\
& \leq \left( \prod_{j=1}^k (1+9.5 \|\delta_{j}\|_{op}) - 1 \right) \|\rho_{\samp}^{(a)}\|_{op} \|Y\|_{F}^{2} \\
&\leq 10 \|\delta\|_{op} \|\rho_{\samp}^{(a)}\|_{op} \|Y\|_{F}^{2}.   \end{align*}
The term in parenthesis is shown by induction, and in the inequality we used $\|\delta\|_{op} \leq \frac{1}{20}$. The final step follows by our initial condition on $\|d_{a} \rho_{\samp}^{(a)}\|_{op}$. 
\end{proof}

\begin{corollary} \label{rankoneRobustness}
For input $x \in \R^{nD}$ such that for all $a \in [k]$ such that $\|d_{a} \rho_{\samp}^{(a)} - Tr[\rho_{\samp}] I_{a}\|_{op} \leq \frac{1}{20}$; and perturbation $\delta := \sum_{a} (\delta_{a} \in \mat(d_{a}))_{a}$ such that $\|\delta\|_{op} = \sum_{a} \|\delta_{a}\|_{op} \leq \frac{1}{20}$; if we denote $\samp' := e^{\delta} \samp$, then we have:
\[ \|(\nabla^{2} f_{\samp'} - \nabla^{2} F_{\samp'}) - (\nabla^{2} f_{\samp} - \nabla^{2} F_{\samp})\|_{op} \leq 1.5 k \|\delta\|_{op}     \]
\end{corollary}
\begin{proof}
Recall again from the discussion after $\ref{convexRobustness}$ that $\langle Z, (\nabla^{2} F - \nabla^{2} f) Z \rangle = \left\langle \rho, \sum_{a} \sqrt{d_{a}} (Z_{a})_{a}  \right\rangle^{2}$. We use the same iterative strategy as $\ref{diagRobustness}$:
\[    \left\langle \rho_{\samp'}, \sum_{a} \sqrt{d_{a}} (Z_{a})_{a}  \right\rangle^{2} -  \left\langle \rho_{\samp}, \sum_{a} \sqrt{d_{a}} (Z_{a})_{a}  \right\rangle^{2}    \]
\[ = \left\langle \rho_{\samp'} + \rho_{\samp}, \sum_{a} \sqrt{d_{a}} (Z_{a})_{a}  \right\rangle \left\langle \rho_{\samp'} - \rho_{\samp}, \sum_{a} \sqrt{d_{a}} (Z_{a})_{a}  \right\rangle  \]
\[ = \left( \sum_{a} \langle (d_{a} \rho_{\samp'}^{(a)} - Tr[\rho_{\samp'}] I_{a}) + (d_{a} \rho_{\samp}^{(a)} - Tr[\rho_{\samp}] I_{a}) , d_{a}^{-1/2} Z_{a} \rangle \right) \left( \sum_{a} \sqrt{d_{a}} \langle \rho_{\samp'}^{(a)} - \rho_{\samp}^{(a)}, Z_{a} \rangle \right)     \]
%\AR{Here I could use that $Z \perp I$ to get a constant factor improvement; need an assumption on $\nabla$; but it improves the overall constant by factor $\approx 2$}
\[ \leq \left( \sum_{a} (2 + 10 \|\delta\|_{op}) \|d_{a} \rho_{\samp}^{(a)} - Tr[\rho_{\samp}] I_{a} \|_{op} \|d_{a}^{-1/2} Z_{a}\|_{1}   \right) 
\left( 10\|\delta\|_{op} \sum_{a} \sqrt{d_{a}} \| \rho_{\samp}^{(a)}\|_{op} \|Z_{a}\|_{1}   \right)    \]
\[ \leq \left( \sum_{a} \frac{2 + 10 \|\delta\|_{op}}{20} \|Z_{a}\|_{F}  \right) 
\left( 10\|\delta\|_{op} \sum_{a} (1 + \frac{1}{20}) \|Z_{a}\|_{F} \right) 
\leq 1.5 k \|\delta\|_{op} \|Z\|^{2}      \]
%\AR{What norm are we using on $Z$ as a whole? Should I keep around the $d_{a}$'s?} \CF{the norm is just the standard norm, there shouldn't be $d_a$'s}
% The final step follows from the initial conditions on $\|d_{a} \rho_{a}\|_{op}, \|d_{b} \rho_{b}\|_{op}$. 
In the third line we used that $Z$ is traceless; in the last line we used our initial conditions on $\rho$; the last step was by Cauchy-Schwarz. 
\end{proof}

The off-diagonal blocks $\{\nabla^{2}_{ba} F\}$ are only slightly more difficult as we need the following on bipartite marginals:

\begin{lemma} \label{btoabRobustness}
For input $\samp \in \R^{nD}$ and perturbation $\delta \in \mat(d_{c})$ such that $\|\delta\|_{op} \leq \frac{1}{20}$; if we denote $\samp' := (e^{\delta})_{c} \samp$, then for $c \in \{a,b\}$ we have
\[ \sup_{Y \in \smallSym_{d_{a}}^{0}, Z \in \smallSym_{d_{b}}^{0}} \frac{\langle \rho_{\samp'}^{(ab)} - \rho_{\samp}^{(ab)}, Y \otimes Z \rangle}{\|Y\|_{F} \|Z\|_{F}} \leq 4.5 \|\delta\|_{op} \sup_{Y \in \smallSym_{d_{a}}, Z \in \smallSym_{d_{b}}} \frac{\langle \rho_{\samp}^{(ab)}, Y \otimes Z \rangle}{\|Y\|_{F} \|Z\|_{F}}        \]
Note that in the LHS, $Y,Z$ are traceless, whereas on the RHS they are general symmetric matrices. 
%\[ \|\nabla^{2}_{ab} F_{\samp'} - \nabla^{2}_{ab} F_{\samp}\|_{0} \leq 4.5 \|\delta\|_{op} \|\nabla^{2}_{ab} F_{\samp}\|_{F \to F}    \]
\end{lemma}
\begin{proof}
%Recall from the discussion after $\ref{convexRobustness}$ that $\langle Y, (\nabla^{2}_{ab} F) Z \rangle = \sqrt{d_{a} d_{b}} \langle \rho^{(ab)}, Y \otimes Z \rangle$. 
By taking adjoints, we can assume w.l.o.g. that $c = b$. Let $R : \mat(d_{b}) \to \mat(d_{b})$ be defined as $R(Z) := (1+\eta)^{2} e^{\delta} Z e^{\delta}$ for $\eta$ defined by our normalization $\|(e^{\delta})_{b} \samp\|_{2}^{-1} =: (1+\eta) \|\samp\|_{2}^{-1}$.
\[ |\langle \rho_{\samp'}^{(ab)} - \rho_{\samp}^{(ab)}, Y \otimes Z \rangle| = |\langle \rho_{\samp}^{(ab)}, Y \otimes (R(Z) - Z) \rangle|  \]
The subspace $\smallSym_{d_{b}}^{0}$ is not invariant under $R$, but we show $R \approx I$. Let $\delta' := (1+\eta) e^{\delta} - I$. 
\[ \|R(Z) - Z\|_{F} \leq 2 \|\delta' Z\|_{F} + \|\delta' Z \delta'\|_{F} \leq (2 \|\delta'\|_{op} + \|\delta'\|_{op}^{2}) \|Z\|_{F}    \]
So we complete the proof using the fact that $Y,Z$ are traceless on the LHS of the inequality, and $\|\delta\|_{op} \leq \frac{1}{20}$ by the same calculation as in \ref{atoaaRobustness}. 
\end{proof}

\begin{lemma} \label{ctoabRobustness}
For input $\samp \in \R^{nD}$ and perturbation $\delta \in \mat(d_{c})$ such that $\|\delta\|_{op} \leq \frac{1}{20}$; if we denote $\samp' := (e^{\delta})_{c} \samp$, then for $c \not\in \{a,b\}$ we have
\[ \sup_{Y \in \smallSym_{d_{a}}^{0}, Z \in \smallSym_{d_{b}}^{0}} \frac{\langle \rho_{\samp'}^{(ab)} - \rho_{\samp}^{(ab)}, Y \otimes Z \rangle}{\|Y\|_{F} \|Z\|_{F}} \leq 19 \|\delta\|_{op} \sup_{Y \in \smallSym_{d_{a}}, Z \in \smallSym_{d_{b}}} \frac{\langle \rho_{\samp}^{(ab)}, Y \otimes Z \rangle}{\|Y\|_{F} \|Z\|_{F}}        \]
%\[ \|\nabla^{2}_{ab} F_{\samp'} - \nabla^{2}_{ab} F_{\samp}\|_{0} \leq 19 \|\delta\|_{op} \|\nabla^{2}_{ab} F_{\samp}\|_{F \to F}    \]
\end{lemma}
\begin{proof}
Define $\eta$ for normalization $\|(e^{\delta})_{c} \samp\|_{2}^{-1} =: (1+\eta) \|\samp\|_{2}^{-2}$, and let $\delta' := (1+\eta) e^{2 \delta} - I_{c}$. We will use a similar decomposition to lemma \ref{btoaaRobustness}, so first assume $Y,Z \succeq 0, \|Y\|_{F} = \|Z\|_{F} = 1$:
%\[ \frac{1}{\sqrt{d_{a} d_{b}} } \langle Y, (\nabla^{2}_{ab} F_{\samp'} - \nabla^{2}_{ab} F_{\samp}) Z \rangle = \langle \rho_{\samp}^{(abc)}, Y \otimes Z \otimes \delta' \rangle   \]
\[ |\langle \rho_{\samp}^{(abc)}, Y \otimes Z \otimes \delta' \rangle| \leq \langle \rho_{\samp}^{(abc)}, Y \otimes Z \otimes |\delta'| \rangle \leq \|\delta'\|_{op} \langle \rho_{\samp}^{(ab)}, Y \otimes Z \rangle   \]
Here we again used that $\rho_{\samp}^{(abc)} \succeq 0$. %We cannot bound this by $c_{0}$ as $Y \succeq 0$, but the RHS $c$ is sufficient.  
To finish the lemma we decompose $Y = Y_{+} - Y_{-}, Z = Z_{+} - Z_{-}$ and bound
\[ |\langle \rho_{\samp'}^{(ab)} - \rho_{\samp}^{(ab)}, Y \otimes Z \rangle| \leq \left( \sup_{Y \in \smallSym_{d_{a}}, Z \in \smallSym_{d_{b}}} \frac{\langle \rho_{\samp}^{(ab)}, Y \otimes Z \rangle}{\|Y\|_{F} \|Z\|_{F}} \right) \|\delta'\|_{op} \sum_{s,t \in \{+,-\}} \|Y_{s}\|_{F} \|Z_{t}\|_{F}   \]
The summation we can bound by Cauchy Schwarz:
\[ \leq (2\|Y_{+}\|_{F}^{2} + 2\|Y_{-}\|_{F}^{2})^{1/2} (2\|Z_{+}\|_{F}^{2} + 2\|Z_{-}\|_{F}^{2})^{1/2} = 2 \|Y\|_{F} \|Z\|_{F}     \]
Using the the fact that the LHS is the $\sup$ over traceless matrices, as well as the same calculation from \ref{btoaaRobustness} using the condition $\|\delta\|_{op} \leq .05 \implies \|\delta'\| \leq 9.5 \|\delta\|_{op}$; we get the lemma. 
\end{proof}

We need the following to translate to statements on the Hessian:

\begin{definition}
For operator $M : \mat(d_{b}) \to \mat(d_{a})$, we let $\|M\|_{0}$ denote the $F \to F$ norm of its restriction to the traceless subspaces $\smallSym^0_{d_b} \to \smallSym^0_{d_a}$
\end{definition}

\begin{lemma}[\cite{KLR19}] \label{inftyto2}
$\|\nabla^{2}_{ab} F_{\samp}\|_{F \to F}^{2} \leq \|d_{a} \rho_{\samp}^{(a)}\|_{op} \|d_{b} \rho_{\samp}^{(b)}\|_{op}$
\end{lemma}
%\begin{proof}This was already in KLR and we have two new proofs: one by convexity, and one by Riesz-Thorin. \AR{The proofs are in some other file, we can add it if we like}\end{proof}

\begin{corollary} \label{offdiagRobustness}
For input $\samp \in \R^{nD}$ such that $\|d_{a} \rho_{\samp}^{(a)}\|_{op}, \|d_{b} \rho_{\samp}^{(b)}\|_{op} \leq 1+\frac{1}{20}$; perturbation $\delta := \sum_{c} (\delta_{c} \in \mat(d_{c}))_{c}$ with $\|\delta\|_{op} = \sum_{c} \|\delta_{c}\|_{op} \leq \frac{1}{20}$; if we denote $\samp' := e^{\delta} \samp$, then we have:
%\[ \frac{1}{\sqrt{d_{a} d_{b}}} \|\nabla^{2}_{ab} f(e^{2 \delta}) - \nabla^{2}_{ab} f(I)\|_{op} \leq 100 \|\delta\|_{op} \sqrt{\|\rho_{\samp}^{(a)}\|_{op} \|\rho_{\samp}^{(b)}\|_{op}}     \]
\[ \|\nabla^{2}_{ab} F_{\samp'} - \nabla^{2}_{ab} F_{\samp}\|_{0} \leq 21 \|\delta\|_{op}  \]
\end{corollary}
\begin{proof}
This is just a translation of $\ref{btoabRobustness},\ref{ctoabRobustness}$:
\[ \sup_{Y \in \smallSym_{d_{a}}^{0}, Z \in \smallSym_{d_{b}}^{0}} \frac{\langle \rho_{\samp'}^{(ab)} - \rho_{\samp}^{(ab)}, Y \otimes Z \rangle}{\|Y\|_{F} \|Z\|_{F}} = \frac{\|\nabla^{2}_{ab} F_{\samp'} - \nabla^{2}_{ab} F_{\samp}\|_{0}}{\sqrt{d_{a} d_{b}} } \]
\[ \sup_{Y \in \smallSym_{d_{a}}, Z \in \smallSym_{d_{b}}} \frac{\langle \rho_{\samp}^{(ab)}, Y \otimes Z \rangle}{\|Y\|_{F} \|Z\|_{F}} = \frac{\|\nabla^{2}_{ab} F_{\samp}\|_{F \to F}}{\sqrt{d_{a} d_{b}} }       \]
Using the same iterative strategy as $\ref{diagRobustness}$ we can show: 
\[ |\langle Y, (\nabla^{2}_{ab} F_{\samp'} - \nabla^{2}_{ab} F_{\samp}) Z \rangle| \leq 20 \|\delta\|_{op} \|\nabla^{2}_{ab} F_{\samp}\|_{F \to F} \|Y\|_{F} \|Z\|_{F}    \]
We used $\ref{btoabRobustness}$ for $\{a,b\}$ and $\ref{ctoabRobustness}$ for the rest. The final step follows from $\ref{inftyto2}$ and the initial conditions on $\|d_{a} \rho_{a}\|_{op}, \|d_{b} \rho_{b}\|_{op}$. 
\end{proof}

Now it is a simple matter of putting the three terms together. 

\begin{proof} [Proof of Theorem \ref{convexRobustness}]
Recall the definition of a quadratic form of the Hessian:
\[ \langle Z, (\nabla^{2} f) Z \rangle = \langle Z, (\nabla^{2} F) Z \rangle + \langle Z, (\nabla^{2} f - \nabla^{2} F) Z \rangle     \]
\[ = \sum_{a} \langle Z_{a}, (\nabla^{2}_{aa} F) Z_{a} \rangle + \sum_{a \neq b} \langle Z_{a}, (\nabla^{2}_{ab} F) Z_{b} \rangle - \left( \sum_{a} \sqrt{d_{a}} \langle \rho^{(a)}, Z_{a} \rangle  \right)^{2}       \]
Let $\{\samp'_{i} := e^{\delta} \samp\}$. Then by $\ref{diagRobustness}$ we have a bound on the diagonal blocks; by $\ref{offdiagRobustness}$ we have a bound on the off-diagonal blocks; and by $\ref{rankoneRobustness}$ we have a bound on the rank-one term. 
\[ \langle Z, (\nabla^2 f_{\samp'} - \nabla^{2} f_{\samp}) Z \rangle \leq \|\delta\|_{op} \left( 11 \sum_{a} \|Z_{a}\|_{F}^{2} + 21 \sum_{a \neq b} \|Z_{a}\|_{F} \|Z_{b}\|_{F} + 1.5 k \|Z\|^{2} \right)   \]
\[ \leq (11 + 21(k-1) + 1.5 k) \|\delta\|_{op} \|Z\|^{2}    \]
Note that this also gives a spectral upper bound for $\nabla^{2} f_{\samp'}$. 
%\AR{What norm are we using on $Z$ as a whole? Should I keep around the $d_{a}$'s? Also the constant is $50 k$ now, yay for better constants!}\CF{yay!}
\end{proof}

Finally we may prove \cref{thm:ball-convexity}.
\begin{proof}[Proof of \cref{thm:ball-convexity}]
By \cref{thm:tensor-convexity}, with failure probability $k^2 ({\sqrt{nD}}/{kd_1})^{ - \Omega(d_1)}$ we have that $f$ is $3/4$-strongly convex at $I$. By our bound on $n$,  \cref{prop:gradient-bound} applies and so $\|\sqrt{d_{a}} \nabla_{a} f_{\samp}\|_{op} \leq c$ with failure probability $\sum_a e^{ - \Omega(nD/d_a)})$. By our assumption on $n$ this is at most $k^2 ({\sqrt{nD}}/{kd_1})^{ - \Omega(d_1)}$. Conditioned on both bounds, by \cref{convexRobustness} there is a constant $c$ such that $f$ is $1/2$ strongly convex at any point $Z$ such that $\sum_a \|\log Z_a\|_{op} \leq  c$ for all $a \in [k]$. \end{proof}


%$O\left( \sum_a e^{ - \Omega(nD/d_a)}\right)\leq k e^{- \Omega(nD/d_1)}$





%\CF{make consistent with def 4, hessian}




\subsection{Proof of \cref{thm:tensor-frobenius}}
We are now ready to prove the main result of this section according to the plan outlined in \cref{subsec:outline}.
\TODO{restate it here?}

\begin{proof}[Proof of \cref{thm:tensor-frobenius}]By \cref{it:reduce} in \cref{subsec:outline}, it is enough to prove the theorem assuming $\Theta = I$. We first show that the minimizer of $f_\rv$ exists and is close to $I$ with high probability, and then use this to show that the minimizer of $\ell_\rv$ is also close.  Set $\delta = \eps/\sqrt{k d_k}$ and let $c$ be a constant. Consider the following three events.
\begin{enumerate}
\item\label{it:grad-bd} $\|(\nabla f_\rv)_{a}\|_{op} \leq \frac{\delta}{\sqrt{d_{a}}}$ for all $a \in [k]$,
\item\label{it:sc-ball} $f_\rv$ is $1/2$ strongly convex at any point $\Theta$ such that $\sum_a \|\log \Theta_a\|_{op} \leq  c.$
\item\label{it:norm-bd} $\| \rv\|_F^2 \in (1 \pm k \eps/\sqrt{d_k}) nD.$
\end{enumerate}
We now show that these events occur with failure probability $$k^2 \left(\sqrt{nD} / kd_1 \right)^{ - \Omega(d_1)} + O(k e^{ - nD \eps^2 / k d_k^2}).$$ Indeed, by our first assumption on $\eps$, we have $N_a \geq C d_a/\delta^2$ for all $a \in [k]$, we so we may apply \cref{prop:gradient-bound} to conclude \cref{it:grad-bd} occurs with failure probability $O\left( \sum_a \exp ( - N_a \delta^2)\right)$. 
By \cref{thm:ball-convexity}, if $c$ is a small enough absolute constant then \cref{it:sc-ball} occurs with failure probability at most $k^2 \left(\sqrt{nD} / kd_1 \right)^{ - \Omega(d_1)}$. By \cref{prp:xnorm} with $t = k \eps/\sqrt{d_k}$, \cref{it:norm-bd} occurs with failure probability $2e^{- k nD \eps^2/8 d_k},$ which is dominated by the failure probability for \cref{it:grad-bd}. By the union bound, all three items hold with the desired failure probability. Let $\samp$ be a sample satisfying all three items.


We seek to apply \cref{lem:convex-ball} with $\lambda = 1/2$, so we must find the radius of the largest geodesic ball in the region of $1/2$-strong convexity of $f_\samp$. Thus if we set $\kappa = c/\sqrt{d_1}$ we will have that $f_\samp$ is $1/2$-strongly convex on the $\kappa$-ball by \cref{it:sc-ball}, because for $\Theta \in B_\kappa(I)$ we have
$$ \sum_a \|\log \Theta_a\|_{op} \leq \sqrt{d_1} d(I, \Theta) \leq c.$$
Next we must verify that $\|\nabla f\| < \kappa/2$. By \cref{it:grad-bd} we have
\[  \|\nabla f\|_F^{2} = \sum_{a} \|(\nabla f)_{a}\|_{F}^{2} \leq \sum_{a} d_a \|(\nabla f)_{a}\|_{op}^{2} \leq  k \delta^{2} \]
%\CF{new cite theorem 9}

%By \cref{thm:tensor-convexity}, with failure probability $\CF{1/poly(D)}$ we have that $f$ is $1-o_{d}(1)$ strongly convex at $I$. Because $\|\sqrt{d_{a}} \nabla_{a} f_{\samp}\|_{op} \leq \delta \leq c$, by \cref{convexRobustness} there is a constant $c$ such that $f$ is $1/2$ strongly convex at any point $\Theta$ such that $\sum_a \|\log \Theta_a\|_{op} \leq  c$ for all $a \in [k]$. \CF{end old}

By \cref{lem:convex-ball}, provided $\sqrt{k} \delta \leq c/\sqrt{d_k}$, which holds by our definition of $\delta$ and our second assumption on $\eps$, there is an optimizer $\widehat{\Theta}$ in a geodesic $2 \sqrt{k} \delta$-ball about $I$. We now use this bound on geodesic distance to bound error in $d_F$ for the individual Kronecker factors. By our choice of $\kappa$ and because $\widehat{\Theta}$ is in the $\kappa$ ball, $\|\log \widehat{\Theta}_a\|_{op} \leq c$ and so $\|I_a - \widehat{\Theta}_a\|_F = O( \| \log \widehat{\Theta}_a\|_F) \leq \sqrt{d_a} d(I, \widehat{\Theta}) \leq 2\delta \sqrt{ k d_a}.$ Applying $\delta = \eps /\sqrt{k d_k}$ yields $\| I_a - \widehat{\Theta}_a\|_F \leq 2\eps \sqrt{ d_a/d_k}$.

We have shown that the minimizer $\widehat{\Theta}$ of $f_\samp$ is close to $I$, but it remains to show that the minimizer $\Theta'$ of $\ell_\samp$ exists and is close to $I$. For this, recall that if $f_\samp$ has the unique minimizer $\widehat{\Theta} \in \SPD$, then $\ell_\samp$ is minimized at $\Theta':= \alpha^{1/k}  \widehat{\Theta}$ where $\alpha:= n D e^{ - f_\samp(\widehat{\Theta})}$. We now show that $\alpha$ is very close to $1$. By \cref{lem:convex-ball}, $f_\samp(\widehat{\Theta}) \geq f_\samp(0) - k\delta^2$. We have $f(0) \geq f_\samp(0) = \log\| \samp\|^2_F$, and by \cref{it:norm-bd} we have $\| \samp\|_F^2 \in (1 \pm t) nD$ with $t = k \eps/\sqrt{\delta_1}$. Thus we have $\alpha \in [(1-t),  (1+t) e^{k \delta^2}]$, or $\alpha^{1/k} \in [1 \pm  O(\eps/\sqrt{d_k})]$ by our choice of $t$. By the approximate triangle inequality for $d_F$ \CF{insert and cite}, $d_F( \Theta'_a; I_a) = O(d_F(\widehat{\Theta}_a; I_a) + d_F(\alpha^{1/k} I_a; I_a)) = O(\eps \sqrt{d_a/d_k})$. The reparametrization $\eps \leftarrow \eps \sqrt{d_k^{3/2}/ n D}$ completes the proof.
\end{proof}



%\CF{I think right now this doesn't use our convention in \cref{dfn:g-convexity} which leads to the Hessian in \cref{lem:hessian} }

\section{Improvements for the matrix normal model}\label{sec:matrix-normal}
We now prove \cref{thm:matrix-normal}, an improvement to \cref{thm:tensor-frobenius} in the case $k=2$. The results for $k = 2$ are stronger in that the MLE can be shown to be close to the truth in operator norm rather than the looser Frobenius norm, and that the failure probability is inverse exponential in the number of samples rather than inverse polynomial.




The proof plan is similar to that in \cref{subsec:outline}, but rather than strong convexity we use the similar but stronger notion of quantum expansion \CF{cite klr, etc}, which allows us to obtain bounds on the distance to the optimizer from bounds on the \emph{operator norm} on the gradient rather than the Frobenius norm.
\CF{move this to appropriate place?}
\begin{definition}[Quantum expansion]
$ $
\begin{enumerate}
\item Let $\Phi:\mat(d_1) \to \mat(d_2)$ be the operator defined by $\Phi(Y) = \tr_{\{1,3\}} ( Y \ot I_{d_2} \ot I_{n}) \samp \samp^T$. Equivalently,
$$\Phi(Y) = \sum_{i = 1}^n \samp_i^T Y \samp_i.$$
$\Phi$ is known as the \emph{completely positive map} with Kraus operators $\samp_1, \dots, \samp_n$.
\item $\Phi$ is said to be a \emph{$(1 - \lambda)$-quantum expander} if the second singular value $\sigma_2(\Phi)$ satisfies the following bound:
$$\sigma_2(\Phi) \leq \frac{(1 - \lambda)}{\sqrt{d_1d_2}} \tr \Phi(I_{d_2}).$$
\item Say $\Phi$ is \emph{$\eps$-doubly balanced} if
\begin{align*}
\|d_2 \Phi(I_{d_1})/\tr \Phi(I_{d_1})  - I_{d_2} \|_{op}& \leq \eps\\
\textrm{and }\|d_1 \Phi^*(I_{d_2})/\tr \Phi(I_{d_1})  - I_{d_1}  \|_{op} & \leq \eps,
\end{align*}
\end{enumerate}
\end{definition}

The rationale for the above definition is that $\Phi$ is doubly balanced when $(I_{d_1}/\sqrt{d_1}, I_{d_2}/\sqrt{d_2})$ is a singular pair for $\Phi$. This will approximately be the case in our setting (in fact, $\eps$-balancedness is none other than the conclusion of \cref{prop:gradient-bound} for $k = 2$), and in this case $\sigma_1(\Phi) =  \tr \Phi(I_{d_1})/\sqrt{d_1 d_2}.$ Thus $(1-\lambda)$ is like a spectral gap.
%$\sigma_1(\Phi) = \langle I_{d_1}/\sqrt{d_1}, \Phi(I_{d_1}/\sqrt{d_2}) \rangle$
Our main tool is the following:


\begin{theorem}[\CF{cite klr, comment about how to assume in SL?}]\label{thm:klr}
If $\Phi$ is an $\eps$-balanced, $(1 - \lambda)$-quantum expander, and $\eps \leq c \lambda^2/\log d_1$, then the maximum likelihood estimator $(\widehat{\Theta}_1, \widehat{\Theta}_2) \in \SL_{d_1}\times \SL_{d_2}$ is unique and satisfies
$$\| \widehat{\Theta}_1 - I_{d_1}\|_{op}, \| \widehat{\Theta}_2 - I_{d_2}\|_{op} \leq \frac{\eps \log d_1}{\lambda}.$$
\end{theorem}
Our main result for this section is that $\Phi$ is with high probability a good quantum expander, which allows us to apply the previous theorem.

\begin{theorem}\label{thm:operator-cheeger}
There are absolute constants $c, C$ such that the following holds for all $d_1 \leq d_2$. Suppose $n \geq C (d_2/d_1) \max\{\log (d_2/d_1),  \eps^{-2}\} $. Then $\Phi$ is $\eps \sqrt{d_2/n d_1}$-balanced and a $1 - c$ quantum expander with failure probability $O(e^{ - \Omega( d_2 \eps^2)}).$
\end{theorem}

Before proving the above result, we use it to prove \cref{thm:matrix-normal}.

\begin{proof}[Proof of \cref{thm:matrix-normal}] As discussed in \cref{subsec:outline}, it is enough to prove \cref{thm:matrix-normal} under the assumption $\Theta_a = I_{d_a}$ for $a \in \{1,2\}$. By \cref{thm:operator-cheeger}, provided $n \geq C (d_2/d_1) \max\{\log (d_2/d_1),  \eps_0^{-2}\} $ with probability $1 - O(e^{ - \Omega( d_2 \eps_0^2)})$ the operator $\Phi$ is $\delta:=\eps_0  \sqrt{\frac{d_2}{n d_1}}$-balanced and is a $(1 - c)$-quantum expander. Condition on this event. By our choice of $n$, we have $\delta \leq \lambda^2/\log d_1$ if we take $\eps_0 = \eps/\log(d_1)$ for $\eps \leq c$. By \cref{thm:klr},
\begin{gather*} \| \widehat{\Theta}_1 - I_{d_1}\|_{op}, \| \widehat{\Theta}_2 - I_{d_2}\|_{op} \leq C \eps_0 \sqrt{\frac{d_2}{n d_1}} \log d_1 = O\left(\eps \sqrt{\frac{d_2}{n d_1}}\right).\end{gather*}
The failure probability becomes $O(e^{ - \Omega( d_2 \eps^2/\log^2 d_1)})$.
\end{proof}




%The results in this section are better in that the covariances are approximated in operator norm rather than the looser Frobenius norm, and secondly that the failure probability is inverse exponential.







\subsection{The Cheeger constant of a random operator}

To prove \cref{thm:operator-cheeger}, we first define the Cheeger constant of an operator $\Phi:\mat(d_1) \to \mat(d_2)$. This is similar to a concept defined in \cite{H07}.
\begin{definition}
Let $\Phi : \mat(d_1) \to \mat(d_2)$ be a completely positive map. The Cheeger constant $\ch(\Phi)$ of the weighted bipartite graph associated to $B$ is given by
$$\ch(\Phi):=\min_{\Pi_1, \Pi_2: \vol(\Pi_1, \Pi_2) \leq \tr \Phi(I)} \phi(\Pi_1,\Pi_2)$$
where $\Pi_1: \C^{d_1} \to \C^{d_1}$ and $\Pi_1: \C^{d_2} \to \C^{d_2}$ are orthogonal projections that are not both zero and the \emph{conductance} $\phi$ of the cut $\Pi_1, \Pi_2$ is defined to be
$$\phi(\Pi_1,\Pi_2) := \frac{\cut(\Pi_1, \Pi_2)}{\vol(\Pi_1,\Pi_2)}$$
where
%$$ \vol(\Pi_1,\Pi_2):= \sum_{i \in T, j \in [d_2]} b_{ij} + \sum_{i \in [d_1], j \in S} b_{ij}\textrm{ and } \cut(S, T):= \sum_{i \not\in T, j  \in S} b_{ij} + \sum_{i \in T, j \not\in S} b_{ij}.$$
$$ \vol(\Pi_1,\Pi_2):=
\tr \Phi(\Pi_1) + \tr \Phi^*(\Pi_2)$$
and $$ \cut(\Pi_1, \Pi_2):= \tr \Pi_2 \Phi(I_{d_1} - \Pi_1) + \tr (I_{d_2} - \Pi_2) \Phi(\Pi_1).$$
\end{definition}

We now cite a slight generalization of \cite{FM20}.
%Recall the function $$f^{\Phi}:\samp \mapsto \frac{d_1}{d_2} \log\det(\Phi(\samp)) - \log\det (\samp).$$

\begin{lemma} [\cite{FM20}, \cite{KLR19}]\label{lem:op-cheeger} There exist absolute constants $c, C$ such if $\eps < c \ch(\Phi)^2$ and $\Phi$ is $\eps$-balanced, then $\Phi$ is a
$$ \max\left\{1/2, 1 -  \ch(\Phi)^2 + C \frac{\eps}{\ch(\Phi)^2} \right\}$$
quantum expander.
\end{lemma}
We proceed to bound the Cheeger constant of a random operator. The Cheeger constant of an operator is scale-invariant, so for convenience we let $\Phi$ have Kraus operators $\samp_1, \dots, \samp_n$, each drawn from $\cN(0,  I_{d_1} \ot I_{d_2}).$ Our main observation is the following.

\begin{lemma}\label{fact:chi} Let $\Pi_1:\C^{d_1} \to \C^{d_1}, \Pi_2: \C^{d_2} \to \C^{d_2}$ be orthogonal projections, of rank $r_1, r_2$, respectively. Then $\cut(\Pi_1, \Pi_2), \vol(\Pi_1, \Pi_2), \vol(I_{d_1}, I_{d_2})$ is jointly distributed as
$$ R_1, R_1 + 2R_2, 2R_1 + 2 R_2 + 2R_3$$ where
$R_1, R_2, R_3$ are independent $\chi^2$ random variables with $F_1:=n r_1(d_2 - r_2) + n r_2(d_1-r_1), F_2:= n r_1r_2, F_3:= n(d_1 - r_1)(d_2 - r_2)$ degrees of freedom, respectively.
\end{lemma}
\begin{proof} As the distribution of $\Phi$ is invariant under the action of unitaries, the distribution of $\cut(\Pi_1, \Pi_2), \vol(\Pi_1, \Pi_2)$ depends only on the rank of $\Pi_1, \Pi_2$. Thus we may compute in the case that $\Pi_1, \Pi_2$ are coordinate projections, in which case one may verify the fact straightforwardly.
\end{proof}


 We show a sufficient condition for the Cheeger constant being bounded away from $1$ that is amenable to the previous distributional description.
\begin{lemma}\label{lem:suff}
Let $r_1, r_2$ not both zero be the ranks of projections $\Pi_1: \C^{d_1} \to \C^{d_1}, \Pi_2: \C^{d_2} \to \C^{d_2}$, and let $F_1:= n r_1(d_2 - r_2) + n r_2(d_1-r_1)$ and $F_2:=n r_1r_2.$ If
\begin{itemize}
\item for all $\Pi_1, \Pi_2$ such that $F_2 \geq (4/9) n d_1 d_2$ we have
\begin{gather}\vol(\Pi_1, \Pi_2) \geq (1/2 - \delta) \vol(I_{d_1}, I_{d_2}),\label{eq:vol}\end{gather} and
\item for all $\Pi_1, \Pi_2$ such that $F_2 < (4/9) n d_1 d_2$, we have
\begin{gather} \vol(\Pi_1, \Pi_2) \leq (4/3 + \delta)(F_1 + 2 F_2) \textrm{ and } \cut(\Pi_1, \Pi_2) \geq (2/3 - \delta) F_1,\label{eq:cut} \end{gather}
\end{itemize}
then $\ch(\Phi) \geq 1/6 - O(\delta)$ for $\delta \leq c$.
\end{lemma}
\begin{proof} By the first assumption, it remains to show that $F_1/(F_1 + 2 F_2) \geq 1/3$ provided $F_2 < (4/9) n d_1 d_2$, or $r_1 r_2 < (4/9) d_1 d_2$. Indeed, if either $r_1 = 0$ or $r_2 = 0$, then $F_2 = 0$ and $F_1>0$ and the claim holds, else
\begin{align*}F_1/(F_1 + 2 F_2) &= \frac{r_1 d_2 + r_2 d_1 - 2 r_1 r_2}{r_1 d_2 + r_2 d_1}\\
 &= 1 -2 \sqrt\frac{ r_1 r_2}{d_1 d_2} \frac{1}{ \sqrt{ r_1 d_2/r_2 d_1} + \sqrt{r_2 d_1/ r_1 d_2}} \\
 &\geq 1 - \sqrt{4/9} = 1/3.
\end{align*}

In the last inequality we used that $a + a^{-1} \geq 2$ for all $a \in \R_+$ and that $r_1 r_2 < (4/9) d_1 d_2$. \end{proof}


Next we use this to show that for fixed $\Pi_1, \Pi_2$, with high probability the events in \cref{lem:suff} hold.
\begin{lemma}\label{lem:probabilities}
Let $r_1, r_2$ not both zero be the ranks of projections $\Pi_1: \C^{d_1} \to \C^{d_1}, \Pi_2: \C^{d_2} \to \C^{d_2}$, and let $F_1:= n r_1(d_2 - r_2) + n r_2(d_1-r_1)$ and $F_2 = n r_1 r_2$. Then
\begin{itemize}
\item if $F_2 \geq (4/9) n d_1 d_2$, then \cref{eq:vol} holds with $\delta = 0$ with probability at least $1 - e^{-\Omega( n d_1 d_2)}$.
\item else, \cref{eq:cut} holds with $\delta = 0$ with probability at least $1 - e^{-\Omega( F_1)}$.
\item Finally, $\vol(\Pi_1, \Pi_2) \geq \frac{1}{2}\tr \Phi(I_{d_1}) (d_1/d_2)$ with probability at least $1 - e^{- \Omega(n r_1 d_2 + n r_2 d_1)}$.
\end{itemize}
\end{lemma}


\begin{proof}
Recall from \cref{fact:chi} that, $\cut(\Pi_1, \Pi_2), \vol(\Pi_1, \Pi_2), \vol(I_{d_1}, I_{d_2})$ are jointly distributed as $R_1, R_1 + 2R_2, 2R_1 + 2R_2 + 2R_3$ for $R_1, R_2, R_3$ independent $\chi^2$ random variables with $F_1, F_2, F_3$ degrees of freedom, respectively. Thus it is enough to show that
\begin{itemize}
\item If $nr_1 r_2 \geq (4/9) n d_1 d_2$, then with probability $1 - e^{- \Omega( n d_1 d_2)}$ we have $R_2 > R_3$, and
\item if $nr_1 r_2 \leq (2/3) n d_1 d_2$, then with probability $1 - e^{- \Omega(F_1)}$ we have $R_1 \geq (2/3) F_1$ and $R_1 + 2R_2 \leq (4/3) (F_1 + 2 F_2),$
\item and with probability $1 - e^{- \Omega(F_1 + 2 F_2)}$, $R_1 + 2R_2 \geq (2/3) (F_1 + 2 F_2) = (2/3) n (r_1 d_2 + r_2 d_1)$ and $R_1 + R_2 + R_3 \leq (4/3)(F_1 + F_2 + F_3) = (4/3)n d_1 d_2$.
\end{itemize}
All three follow from standard results for concentration of $\chi^2$ random variables \cite{W19}. To prove the first item, first note that $F_1 + 2 F_2 \geq (4/3)(F_1 + F_2 + F_3)$, because
\begin{align*}
(F_1 + 2 F_2)/( F_1 + F_2 + F_3) &= \frac{r_1}{d_1} + \frac{r_2}{d_2}\\
 &= \sqrt{ \frac{r_1 r_2}{d_1 d_2}}\left( \sqrt{ \frac{r_1 d_2}{r_2 d_1}} + \sqrt{ \frac{r_2 d_1}{r_1 d_2}}\right) \geq (2/3) \cdot 2 \geq 4/3.
\end{align*}
In particular, $F_2 \geq (2/3)(F_2 + F_3)$. Thus, with probability $1 - e^{- c F_2}$, $R_2 \geq (5/9) (F_2 + F_3)$ and $R_2 + R_3 \leq (10/9) (F _2 + F_3),$ so $R_2 > R_3$ with probability $1 - e^{- c F_2} \geq 1 - e^{- c n d_1 d_2}$. The second and third items are straightforward.
\end{proof}

Finally, we show using an epsilon net that the Cheeger constant is large for \emph{all} projections.
\begin{lemma}[\cite{FM20}]\label{lem:net} There is a $\delta$-net $N$ of the rank $r$ orthogonal projections $\Pi: \C^d \to \C^d$ with $|N| = \exp(O(d r |\ln \delta|))$.
\end{lemma}
As a corollary, the number of pairs of projections $\Pi_1, \Pi_2$ of rank $r_1, r_2$ has a $\delta$-net of size on the order of $(r_1 d_1 + r_2 d_2) |\ln \delta|$.

\begin{lemma}[A net suffices]\label{lem:net-suffices}
Suppose $\|\Pi'_1 -\Pi_2\|_F, \|\Pi'_2 - \Pi_2\|_F \leq \delta$. Then
\begin{align*} |\cut(\Pi_1, \Pi_2) - \cut(\Pi'_1, \Pi'_2)| \leq4\delta \tr \Phi(I_{d_1})\\
\textrm{ and }|\vol(\Pi_1, \Pi_2) - \vol(\Pi'_1, \Pi'_2)| \leq 4\delta \tr \Phi(I_{d_1}).
\end{align*}
\end{lemma}
\begin{proof}
We first show the first inequality.
\begin{align*}|\cut(\Pi'_1, \Pi'_2) - \cut(\Pi_1, \Pi_2)| & \leq |\tr \Pi'_2 \Phi(I_{d_1} - \Pi'_1) - \tr \Pi_2 \Phi(I_{d_1} - \Pi_1)|\\
&  + |\tr (I_{d_2} - \Pi'_2) \Phi(\Pi'_2) - \tr (I_{d_2} - \Pi_2) \Phi(\Pi_2)|.
\end{align*}
We begin with the first term.
\begin{align*}&|\tr \Pi'_2 \Phi(I_{d_1} - \Pi'_1) - \tr \Pi_2 \Phi(I_{d_1} - \Pi_1)|\\
&= |\tr (\Pi'_2 - \Pi_2) \Phi(I_{d_1} - \Pi'_1) + \tr \Pi_2 \Phi(\Pi_1 - \Pi'_1)|\\
&\leq \delta\| \Phi(I_{d_1} - \Pi'_1)\|_F + \delta\| \tr \Phi^*(\Pi_2)\|_F\\
& \leq 2 \delta \tr \Phi(I_{d_1}).
\end{align*}
The second term follows by symmetry. The proof of the second inequality is similar.
\end{proof}

\begin{lemma}[Applying union bound]\label{lem:union}
Let $d_1 < d_2$. Suppose $n \geq C \frac{d_2}{d_1} \log (d_2/d_1)$. Then $\ch(\Phi) = \Omega(1)$ with failure probability $O(e^{- \Omega(n d_1)})$.
\end{lemma}
\begin{proof} Let $\delta' \leq c d_1/d_2$. Let $\cN(r_1, r_2)$ be a $\delta'$-net for the pairs of projections of rank $r_1, r_2$, respectively, with $|\cN(r_1, r_2)| = e^{O((d_1r_1 + d_2 r_2) \log(1/\delta'))}$, and $N = \bigcup_{r_1, r_2} \cN(r_1, r_2)$. We claim that it is enough to show that with probability $\exp( - c n d_1 )$, for all $r_1, r_2$ not both zero we have
\begin{enumerate}
\item \cref{eq:vol} holds with $\delta = 0$ for every $\Pi_1,\Pi_2 \in \cN(r_1, r_2)$ when $r_1 r_2 \geq (4/9) d_1 d_2$,
\item  and \cref{eq:cut} holds with $\delta =0$ for all $\Pi_1, \Pi_2 \in \cN(r_1, r_2)$ otherwise.
\item $\vol(\Pi_1, \Pi_2) \geq \tr \Phi(I) (d_1/d_2)$.
\end{enumerate}
Let us check that the hypotheses of \cref{lem:suff} with $\delta \leq c$ are implied by these three items; this will imply that conditioned on the three items we have $\ch(\Phi) \geq \Omega(1)$. Because every pair $(\Pi'_1,\Pi'_2)$ of projections of ranks $r_1,r_2$ is most $\delta$ far from some element $(\Pi_1, \Pi_2)$ of $\cN(r_1,r_2)$, then by \cref{lem:net-suffices} (and the inequality $\vol(\Pi_1, \Pi_2) \geq \tr \Phi(I)(d_1/d_2)$) we have
\begin{align*} (1 - 4 \delta'  \cdot d_2/d_1) \vol(\Pi_1, \Pi_2) \leq  \vol(\Pi_1', \Pi_2') \leq  (1 + 4 \delta'  \cdot d_2/d_1) \vol(\Pi_1, \Pi_2).\end{align*}
By assumption, $4 \delta' \cdot d_2/d_1 \leq c$. This shows \cref{eq:vol} holds with $\delta \leq c$ when $r_1 r_2 \geq (4/9) d_1 d_2$. It remains to show that \cref{eq:cut} holds otherwise. Firstly, when $r_1 r_2 < (4/9) d_1 d_2$ we have
\begin{gather} \vol(\Pi_1', \Pi_2') \leq (1 + c) \vol(\Pi_1, \Pi_2) \leq  (1 + c)(4/3)(F_1 + 2 F_2).\label{eq:not-net-9a}\end{gather}
  Next, observe that
$$  \cut(\Pi_1', \Pi_2') \geq \cut(\Pi_1, \Pi_2) - c \vol(\Pi_1, \Pi_2).$$
In the proof of \cref{lem:suff} it is shown that if $r_1 r_2 < (4/9) d_1 d_2$ then $F_1 \geq \frac{1}{3} (F_1 + 2 F_2)$, in which case
\begin{align}
\cut(\Pi_1', \Pi_2') &\geq \cut(\Pi_1, \Pi_2) - c \vol(\Pi_1, \Pi_2) \geq \nonumber\\
& \geq (2/3) F_1 -  c (4/3)(F_1 + 2 F_2) \geq (2/3 - c) F_1.\label{eq:not-net-9b}
\end{align}

Taken together, \cref{eq:not-net-9a,eq:not-net-9b} show that \cref{eq:cut} holds when $r_1 r_2 < (4/9) d_1 d_2$.

We must next show that the three conditions hold with the desired probability. We show that for fixed $r_1, r_2$, each item holds with probability at least $1 - e^{n (r_1 d_2 + r_2 d_1)}$. The sum of $e^{-\Omega(n (r_1d_2 + r_2 d_1))}$ over all $0 \leq r_1 \leq d_1, 0 \leq r_2 \leq d_2$ apart from $r_1 = r_2 = 0$ is $O(e^{- \Omega( n d_1)})$, so the conditions hold for all $r_1, r_2$ with the desired probability. Note that by our choice of $n$ we have $(d_1r_1 + d_2 r_2) \log(1/\delta') \leq c n (r_1d_2 + r_2 d_1)$ for $r_1, r_2$ not both zero.

We first bound the failure probability for the first item. By \cref{lem:probabilities}, if $r_1 r_2 \geq (4/9) d_1 d_2$ then \cref{eq:vol} holds for every $\Pi \in \cN(r_1, r_2)$ with probability
\begin{align*}
1 - |\cN(r_1, r_2)|e^{- \Omega( n d_1 d_2) } &= 1 - |\cN(r_1, r_2)| e^{ - \Omega(n (r_2d_1 + r_1d_2))}\\
&= 1 - e^{ - \Omega(n (r_2d_1 + r_1d_2))}.
\end{align*}

Next we bound the probability for the second item. By \cref{lem:probabilities}, \cref{eq:cut} holds for fixed $\Pi \in \cN(r_1, r_2)$ with probability $1 - e^{-\Omega( F_1)}$, but as in the proof of \cref{lem:suff} we have $F_1 \geq \frac{1}{3} (F_1 + 2 F_2)$ when $r_1 r_2 < (4/9) d_1 d_2$, so $F_1 = \Omega(n (r_1d_2 + r_2 d_1))$. Now, by the union bound and the lower bound on $n$, \cref{eq:cut} holds for every element of $\cN(r_1, r_2)$ with probability $1 - |\cN(r_1,r_2)| e^{-\Omega(n (r_1d_2 + r_2 d_1)} = 1 - e^{-\Omega(n (r_1d_2 + r_2 d_1)}$.


The third item holds with probability $1 - e^{-\Omega(n (r_1d_2 + r_2 d_1))}$ by \cref{lem:probabilities}, so by a similar application of the union bound and our choice of $n$ it holds for all elements of $\cN(r_1, r_2)$ with probability $1 - e^{-\Omega(n (r_1d_2 + r_2 d_1))}$. \end{proof}

\begin{proof}[Proof of \cref{thm:operator-cheeger}]
To prove \cref{thm:operator-cheeger}, we apply \cref{lem:op-cheeger} using \cref{prop:gradient-bound} to bound the balancedness of $\Phi$ and \cref{lem:union} to bound $\ch(\Phi)$. Indeed, $\|\nabla_a f\|_{op} \leq \eps_0$ for $a \in \{1,2\}$ if and only if $\Phi$ is $\eps_0$-balanced, so by \cref{prop:gradient-bound} the operator $\Phi$ is $\eps_0$-balanced with probability $1 -  e^{-\Omega(n d_1 \eps_0^2)} - e^{-\Omega(n d_2 \eps_0^2)} \geq 1 - 2e^{-\Omega(n d_1 \eps_0^2)}$ provided $n \geq C\eps_0^{-2} d_2/d_1 $. Setting $\eps_0 = \eps \sqrt{\frac{d_2 }{n d_1}}$ proves the balancedness claim. For the expansion, \cref{lem:union} shows $\ch(\Phi) = \Omega(1)$ with failure probability $O(e^{- \Omega(n d_1)}) = O(e^{- \Omega(d_2 \eps^2)})$. By \cref{lem:op-cheeger}, $\Phi$ is a $1 - c$-quantum expander.
\end{proof}

\newpage
\section{Convergence of flip-flop algorithms}

In this section we prove that the flip-flop algorithms for the matrix and tensor normal models converge quickly to the MLE estimator with high probability. We begin by stating the flip-flop algorithm and then... \CF{we are using $\Theta_a$ for the true inverse covariances. Would it be possible to use something like $\overline{\Theta}_a$ for the flip-flop scalings? Eventually you'll want a lemma showing that the final $\overline{\Theta}_a$ is close to $\widehat{\Theta}_a$ - this should also follow by strong convexity.}

\begin{Algorithm}
\textbf{Input}: Samples $\samp = (\samp_1, \ldots, \samp_n)$ where $\samp_i \in \R^D$ is sampled from a (unknown) centered normal distribution $\cN(0, \Sigma)$, where each entry of $\samp_i$ is encoded in binary, with bit size $\le b$. Approximation parameter $\eps > 0$, given in binary representation. \\[.3ex]

\textbf{Output}: $\otheta \in \SPD$ such that $d_F(\otheta_a, \htheta_a) < \eps$, for each $i \in [k]$, where $\htheta$ is the MLE for the precision matrix of $\Sigma$. \\[.3ex]
\RMO{The $\epsilon$ above is different from the $\epsilon$ from the algorithm. Fix this.}

\textbf{Algorithm}:
\begin{enumerate}
\item\label{it:flip-flop step 1} Set $\otheta_a = I_a$ for each $a \in [k]$.
\item\label{it:flip-flop step 2} For $t=1,\dots,T = 12 k d_k \cdot \log(1/\eps)$, repeat the following:
%we need T = \log(1/\epsilon) \cdot 1/\alpha \lambda
\begin{itemize}
\item Compute each component of the gradient $\nabla f_{\samp}(\otheta_1, \ldots, \otheta_k)$, denoting $\nabla_a := \nabla_a f_{\samp}(\otheta_1, \ldots, \otheta_k)$, and find the index $a \in [k]$ for which $\norm{\nabla_a}_F$ is largest. 
\item
If $\norm{\nabla_a}_F^2 < \eps$, output $\left( \bigotimes_{a =1}^k \otheta_a \right)^{-1}$ and return.
% S\Bigl(\frac {I_{n_{k^{t}}}{n_{k^{t}}}} \big\Vert \rho^{t}_{k^{t}}\Bigr)$ is largest.
\item Otherwise, set $\otheta_a \leftarrow \det(\rho^{(a)})^{1/d_a} (\rho^{(a)})^{-1} \cdot \otheta_a$.
\end{itemize}
\end{enumerate}
\caption{Flip-flop algorithm}\label{alg:flip-flop}
\end{Algorithm}

\TODO{General descent-lemma + strong convexity folklore}

Before we analyze the convergence of the flip-flop algorithms for the tensor and matrix normal models, we discuss the straightforward generalizations of convergence of general descent methods whenever the objective function is strongly geodesically convex.

The next lemma shows that any descent method which manages to decrease the value of the function with respect to the gradient...
The proof of the lemma is the same as the one from~\cite[Lemma 4.8]{FM20}.

\begin{lemma}\label{lem:descent-sublevel-set}
	Let $f : \SPD \rightarrow \R$ be $\lambda$-strongly geodesically convex in a sublevel set containing $x_0$, $\norm{\nabla f(x_0)}_F^2 \leq 1$ and $\alpha > 0$ be a constant.
	If $\{x_k\}$ is a descent sequence which satisfies 
	$$ f(x_{k+1}) \leq f(x_k) - \alpha \cdot \norm{\nabla f(x_k)}^2_F $$
	then in $T$ iterations we must have an element $x_t$ with $t\leq T$ such that
	$$ \norm{\nabla f(x_t)}^2_F \leq 2^{-T \alpha \lambda}.   $$
\end{lemma}

\begin{proof}
	Let $f^*$ be the minimum value of the function $f$ and let $S$ be the sublevel set of $f$ containing $x_0$ over which $f$ is $\lambda$-strongly geodesically convex. Since $\{x_k\}$ is a descent sequence, we know that each $x_k \in S$.
	
	Since $f$ is $\lambda$-strongly geodesically convex in $S$, we have 
	$$ f^* \geq f(x) - \frac{1}{2\lambda} \cdot \norm{\nabla f(x)}_F^2 $$
	for any $x \in S$.
	
	If $\norm{\nabla f(x_k)}_F^2 \leq \varepsilon$, then we will show that in $\ell \leq 1/\alpha \lambda$ steps we must have an element $x_{k+\ell}$ such that $\norm{\nabla f(x_{k + \ell})}_F^2 \leq \varepsilon/2$. This is enough to conclude the proof of the lemma, as with this claim we see that we half the squared norm of the gradient at every sequence of $1/\alpha \lambda$ steps.
	
	To see this, assume that $\norm{\nabla f(x_{k+\ell})}_F^2 \geq \varepsilon/2$ for $0 \leq \ell \leq m$. Then, from our descent property we have
	$$ f(x_{k+1}) \leq f(x_k) - \alpha \cdot \norm{\nabla f(x_k)}^2_F \leq f(x_k) - \alpha \cdot \varepsilon/2$$
	and in particular $f(x_{k + m}) \leq f(x_k) - m \cdot \alpha \cdot \varepsilon/2$. 
	
	On the other hand, our assumption that $\norm{\nabla f(x_k)}_F^2 \leq \varepsilon$, together with strong geodesic convexity of $f$ and minimality of $f^*$ imply 
	$$ f(x_{k}) - \frac{\varepsilon}{2\lambda} \leq f(x_{k}) - \frac{1}{2\lambda} \cdot \norm{\nabla f(x_k)}_F^2 \leq f^* \leq f(x_{k+m}) $$ 
	and therefore we have
	$$ f(x_{k}) - \frac{\varepsilon}{2\lambda} \leq f(x_{k + m}) \leq f(x_k) - m \cdot \alpha \cdot \varepsilon/2 $$
	which implies $m \leq \frac{1}{\alpha \lambda}$. This concludes our proof.
\end{proof}

\subsection{Tensor flip-flop convergence}


\TODO{Proof of \cref{thm:tensor-flipflop}; vanilla strong convexity in a sublevel set stuff.}

\begin{lemma}[Initial Conditions]\label{lem:tensor-initial-conditions}
	There exist absolute constants $\Gamma, \gamma > 0$ such that the following holds.
	When the number of samples $n \geq \Gamma \cdot k^2 \cdot d_1^2/D$, with probability at least $1 - k^2 \cdot \left( \dfrac{nD}{k d_1} \right)^{-\Omega(d_1)} - 2k \cdot e^{- \Omega(nkD/d_1)}$ we have that the following conditions hold:
	\begin{enumerate}
		\item $\norm{\nabla f_x(I)}_F \leq \dfrac{\gamma \cdot k}{8}$ % \dfrac{\lambda^2}{4 k}$
		%\item $f_x$ is $\lambda$-strongly geodesically convex at a ball of radius $\lambda/k$ around $I$, that is: $B_{\lambda/k}(I) := \{ Z \in \smallSym \ \mid \ \norm{Z}_F \leq \lambda/k \}$.
		\item $f_x$ is $\frac{1}{2}$-strongly geodesically convex at a ball of radius $\gamma \cdot k/2$ around $I$, that is: $B_{\gamma \cdot k/2}(I) := \{ Z \in \Sym \ \mid \ \norm{Z}_F \leq \gamma \cdot k/2 \}$.
	\end{enumerate}
\end{lemma}

\begin{proof}
	The lemma follows from the observation that \cref{prop:gradient-bound} implies condition 1, and \cref{thm:ball-convexity} implies condition 2. So all we need to do is to check the parameters.

	By \cref{thm:ball-convexity}, if we set $\gamma = c$ and if the number of samples $n \geq C k^2 d_1^2/D$, where $c, C > 0$ are the constants from \cref{thm:ball-convexity}, then with probability at most 
	$$k^2 \cdot \left( \dfrac{nD}{k d_1} \right)^{-\Omega(d_1)}$$
	the second condition fails to hold.
	
	By \cref{prop:gradient-bound} with parameter $\varepsilon = \frac{\gamma \sqrt{k}}{8}$, if the number of samples satisfies $n \geq \dfrac{2^{14} \cdot d_1^2}{\gamma^2 \cdot k \cdot D}$ then with probability at most 
	$$2 k \cdot \exp\left(- \frac{n k D \gamma^2}{2^{15} d_1}\right) = 2k \cdot e^{- \Omega(nkD/d_1)}$$
	the first condition will fail to hold.
	
	Letting $\Gamma = \max\{2^{14}/\gamma^2, C \}$, having $n \geq \Gamma k^2 d_1^2/D$ samples gives a sample upper bound that holds for both situations above.
	Thus, by the union bound, with probability at most 
	$$ k^2 \cdot \left( \dfrac{nD}{k d_1} \right)^{-\Omega(d_1)} + 2k \cdot e^{- \Omega(nkD/d_1)} $$
	one of the conditions 1 or 2 will fail to hold. This concludes our proof.
\end{proof}

\begin{lemma}[Descent Lemma]\label{lem:tensor-descent-lemma}
	If $\Theta, \Upsilon$ are successive scalings from the flip-flop algorithm and $\norm{\nabla f_x(\Theta)}_F \leq 1$, then we have:
	$$ f_x(\Upsilon) \leq f_x(\Theta) - \dfrac{1}{6k d_k} \cdot \norm{\nabla f_x(\Theta)}_F^2 $$
\end{lemma}

\begin{proof}
	Let 
	$$\rho := \dfrac{\left( \bigotimes_{j=1}^k \Theta_j \right) \cdot \sum_{i=1}^n x_i x_i^\dagger}{\exp(f_x(\Theta))}.$$
	Additionally, let $a \in [k]$ be such that $\nabla_a := \nabla_a f_x(\Theta)$ is largest. As $\Upsilon$ is the successive scaling, we have that $\Upsilon_b = \Theta_b$ when $b \neq a$ and 
	$$ \Upsilon_a = \det(\rho^{(a)})^{1/d_a} \cdot (\rho^{(a)})^{-1} \cdot \Theta_a = \det(d_a \cdot \rho^{(a)})^{1/d_a} \cdot (d_a \cdot \rho^{(a)})^{-1} \cdot \Theta_a. $$
	In particular, the above means that we can write $\bigotimes_{j=1}^k \Upsilon_j$ in the following way:
	$$ \bigotimes_{j=1}^k \Upsilon_j = \det(d_a \cdot \rho^{(a)})^{1/d_a} \cdot E_{(a)} \cdot \bigotimes_{j=1}^k \Theta_j $$
	where $E_{(a)} = I_1 \otimes \cdots \otimes I_{a-1} \otimes (d_a \cdot \rho^{(a)})^{-1} \otimes I_{a+1} \otimes \cdots \otimes I_k$.
	Hence, we have:
	\begin{align*}
		f_x(\Upsilon) &= \log \sum_{i=1}^n \langle x_i , \bigotimes_{j=1}^k \Upsilon_j x_i \rangle \\
		&= \log\left(\tr\left[ \bigotimes_{j=1}^k \Upsilon_j \cdot  \sum_{i=1}^n x_i x_i^\dagger  \right]\right) \\ 
		&= \log\left(\tr\left[ \det(d_a \cdot \rho^{(a)})^{1/d_a} \cdot E_{(a)} \cdot \bigotimes_{j=1}^k \Theta_j \cdot  \sum_{i=1}^n x_i x_i^\dagger  \right]\right) \\
		&= \log\left(\det(d_a \cdot \rho^{(a)})^{1/d_a} \right) +  \log\left(\tr\left[ \cdot E_{(a)} \cdot \bigotimes_{j=1}^k \Theta_j \cdot  \sum_{i=1}^n x_i x_i^\dagger  \right]\right) \\
		&= \log\left(\det(d_a \cdot \rho^{(a)})^{1/d_a} \right) +  \log\left(\tr\left[ E_{(a)} \cdot \rho \right] \cdot \exp(f_x(\Theta)) \right) \\
		&= f_x(\Theta) + \frac{1}{d_a} \cdot  \log\det(d_a \cdot \rho^{(a)})  +  \log\left(\tr\left[ E_{(a)} \cdot \rho \right] \right) \\
		&= f_x(\Theta) + \frac{1}{d_a} \cdot  \log\det(d_a \cdot \rho^{(a)})  +  \log\left(\tr\left[ (d_a \rho^{(a)})^{-1} \cdot \rho^{(a)} \right] \right) \\
		&= f_x(\Theta) + \frac{1}{d_a} \cdot  \log\det(d_a \cdot \rho^{(a)})
	\end{align*}
	Lemma 5.1 from~\cite{GGOW19} states that for any $d$-dimensional PSD matrix $Z$ of trace $d$, the following inequality holds:
	$$ \log\det(Z) \leq \max\left\{- \dfrac{\norm{Z - I_d}_F^2}{6}, - \dfrac{1}{6} \right\}. $$
	Since $\tr \rho^{(a)} = 1$, if $\norm{d_a \rho^{(a)} - I_a}_F \leq 1$ we obtain that:
	\begin{align*}
		\frac{1}{d_a} \cdot  \log\det(d_a \cdot \rho^{(a)}) &\leq - \dfrac{\norm{d_a \rho^{(a)} - I_a}_F^2}{6 d_a} \\
		&= - \dfrac{\norm{\sqrt{d_a} \rho^{(a)} - \frac{1}{\sqrt{d_a}} I_a}_F^2}{6} = - \dfrac{\norm{\nabla_a}_F^2}{6} \leq - \dfrac{1}{6k} \cdot \norm{\nabla f_x(\Theta)}_F^2
	\end{align*}
	If $\norm{d_a \rho^{(a)} - I_a}_F > 1$, we have
	\begin{align*}
		\frac{1}{d_a} \cdot  \log\det(d_a \cdot \rho^{(a)}) &\leq - \dfrac{1}{6 d_a} \leq - \dfrac{1}{6 d_k} \leq - \dfrac{1}{6d_k} \cdot \norm{\nabla f_x(\Theta)}_F^2
	\end{align*}
\end{proof}

\begin{lemma}[Distance to Optimum]\label{lem:tensor-distance-opt}
	Let $\htheta$ be the MLE estimator for $\Theta$, and $\otheta$ be the output of the flip-flop algorithm, assuming that our input samples satisfy the initial conditions of \cref{lem:tensor-initial-conditions}. Then, we have
	$$ d_F(\htheta; \otheta) \leq  $$
\end{lemma}

\begin{proof}
	Let $Z \in \Sym$ be such that $\htheta = \otheta^{1/2} \cdot \exp(\sqrt{\vec d} \cdot Z) \cdot \otheta^{1/2}$, and $\oZ = Z / \norm{Z}_F$.
	Since 
	$$ d_F(\htheta, \otheta) = \norm{I_D - \otheta^{-1/2} \htheta \otheta^{-1/2}}_F = \norm{I_D - \exp(\sqrt{\vec d} \cdot Z)}_F $$
	to prove a good bound on the distance it is enough to show that $\norm{Z}_F$ is small. 
	We will achieve this by using the strong geodesic convexity of $f_x$.
	
	By our initial conditions, we know that $f_x$ is $1/2$-strongly geodesically convex. 
	Thus, we have that the function $g(t) = f_x(\exp_{\otheta} (t \oZ) )$ is $1/2$-strongly convex, $g(0) = \otheta$ and $g(\norm{Z}_F) = \htheta$, which implies:
	$$ g(\norm{Z}_F) \geq g(0) + g'(0) \cdot \norm{Z}_F + \frac{\norm{Z}_F^2}{4}. $$
	Since $\htheta$ is the MLE estimator, we have $g(\norm{Z}_F) \leq g(0)$, and by definition of
	$g$ and Cauchy-Schwarz, we have that 
	$$ g'(0) = \langle \nabla f_x(\otheta), \oZ \rangle \geq - \norm{\nabla f_x(\otheta)}_F \norm{\oZ}_F = - \norm{\nabla f_x(\otheta)}_F. $$
	Putting all the above together, we get 
	$$ \norm{Z}_F \leq 4 \cdot \norm{\nabla f_x(\otheta)}_F. $$ 
	Setting $\delta = \norm{\nabla f_x(\otheta)}_F$, the above inequality implies that $Z_a \preceq 4 \delta I_a$ for each $a \in [k]$. This in turn yields 
	$$ \exp(\sqrt{\vec d} Z) \preceq \exp(\sqrt{\vec d} \cdot 4\delta (I_1, \dots, I_k)) = 
	\exp\left(4 \delta \cdot \sum_{a=1}^k \sqrt{d_a} \right) \cdot I_D $$
	and our distance function becomes:
	$$ d_F(\htheta, \otheta) \leq \exp\left(4 \delta \cdot \sum_{a=1}^k \sqrt{d_a} \right) - 1 \leq 8 \delta \cdot \sum_{a=1}^k \sqrt{d_a}  $$
	where the last inequality we used $e^t \leq 1 + 2t$ for $t \in (0,1)$.
\end{proof}

\RMO{Initial Conditions + descent lemma imply convergence of flip-flop via \cref{lem:descent-sublevel-set} and \cref{lem:convex-ball}}

\begin{theorem}[Restatement of \cref{thm:tensor-flipflop}]
	If $\htheta$ denotes the MLE estimator for $\Theta$, then provided $n = \Omega(k^2 \cdot d_1^2/D)$, the flip-flop algorithm computes $\otheta$ with 
	$$ d_F(\htheta_a, \otheta_a) \leq \epsilon $$
	in $O(k d_k \log(1/\epsilon))$ iterations with probability at least 
	$$ 1 - k^2 \cdot \left( \dfrac{nD}{k d_1} \right)^{-\Omega(d_1)} - 2k \cdot e^{- \Omega(nkD/d_1)}.$$
\end{theorem}

\begin{proof}

\end{proof}

\subsection{Matrix flip-flop convergence}


\TODO{Proof of \cref{thm:matrix-flipflop}}


\CF{prove that flip-flop works as soon as estimation in operator norm works. This goes by using KLR to show strong convexity holds in a ball about the optimizer}.






\section{Noise}
\TODO{make sure things work under some error in the data; somewhat optional}


\section{Complex tensors}
\TODO{explain how to generalize to complex tensor models, maybe some quantum motivation?!}

\section{Open problems}
\TODO{$d_{op}$} for tensors??








\appendix


%One can calculate $$ H_{I, v}(\samp, \samp) = \langle w, \Pi(\samp)^2 w \rangle - \langle w, \Pi(\samp) w \rangle^2 $$ where $w = v/\|v\|$. We may calculate the Hessian $H_{P,v}$ using $H_{P, v} = H_{I, \sqrt{P}v}$.


\section{Pisier's proof of expansion}\label{sec:pisier}
We restate the theorem in language closer to Pisier's original:

% Appendix - \url{https://arxiv.org/abs/1209.2059}
% Theorem 16.6 - \url{https://arxiv.org/pdf/1101.4195.pdf}
%\CF{at some point include a sentence about the setting of $\alpha_i$, maybe in the main body}
%\CF{choose different letter for $n$}
\begin{theorem} \label{thm:Pisier-expansion}
Let $\Pi: \mat(m) \to \mat(m)$ denote the projection onto the traceless matrices, and $Y$ a tuple of i.i.d. standard Gaussians in $\mat(n,m)$. There are constants $c,C > 0$ such that for all $m \leq n$ we have 
\[ \left\| \left(\sum_{i=1}^{N} \alpha_{i} Y_{i} \otimes Y_{i}\right) \circ \Pi \right\|_{op} \leq C t^{2} \|\alpha\|_{2} \left( \E \|Y\|_{op} \right)^{2} \]
with probability at least $ 1- t^{-c(m+n)}$. 
\end{theorem}

The relation to our desired expansion result follows from the following:  \CF{some bad latex thing happening below}
\[ Y \otimes Y vec(Z \in \mat(m)) = vec( Y_{i}^{*} Z Y_{i})      \]
Theorem \ref{thm:hess-pisier} then follows as a corollary by choosing $\alpha \propto \vec{1}$ and standard results from Gaussian concentration. The proof of \ref{thm:Pisier-expansion} proceeds by a symmetrization trick, followed by the trace method. We will first state the necessary concentration results and then give the proof. Our setting required the result on rectangular matrices with strong error bounds, but we claim no originality.  

\begin{theorem} \cite{P86}
We denote a random Gaussian matrix $Y \in \R^{m \times n}$ (for $m < n$) with $Y_{ij} \sim \mathcal{N}(0,1)$, we have the following bound
\[ \E \|Y\|_{op} \leq \sqrt{m} + \sqrt{n} \]
\end{theorem}


\begin{theorem} \cite{P86}
Let $Y$ be a standard Gaussian in a separable Banach space with norm $\|\cdot\|$. Then $\|Y\|$ is subGaussian with parameter $\sigma^{2} := \sup \{ \E \langle Y, \xi \rangle^{2} \mid \|\xi\|_{*} \leq 1 \}; $ that is
\[ \Pr [ | \|Y\| - \E \|Y\| | \geq t ] \leq \exp \left( - \frac{\Omega(t^{2})}{\sigma^{2}} \right).   \]
Another equivalent definition of sub-Gaussianity is
\[ \forall p \geq 2: (\E \|Y\|^{p})^{\frac{1}{p}} \leq \E \|Y\| + O \left( \sqrt{\frac{p}{\sigma^{2}}} \right)   \]
\end{theorem}

\begin{theorem}[Non-commutative H\"older inequality]\label{thm:holder}
For $p = 2^{m}$, we have the following (Holder's type) inequality
\[ |\tr[\prod_{i=1}^{p} A_{i}]| \leq \prod_{i=1}^{p} \|A_{i}\|_{p}   \]
\end{theorem}



\begin{proof} [Proof of \ref{thm:Pisier-expansion}]
We first begin by a standard symmetrization trick to linearize: if $Y$ is a standard Gaussian on $\mat(n,m)$, then by a straightforward calculation $\E (Y \otimes Y)\circ \Pi = 0$.

%\[ \E (Y \otimes Y)\circ \Pi = vec(I_{n}) vec(I_{m})^{*} (I - \Pi) = 0  \]
%then $\E Y \otimes Y (I-\Pi) = 0$ because for any $X \in \mat(m)$ we have
%$$ \E Y \otimes Y (I-\Pi)X = \E (Y \ot Y) X - \frac{1}{m} (\tr X) \E (Y \ot Y)  I_m,$$ which is easily calculated to be zero.
%\CF{I think $m$ and $n$ are switched below}
%\[\E Y^{*} I_{m} Y = m I_{n} \implies \E Y \otimes Y (I-P) = vec(I_{n}) vec(I_{m})^{*} (I-\Pi) = 0\]
Therefore we can add the a copy of the sum with new iid variables $\{Z_{i}\}$: using Jensen's inequality and the convexity of the operator norm, we have
\[ \E_{Y} \left\|\left(\sum_{i=1}^{N} \alpha_{i} Y_{i} \otimes Y_{i}\right) \circ \Pi\right\|_{op} \leq 
\E_{Y,Z} \left\|\left(\sum_{i=1}^{N} \alpha_{i} Y_{i} \otimes Y_{i}-\sum_{i=1}^{N} \alpha_{i} Z_{i} \otimes Z_{i}\right) \circ \Pi \right\|_{op}  \] Now note $(Y,Z)$ has the same distribution as $(\frac{Y+Z}{\sqrt{2}},\frac{Y-Z}{\sqrt{2}})$, so the right-hand-side of the above equation is
\begin{align}\frac{1}{2}\E &\left\|\left(\sum_{i=1}^{N} \alpha_{i} (Y_{i}+Z_{i}) \otimes (Y_{i}+Z_{i})  - \sum_{i=1}^{N} \alpha_{i} (Y_{i}-Z_{i}) \otimes (Y_{i}-Z_{i})\right)\circ \Pi \right\|_{op}\nonumber \\ 
& = \E \left\|\left(\sum_{i} \alpha_{i} Y_{i} \otimes Z_{i}  + \alpha_{i} Z_{i} \otimes Y_{i} \right) \circ \Pi \right\|_{op}  \nonumber\\
& \leq 2 \E \left\|\sum_{i} \alpha_{i} Y_{i} \otimes Z_{i}\right\|_{op}.   \label{eq:yz}
\end{align}
Note we've lost the projection, but the left and right operators are independent. Next we use the trace method to bound the expectation of \cref{eq:yz}; that is, we approximate the operator norm by the Schatten $p$-norm for a high enough $p$ and control these Schatten norms using concentration of moments of Gaussians.  

\begin{align*} \E \left\|\sum_{i} \alpha_{i} Y_{i} \otimes Z_{i}\right\|_{2p}^{2p} & = \E \tr \left[ \left( \sum_{ij} \alpha_{i} \alpha_{j} Y_{i}^{*} Y_{j} \otimes Z_{i}^{*} Z_{j} \right)^{p} \right]  \\
& = \sum_{\vec{i},\vec{j} \in [N]^{p}} \alpha^{\vec{i}} \alpha^{\vec{j}} \E \tr [ Y_{i_{1}}^{*} Y_{j_{1}} ... Y_{i_{p}}^{*} Y_{j_{p}} \otimes Z_{i_{1}}^{*} Z_{j_{1}} ... Z_{i_{p}}^{*} Z_{j_{p}}  ]   \\
& = \sum_{\vec{i},\vec{j}} \alpha^{\vec{i}} \alpha^{\vec{j}} (\E_{Y} \tr [ Y_{i_{1}}^{*} Y_{j_{1}} ... Y_{i_{p}}^{*} Y_{j_{p}} ] ) (\E_{Z} \tr [ Z_{i_{1}}^{*} Z_{j_{1}} ... Z_{i_{p}}^{*} Z_{j_{p}} ] )  
\end{align*}
Here we used independence of $(Y,Z)$. We eventually want to charge to $\E \|\sum_{i} \alpha_{i} Y_{i}\|_{op}$. First note that expectations of Gaussian monomials are non-zero and positive iff the polynomial is even. Therefore the coefficient $\alpha^{\vec{i}} \alpha^{\vec{j}}$ from all non-vanishing terms is a square, and in particular positive. So we can upper bound each term individually by the nc-Holder inequality (\cref{thm:holder}) to find: 
\[\E \|\sum_{i} \alpha_{i} Y_{i} \otimes Z_{i}\|_{2p}^{2p} \leq \sum_{\vec{i},\vec{j}} \alpha^{\vec{i}} \alpha^{\vec{j}} (\E_{Y} \tr [ Y_{i_{1}}^{*} Y_{j_{1}} ... Y_{i_{p}}^{*} Y_{j_{p}} ] ) ( \E_{Z} \|Z_{i_{1}}^{*}\|_{2p} ... \|Z_{j_{p}}\|_{2p} )  \]

We now consider the term $\E_{Z} \|Z_{i_{1}}^{*}\|_{2p} ... \|Z_{j_{p}}\|_{2p}$. Different indeces are independent, and for a repeated index we use Jensen's: 
\[ \forall k \leq 2p: \E \|Z_j\|_{2p}^{k} = \E (\|Z_j\|_{2p}^{2p})^{k/{2p}} \leq  ( \E \|Z_j\|_{2p}^{2p} )^{k/2p}  \] 
Thus, we can collect like terms:

%\begin{claim}
%For iid $\{Z_{i}\}$ and $\sum_{i} q_{i} = 2p$:
%\[ \E \|Z_{1}\|_{2p}^{q_{1}} ... \|Z_{k}\|_{2p}^{q_{k}} \leq \prod_{i} (\E \|Z_{i}\|_{2p}^{q_{i} \cdot 2p/q_{i}} )^{q_{i}/2p} = \E \|Z\|_{2p}^{2p}    \]
%\end{claim}

\[ \E \|\sum_{i} \alpha_{i} Y_{i} \otimes Z_{i}\|_{2p}^{2p}
\leq \left( \E \|Z\|_{2p}^{2p} \right) \left( \sum_{\vec{i},\vec{j}} \alpha^{\vec{i}} \alpha^{\vec{j}} (\E \tr [ Y_{i_{1}}^{*} Y_{j_{1}} ... Y_{i_{p}}^{*} Y_{j_{p}} ] ) \right) \]
\[ = \left( \E \|Z\|_{2p}^{2p} \right) \left( \E \left\| \sum_{i} \alpha_{i} Y_{i} \right\|_{2p}^{2p} \right)
= \left( \E \|Z\|_{2p}^{2p} \right) \left( \sum_{i} \alpha_{i}^{2} \right)^{p} \left( \E \|Y\|_{2p}^{2p} \right)    \]
In the last step we used unitary invariance of the joint $\{Y_{i}\}$ distribution, i.e. that $\sum_{i} c_{i} Y_{i}$ has the same distribution as $\|c\|_{2} Y_{1}$.
Finally we can bound $\|\cdot\|_{op}$ by the trace method. Assume wlog (by homogeneity) that $\|\alpha\|_{2} = 1$:
\[ \E_{Y, Z} \|\sum_{i} \alpha_{i} Y_{i} \otimes Z_{i} \|_{op}^{2p} \leq \E_{Y, Z} \|\sum_{i} \alpha_{i} Y_{i} \otimes Z_{i} \|_{2p}^{2p} \]
\[ \leq ( \E \|Y\|_{2p}^{2p} )^{2} \leq ( m \E \|Y\|_{op}^{2p} )^{2} \leq m^{2} \left(\E \|Y\|_{op} + O(\sqrt{\frac{2p}{\sigma^{2}}})  \right)^{4p} \]
Here we've used that $\|Y\|_{op}$ is subGaussian with parameter 
\[ \sigma^{2} = \sup_{\xi} \frac{ \E \langle Y, \xi \rangle^{2} }{\|\xi\|_{1}^{2}} = \E Y_{11}^{2} \sup_{\xi} \frac{ \|\xi\|_{F}^{2} }{\|\xi\|_{1}^{2} } = 1     \]
So again assuming $\|\alpha\|_{2} = 1$, we can apply Markov's to get the bound: 
\[ \Pr[\|\sum_{i} \alpha_{i} Y_{i} \otimes Y_{i}\|_{op} \geq (2t \E \|Y\|_{op})^{2} ] \leq    \frac{m^{2} (\E\|Y\|_{op} + C \sqrt{p} )^{4p}}{(2t \E\|Y\|_{op})^{4p}}    \]
Now we choose $C\sqrt{p} \leq \E \|Y\|_{op} = \sqrt{m} + \sqrt{n}$, i.e. $p = \left( \frac{\sqrt{m} + \sqrt{n}}{C} \right)^{2}$: 
\[ \leq \frac{m^{2} (2\E \|Y\|_{op})^{4p}}{(2t \E \|Y\|_{op})^{4p}} \leq \frac{m^{2}}{\exp(\frac{4 \log t}{C^{2}} (\sqrt{m} + \sqrt{n})^{2})}    \]
The statement follows by $m \leq n$. 
\end{proof}

%\CF{this section should be modified to be more free standing, i.e. not refer to things "above"}
%We have shown above that the diagonal blocks $\nabla^{2}_{aa} f \approx I$ \CF{ I got rid of the $1/{d_a}$ factor to reflect \cref{lem:hessian}, and changed $I_{a}$ to $I$ because it's an $I$ on matrices not $\CC^{d_a}$, we should figure out how to denote it properly at some point}. Therefore to show strong convexity we would like to bound the off-diagonal blocks \CF{pick better letters for following expression}
%\[ \forall X \perp I_{a},Y \perp I_{b}:  \langle \nabla^{2}_{ab} f, X \otimes Y \rangle \leq \lambda \frac{\|X\|_{F} \|Y\|_{F}}{\sqrt{d_{a} d_{b}}}   \]
%\CF{based on new convention (see \cref{dfn:g-convexity,lem:hessian}) we may want to omit the denominator $\sqrt{d_{a} d_{b}}$ now.} \AR{I think it makes sense to implement our new notational convention in the corollary we use, but this proof is more clear if it's about standard Gaussian imo} Pisier's method of proof uses the trace method along with Gaussian concentration in Banach spaces.


%\begin{corollary}
%For $Y \in \R^{d_{a} \times d_{b}}$ where $Y_{ij} \sim \mathcal{N}(0,1)$, our $n$ samples give Kraus operators of the form $\frac{1}{\sqrt{nD}} Y$, and our off-diagonal operator is
%\[ \sum_{i=1}^{nD/d_{a}d_{b}} \frac{1}{nD} Y_{i} \otimes Y_{i}   \]
%The norm $\|\cdot\|_{0}$ on its restriction to $\Sym_{d_{b}}^{0} \to \Sym_{d_{a}}^{0}$ is less than the quantity in the theorem, which is on the order $\frac{d_{a}+d_{b}}{\sqrt{d_{a} d_{b} nD}}$ whp, so for constant expansion it suffices that $nD \gg \max_{a} d_{a}^{2}$
%\end{corollary}

\bibliographystyle{alpha}
\bibliography{refs}

\end{document}
