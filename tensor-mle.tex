\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amssymb,bm}
\usepackage{xcolor}
\usepackage[capitalize]{cleveref}
\newtheorem{theorem}{Theorem}
\newtheorem{corollary}{Corollary}
\newtheorem{prop}{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corol}[theorem]{Corollary}
\newtheorem{fact}[theorem]{Fact}
\newtheorem{claim}[theorem]{Claim}
\newtheorem{remark}{Remark}
\newtheorem{definition}{Definition}
\newtheorem{example}{Example}

\newcommand{\junk}[1]{}


\newcommand{\R}{{\mathbb{R}}}
\newcommand{\C}{{\mathbb{C}}}
\newcommand{\ot}{\otimes}
\newcommand{\mat}{\operatorname{Mat}}
\newcommand{\Q}{{\mathbb{Q}}}
\newcommand{\N}{{\mathbb{N}}}
\renewcommand{\vec}{\bm}
\newcommand{\Z}{{\mathbb{Z}}}
\renewcommand{\S}{\mathbb{S}}

\newcommand{\E}{\mathbb{E}}
\renewcommand{\Pr}{\mathbb{P}}

\newcommand\eps{\varepsilon}

\newcommand\FF{\mathcal{F}}
\newcommand\HH{\mathcal{H}}
\newcommand\GG{\mathcal{G}}
\newcommand\SL{\operatorname{SL}}
\newcommand\PD{\operatorname{PD}}
\newcommand\GL{\operatorname{GL}}
\newcommand\TT{\mathcal{T}}
\newcommand\PP{\mathcal{P}}
\newcommand\tr{\operatorname{Tr}}
\newcommand\RR{\mathcal{R}}
\newcommand\CC{\mathcal{C}}
\newcommand\BB{\mathcal{B}}
\newcommand\II{\mathcal{I}}
\renewcommand\AA{\mathcal{A}}
\newcommand{\MM}{\mathcal{M}}
\newcommand\AP{\mathcal{AP}}

\newcommand{\eqdef}{:=}
\newcommand{\maps}{\colon}
\newcommand{\ale}{\lesssim}
\newcommand{\age}{\gtrsim}
\newcommand{\CF}[1]{{\color{purple}[CF: #1]}}
\newcommand{\AR}[1]{{\color{green}[AR: #1]}}
\newcommand{\RMO}[1]{{\color{red}[RMO: #1]}}
\newcommand{\TODO}[1]{{\color{blue}[TODO: #1]}}


\title{Logarithmic sample complexity for dense matrix models}
\author{Cole Franks, Rafael Oliveira, Akshay Ramachandran}
\date{February 2020}

\begin{document}

\maketitle
\TODO{
\begin{enumerate}
\item Copy pasta stuff from main.
\item 
\end{enumerate}

}


\section{Introduction}
\CF{background, history; remark that we automatically know flip-flop works but our strong convexity will give efficient algorithms in this setting, as opposed to just converging algorithms. } 

\section{Structured covariance estimation}
We study covariance estimation in the tensor normal model, in which $X$ is sampled according to $N(0, \Sigma = \bigotimes_{a = 1}^{k} \Sigma_a)$ for $\Sigma_a$ a positive-semidefinite $d_a\times d_a$ matrix. Let $D = \prod_{i = 1}^k d_i$. 

First, recall the maximum-likelihood estimator for $\Sigma$ without any structure. If $X_1, \dots, X_n \in \R^D$ are the samples from $N(0, \Sigma)$, then the log-likelihood for $\Sigma$ is (up to an additive constant) given by the function 
$$L(\Sigma^{-1}) = \sum_{i = 1}^n \langle X_i,  \Sigma X_i\rangle - \CF{sg} \log\det \Sigma.$$
If we are only interested in estimating $\Sigma$ only up to a constant, we may instead maximize 
\begin{gather}\tilde{L}(\Sigma^{-1}) = \log \sum_{i = 1}^n \langle X_i,  \Sigma X_i\rangle - \frac{1}{D} \log\det \Sigma. \label{eq:projective-likelihood}\end{gather}
and the solutions will differ only by a constant.

\begin{remark}[Projective space] One may think of \cref{eq:projective-likelihood} as the MLE for $[\Sigma]$, the equivalence class of $\Sigma$ in projective space.
\end{remark}
Restricting $\tilde{L}$ to $\Sigma$ of the form $\bigotimes_{a = 1}^{k} \Sigma_a$, we are led to consider the minima of the following function. \CF{actually, this interpretation isn't quite right. Nonetheless the function below is what we want.}
\begin{definition}[Projective likelihood function]
Given $\vec{X} = (X_1, \dots, X_n)$, let the function $f_{\vec X}:\bigoplus_{i} \PD(d_i) \to \R$ be given by 
\begin{gather}f_{\vec X}(\Sigma_1, \dots, \Sigma_n) = \log \sum_{i = 1}^n \left\langle X_i,  \bigotimes_{i = 1}^k \Sigma_i X_i\right\rangle - \sum_{i = 1}^k \frac{1}{d_i} \log\det \Sigma_i. \label{eq:projective-likelihood}\end{gather}
\end{definition}



Our main theorem is the following:

\begin{theorem} The MLE for $\bigotimes_{a = 1}^{k} \Sigma_a$ from $n$ independent samples satisfies 
$$ d(\hat{\Sigma}, \Sigma) = O(\TODO{}). $$
with probability $\TODO{}$.
\CF{presumably there will be some dimension-dependent factors}.
\end{theorem}

\begin{proof}
\CF{cite the MLE paper and then our sections \cref{sec:g-convex,sec:scaling}.}
\end{proof}

\begin{theorem} If $\hat{\Sigma}$ denotes the MLE estimator for $\Sigma$, then provided $n \geq \TODO{}$ the flip-flop algorithm computes $\underline{\Sigma}$ 
$$ d(\underline{\Sigma}, \hat{\Sigma}) = O(\TODO{1/\sqrt{n}}) $$
in $O(\TODO{\log(1/\eps)})$ iterations with probability at least $\TODO{}$.
\end{theorem}

\begin{proof}
\CF{cite the MLE paper and then our sections \cref{sec:g-convex,sec:scaling}.}
\end{proof}



\section{Geodesic convexity}\label{sec:g-convex}
\TODO{Put these lemmas about how if you are geodesically convex on a ball, you get good condition number bounds/convergence rates for any descent method}

\section{Operator and tensor scaling}\label{sec:scaling}
\TODO{lower bound Hessian for operators and tensors for all different formats; we hope to get strong convexity with $\prod d_i /(d_1^2 + \dots + d_k^2)$ samples. I am concerned that a KLR19-style operator norm type theorem is needed to get $\tilde{O}$ of this, but we will do what we can with the Frobenius bounds for now; I'd expect to need at least $\max_i \sqrt{d_i}$ too many samples.}\\
\TODO{It would also be nice to have that tight example for the $\log$ in KLR19...}

We recall the moment map and Hessian calculations
\[ \partial_{t=0} f(e^{tX_{a}}) = \langle \nabla_{a}, X \rangle = \langle Q^{a} - sI_{a}, X \rangle   \]
\[ \partial_{t=0}^{2} f(e^{tX_{a}}) = \langle X, (\nabla^{2})_{aa} X \rangle = \langle Q^{a}, X^{2} \rangle  \]
\[ \partial_{s=0} \partial_{t=0} f(e^{tX_{a}} \otimes e^{sY_{b}}) = \langle Y, (\nabla^{2})_{ab} X \rangle = \langle Q^{ab}, X \otimes Y \rangle   \]

\section{Operator Scaling}
In this section we have $T$ samples of $g \sim N(0,\frac{1}{T} (\frac{1}{d_{1}} I_{1}) \otimes (\frac{1}{d_{2}} I_{2}))$. We will denote $D := d_{1} d_{2}$. In order to use the KLR analysis, we will show that the one-body marginals have low error in $\|\cdot\|_{op}$ and the whole operator is a sufficient expander at the start. 

\subsection{Moment map}
This is proven using matrix concentration

\begin{theorem} [Bernstein]
Consider independent $\{X_{k}\}$ such that $\E X_{k} = 0$ and $\lambda_{max}(X_{k}) \leq R$ almost surely. Further let the variance be $\sigma^{2} := \|\sum_{k} \E X_{k}^{2} \|_{op}$. 
\begin{eqnarray*} \Pr [ \lambda_{max} \left( \sum_{k} X_{k}  \right) \geq t ] & \leq & d \exp\left( - \frac{\Omega(t^{2})}{\sigma^{2} + t R} \right)
\\ & \leq & \begin{cases} 
d \exp ( - \Omega(t^{2}/\sigma^{2}) ) & \text{if $t \leq \sigma^{2}/R$ } 
\\ d \exp ( - \Omega(t/R) )           & \text{if $t \geq \sigma^{2}/R$}
\end{cases} 
\end{eqnarray*}
\end{theorem}

In our setting, $Q^{a}$ is comprised of $N := \frac{TD}{d_{a}}$ copies of a rank one $g g^{*}$ where each gaussian is $g \sim \mathcal{N}(0, N^{-1} \frac{1}{d_{a}} I_{a} ) = \mathcal{N}(0, \frac{1}{TD} I_{a}) $. We will drop subscripts for $d_{a}, I_{a}$ etc when they can be understood from context. Therefore we define $X := g g^{*} - \frac{1}{TD} I_{a}$ and note the following parameters:
\[ \lambda_{max}(X) = \|g\|_{2}^{2} - \frac{1}{TD} \hspace{10mm} \lambda_{min}(X) = - \frac{1}{TD}   \]
While $\|g\|_{2}$ is unbounded, we can threshold our distribution with a small loss in probability. Since we will be using $\chi^{2}$ distributions much from now on, we will do a quick exercise to prove our threshold bounds:

\begin{definition}
$\chi(\mu,d)$ denotes the $\chi^{2}$ distribution with mean $\mu$ and $d$ degrees of freedom. Explicitly $X \sim \chi(\mu,d) \implies X = \frac{\mu}{d} \sum_{i=1}^{d} g_{i}^{2}$
where $g \sim \mathcal{N}(0,1)$. 
\end{definition}

\begin{lemma}
For $X \sim \chi(\mu,d)$ we have the following (explicit and approximate) formula for the MGF, $\forall \theta < \left(O(\frac{\mu}{d}) \right)^{-1}$:
\begin{eqnarray*} \log \E \exp(\theta X) & = & - \frac{d}{2} \log \left(1 - 2 \theta \frac{\mu}{d} \right) 
\\ & \leq & \theta \mu + \theta^{2} \frac{O(\mu^{2})}{2 d}
\end{eqnarray*}
\end{lemma}

\begin{theorem} [Sub-exp variables]
The above MGF bound gives tail decay:
\[ \forall \theta < b^{-1}: \log \E \exp(\theta (X - \E X)) \leq \theta^{2} \frac{\sigma^{2}}{2} \]
\[ \implies  \Pr[X - \mu \geq t] \leq \begin{cases}
\exp( - \Omega(t^{2}/\sigma^{2}) ) & t \leq \sigma^{2}/b
\\ \exp( - \Omega(t/b) ) & t \geq \sigma^{2}/b
\end{cases}   \]
\end{theorem}

With these bounds in mind, note our variables $\|g\|_{2}^{2} \sim \chi(\frac{d_{a}}{TD},d_{a})$ so we have $\sigma^{2} = \frac{d}{(TD)^{2}}, b = \frac{1}{TD} \implies \sigma^{2}/b = \frac{d}{TD}$
\[ \Pr[ \exists k: \lambda_{max}(X_{k}) \geq M \sqrt{\log N}\frac{d}{TD} ] \leq \exp( - \Omega(M^{2}) )  \]
If we're happy with $1/poly$ failure probability we will take $M^{2} \sim \log D$, so in our matrix bound $R_{max} \leq \frac{d \log D}{TD}$

\[ \E X^{2} = \E (g g^{*})^{2} - \frac{1}{(TD)^{2}} I = \E \|g\|_{2}^{4} \hat{g} \hat{g}^{*} - \frac{1}{(TD)^{2}} \]
\[ = \frac{1}{(TD)^{2}} ( (3d + d(d-1)) \frac{1}{d} I - I ) = \frac{d+1}{(TD)^{2}} I   \]
Here $\hat{g} := g / \|g\|_{2}$ and the calculation is done by independence of $\|g\|_{2}, \hat{g}$. So we also have the variance parameter
\[ \sigma^{2} = N \|\E X^{2}\|_{op} = \frac{TD}{d} \frac{d+1}{(TD)^{2}} \sim \frac{1}{TD}  \]

\begin{corollary}
We have the following operator norm concentration
\[ \Pr[ \|Q^{a} - sI_{a}\|_{op} \geq t ] \leq d \exp \left( - \frac{\Omega(t^{2} TD)}{1 + t d_{a} \log D }  \right)  \]
Since we require $\|\cdot\|_{op}$ error $\ll \frac{1}{d_{a} \log D}$, if we are happy with $1/poly$ failure probability we require $TD \gg \max_{a} d_{a}^{2} \log^{3} D$. 
\end{corollary}

\begin{remark} Note I'm using $\min_{a} d_{a} < \max_{a} d_{a} < D$ in a couple places so the $\log$ term may be slightly sharpened. But the exponent is tight as we require $TD > \max_{a} d_{a}^{2}$ samples for existence/ uniqueness of the solution. 
\end{remark}


\subsection{Pisier's proof of Expansion}
We have shown above that the diagonal blocks $(\nabla^{2})_{aa} \approx \frac{1}{d_{a}} I_{a}$. Therefore to show strong convexity we would like to bound the off diagonal blocks 
\[ \forall X,Y:  \langle Q^{ab}, X \otimes Y \rangle \leq \lambda \frac{\|X\|_{F} \|Y\|_{F}}{\sqrt{d_{a} d_{b}}}   \]
Pisier's method of proof uses the trace method along gaussian concentration in Banach spaces. 

\begin{theorem}
We denote a random gaussian matrix $Y \in \R^{m \times n}$ (for $m < n$) with $Y_{ij} \sim \mathcal{N}(0,1)$, we have the following bound
\[ \E \|Y\|_{op} \leq \sqrt{m} + \sqrt{n} \]
\end{theorem}

\begin{theorem} [Pisier]
For any Banach space $\sigma_{B}^{2} := \sup \{ \E \langle Y, \xi \rangle^{2} \mid \|\xi\|_{*} \leq 1 \} $
\[ \implies \Pr [ | \|Y\| - \E \|Y\| | \geq t ] \leq \exp \left( - \frac{\Omega(t^{2})}{\sigma^{2}} \right)   \]
\end{theorem}

\begin{corollary}
\[ (\E \|Y\|^{p})^{\frac{1}{p}} \leq \E \|Y\| + O \left( \sqrt{\frac{p}{\sigma^{2}}} \right)   \]
\end{corollary}

\begin{theorem}
Let $P$ denote the projection onto $(\vec{I_{n}},\vec{I_{m}})$, then the following holds with $1/poly(mn)$ failure probability
\[ \|\sum_{i=1}^{N} \alpha_{i} Y_{i} \otimes Y_{i} (I - P) \|_{op} \leq O \left( \sqrt{\sum_{i} \alpha_{i}^{2}} \right) \left( \E \|Y\|_{op} \right)^{2} \]
\end{theorem}
\begin{proof}
We first begin by a standard symmetrization trick so that there are no products of variables
\[\E Y^{*} I_{m} Y = m I_{n} \implies \E Y \otimes Y (I-P) = 0\]
\[ \E_{Y} \|\sum_{i=1}^{N} \alpha_{i} Y_{i} \otimes Y_{i} (I - P)\|_{op} \leq \E_{Y,Z} \|\sum_{i=1}^{N} \alpha_{i} Y_{i} \otimes Y_{i} (I - P) - \sum_{i=1}^{N} \alpha_{i} Z_{i} \otimes Z_{i} (I - P) \|_{op}  \]
Now note $(Y,Z)$ has the same distribution as $(\frac{Y+Z}{\sqrt{2}},\frac{Y-Z}{\sqrt{2}})$
\[ = \frac{1}{2}\E_{Y,Z} \|\sum_{i=1}^{N} \alpha_{i} (Y_{i}+Z_{i}) \otimes (Y_{i}+Z_{i}) (I - P) - \sum_{i=1}^{N} \alpha_{i} (Y_{i}-Z_{i}) \otimes (Y_{i}-Z_{i}) (I - P) \|_{op}  \]
\[ = \E_{Y,Z} \|\sum_{i} \alpha_{i} Y_{i} \otimes Z_{i} (I-P) + \alpha_{i} Z_{i} \otimes Y_{i} (I-P)\|_{op}  \]
\[ \leq 2 \E_{Y,Z} \|\sum_{i} \alpha_{i} Y_{i} \otimes Z_{i}\|_{op}   \]
Note we've gotten rid of the projection $(I-P)$, but now the left and right Kraus operators are independent. Now we apply the trace method to this operator
\[ \E \|\sum_{i} \alpha_{i} Y_{i} \otimes Z_{i}\|_{2p}^{2p} \]
\[ = \E Tr [ \left( \sum_{ij} \alpha_{i} \alpha_{j} Y_{i}^{*} Y_{j} \otimes Z_{i}^{*} Z_{j} \right)^{p} ]  \]
\[ = \sum_{\vec{i},\vec{j} \in [N]^{p}} \alpha^{\vec{i}} \alpha^{\vec{j}} \E_{Y,Z} Tr [ Y_{i_{1}}^{*} Y_{j_{1}} ... Y_{i_{p}}^{*} Y_{j_{p}} \otimes Z_{i_{1}}^{*} Z_{j_{1}} ... Z_{i_{p}}^{*} Z_{j_{p}}  ]   \]
\[ = \sum_{\vec{i},\vec{j}} \alpha^{\vec{i}} \alpha^{\vec{j}} (\E_{Y} Tr [ Y_{i_{1}}^{*} Y_{j_{1}} ... Y_{i_{p}}^{*} Y_{j_{p}} ] )^{2}  \]
Here we used independence of $(Y,Z)$ so that we have $(\E Tr [ \cdot ] )^{2}$ instead of $\E Tr[ \cdot ^{2}]$. We eventually want to charge to a term $\E \|\sum_{i} \alpha_{i} Y_{i}\|_{op}$. To this end we notice that since any expectations of polynomials of gaussians with positive coefficients are nonnegative and positive iff the polynomial is even; therefore $\alpha^{\vec{i}} \alpha^{\vec{j}}$ is also positive for all non-vanishing terms. So we can upper bound the term individually by the nc-Holder inequality below
\[ \leq  \sum_{\vec{i},\vec{j}} \alpha^{\vec{i}} \alpha^{\vec{j}} \E_{Y} Tr [ Y_{i_{1}}^{*} Y_{j_{1}} ... Y_{i_{p}}^{*} Y_{j_{p}} ] \E_{Z} \|Z_{i_{1}}^{*}\|_{2p} ... \|Z_{j_{p}}\|_{2p}   \]
\[ = \sum_{\vec{i},\vec{j}} \E_{Z} \prod_{l=1}^{p} \alpha_{i_{l}} \alpha_{j_{l}} \|Z_{i_{l}}\|_{2p} \|Z_{j_{l}}\|_{2p} \E_{Y} Tr [ Y_{i_{1}}^{*} Y_{j_{1}} ... Y_{i_{p}}^{*} Y_{j_{p}} ]  \]
\[ \leq \E_{Y} \|\sum_{i} \alpha_{i} \E_{Z} \|Z_{i}\|_{2p} Y_{i} \|_{2p}^{2p}   \]
Finally, since $\{Y_{i},Z_{i}\}$ are all mutually independent:
\[ = \|\alpha\|_{2}^{2p} ( \E \|Y\|_{2p}^{2p} )^{2} = \|\alpha\|_{2}^{2p} ( \E \|Y^{*} Y\|_{p}^{p} )^{2}  \]
Finally we can bound $\|\cdot\|_{op}$ by the trace method. Assume wlog (by homogeneity) that $\|\alpha\|_{2} = 1$:
\[ \E_{Y,Z} \|\sum_{i} \alpha_{i} Y_{i} \otimes Z_{i} \|_{op}^{2p} \leq \E_{Y,Z} \|\sum_{i} \alpha_{i} Y_{i} \otimes Z_{i} \|_{2p}^{2p} \]
\[ \leq ( \E \|Y\|_{2p}^{2p} )^{2} \leq ( m \E \|Y\|_{op}^{2p} )^{2} \leq m^{2} (\E \|Y\|_{op} + O(\sqrt{\frac{2p}{m+n}}))^{4p} \]
So choosing $p \sim \log m$ and using Markov's on the $2p$-th moment gives the bound. 
\end{proof}

\begin{theorem}
For $p = 2^{m}$, we have the following generalization of Holder's inequality
\[ |Tr[\prod_{i} A_{i}]| \leq \prod_{i} \|A_{i}\|_{p}   \]
\end{theorem}
\begin{proof} [Tao notes]
Done by induction. Note if $A$ are Hermitian, there is a simple tensor trick which allows us to prove the above for all $p \in \mathbb{N}$. 
\end{proof}

\begin{corollary}
For $Y \in \R^{d_{a} \times d_{b}}$ where $Y_{ij} \sim \mathcal{N}(0,1)$, our Kraus operator is of the form $\frac{1}{\sqrt{TD}} Y$, and our off-diagonal operator is
\[ \sum_{k=1}^{TD/d_{a}d_{b}} \frac{1}{TD} Y_{k} \otimes Y_{k}   \]
By the above theorem, the operator norm on the subspace $\perp$ to $(I_{a},I_{b})$ is of the order $\frac{d_{a}+d_{b}}{\sqrt{d_{a} d_{b} TD}}$, so for constant expansion it suffices that $TD \gg \max_{a} d_{a}^{2}$
\end{corollary}


\section{Tensor Scaling}
We will maintain similar notation. We have T samples of $g \sim \mathcal{N}(0,\frac{1}{T} \otimes_{a} \frac{1}{d_{a}} I_{a})$ with $D := \prod_{a} d_{a}$. We don't have a KLR style analysis at the moment, but strong convexity is enough by the FM analysis, and this can be proven by just controlling each bipartite piece. So the operator scaling analysis does give us very good bounds for $\|\mu\|_{op}$ and expansion with $TD \gg \max_{a} d_{a}^{2} \log^{c}(D)$. These bounds are not enough though, so in this section we will follow the FM analysis to give the requirements, then show the required strong convexity, and show how to maintain this under perturbation. 

\subsection{FM Analysis}
Recall that $\forall a: Q^{a} \approx \frac{1}{d_{a}} I_{a}$, so if we can show $\forall a \neq b: \langle Q^{ab}, X \otimes Y \rangle \lesssim \frac{\|X\|_{F} \|Y\|_{F}}{\sqrt{d_{a} d_{b}}}$ then we have strong convexity with $\langle X, \nabla^{2} X \rangle \gtrsim \sum_{a} \frac{\|X_{a}\|_{F}^{2}}{d_{a}}$, i.e. the Hessian is strongly diagonally dominant. We will derive our requirements on strong convexity, perturbation bounds, and initial error. Assume we have the following strong convexity 
\[ \forall X: \sum_{a} \frac{\|X_{a}\|_{F}^{2}}{d_{a}} \leq \kappa^{2}: \langle Y, \nabla^{2}_{e^{X}}, Y \rangle \geq \lambda \sum_{a} \frac{\|Y_{a}\|_{F}^{2}}{d_{a}}  \]

Choose $X$ to be the geodesic towards the optimum and $g(t) := f(e^{tX})$ with the opt at $t=1$:
\[ g'(1) = \int_{0}^{1} g''(t) + g'(0) \geq \lambda \sum_{a} \frac{\|X\|_{F}^{2}}{d_{a}} - |\langle \nabla_{a}, X_{a} \rangle| \]
\[ \geq \sum_{a} \frac{\|X_{a}\|_{F}}{\sqrt{d_{a}}} \left( \lambda \frac{\|X_{a}\|_{F}}{\sqrt{d_{a}}} - \sqrt{d_{a}} \|\nabla_{a}\|_{F}  \right)  \]
\[ \geq \sqrt{\sum_{a} \frac{\|X_{a}\|_{F}^{2}}{d_{a}}} \left( \lambda \sqrt{\sum_{a} \frac{\|X_{a}\|_{F}^{2}}{d_{a}}} - \sqrt{\sum_{a} d_{a} \|\nabla_{a}\|_{F}^{2}} \right)  \]
This is $> 0$ if $\forall a: \lambda > d_{a} \frac{\|\nabla_{a}\|_{F}}{\|X_{a}\|_{F}}$ or $\lambda^{2} > \frac{ \sum_{a} d_{a} \|\nabla_{a}\|_{F}^{2} }{\sum_{a} d_{a}^{-1} \|X_{a}\|_{F}^{2}} $. 


\subsection{$\chi^{2}$ Concentration}
We will be running various bounds and concentration inequalities in the following setting. For $g \sim \frac{1}{TD} \otimes_{a} I_{a}$:
\[ \E \langle \sum_{t} g_{t} g_{t}^{*}, \otimes_{a} X_{a} \rangle  = \prod_{a} \langle \frac{1}{d_{a}} I_{a}, X_{a} \rangle \]
\[ \log \E \exp \theta \langle \sum_{t} g_{t} g_{t}^{*}, \otimes_{a} X_{a} \rangle =   \]


\subsection{Moment Map}
For $g \sim \frac{1}{TD} \otimes_{a} I_{a}$, we want to bound $\|Q^{a} - sI_{a}\|_{F}$ using a net:
\[ \E \langle \sum_{t} g_{t} g_{t}^{*}, X_{a} \rangle = \sum_{i} x_{i} \chi(\frac{1}{d_{a}}, \frac{TD}{d_{a}}) = \langle \frac{1}{d_{a}} I_{a}, X \rangle = 0 \]
\[ \log \E \exp \theta \langle \sum_{t} g_{t} g_{t}^{*}, X_{a} \rangle = \log \E \exp \theta \sum_{i} x_{i} \chi(\frac{1}{d_{a}}, \frac{TD}{d_{a}})  \]
\[ = \sum_{i} \frac{-TD}{2 d_{a}} \log \left( 1 - 2 \theta \frac{x_{i}}{TD} \right)   \]
\[ \lesssim \theta^{2} \frac{\|X\|_{F}^{2}}{2 d_{a} TD} \hspace{10mm} \forall \theta < \left( \frac{\|X\|_{op}}{TD} \right)^{-1}  \]
\[ \implies \Pr[ \langle \sum_{t} g_{t} g_{t}^{*}, X_{a} \rangle \geq \epsilon \|X\|_{F} ] \leq 
\begin{cases} 
\exp( - \Omega(\epsilon^{2} TD d_{a}) ) & \epsilon  < \frac{\|X\|_{F}}{d_{a} \|X\|_{op}} 
\\ \exp ( - \Omega(\epsilon TD) \frac{\|X\|_{F}}{\|X\|_{op}} ) & \epsilon \geq \frac{\|X\|_{F}}{d_{a} \|X\|_{op}}
\end{cases}
\]
We will need the following settings of $\epsilon$ in future:
\[ \epsilon \approx \frac{1}{\sqrt{d_{a}}} \implies \Pr [ d_{a} \|Q^{a} - sI_{a}\|_{F}^{2} \gtrsim c ] \leq \exp(d_{a}^{2} \log d_{a} - c \frac{TD}{\sqrt{d_{a}}})  \]
For which we need $TD \gtrsim \max_{a} d_{a}^{5/2} \log d_{a}$.
\[ \epsilon \approx \frac{1}{d_{a}} \implies \Pr [ d_{a} \|Q^{a} - sI_{a}\|_{F}^{2} \gtrsim \frac{c}{d_{a}} ] \leq \exp(d_{a}^{2} \log d_{a} - c^{2} \frac{TD}{d_{a}})   \]
For which we need $TD \gtrsim \max_{a} d_{a}^{3} \log d_{a}$.

\begin{remark}
Note we lose out on the subgaussian part of the bound only when $\frac{d_{a} \|X\|_{op}^{2}}{\|X\|_{F}^{2}}$ is large. It is quite possible that for our setting, we can bound e.g. the condition number or stable rank of is small w.h.p. In particular if we can show the only relevant part of the net has $s \|X\|_{F}^{2} \geq d \|X\|_{op}$ then we only incur a $\sqrt{s}$ loss in required samples. 
\end{remark}

Actually there seems to be a simpler way to prove these statements using the $\|\cdot\|_{op}$ bounds derived earlier. 
\[ \|Q^{a} - sI_{a}\|_{op} \leq \frac{\sqrt{f(d)}}{d} \implies d_{a} \|Q^{a} - sI_{a}\|_{F}^{2} \leq f(d)  \]
So this means in the first case, we need $TD \gtrsim \max_{a} d_{a}^{2} \log D$ and the second case we need $TD \gtrsim \max_{a} d_{a}^{3} \log D$. But by this analysis we only get $1/poly$ failure probability. 


\subsection{$\delta$-net proof of Expansion}
\[ \E \langle \sum_{t} g_{t} g_{t}^{*}, X_{a} \otimes Y_{b} \rangle = \sum_{ij} x_{i} y_{j} \chi(\frac{1}{d_{a} d_{b}}, \frac{TD}{d_{a} d_{b}}) = \langle \frac{1}{d_{a}} I_{a}, X \rangle \langle \frac{1}{d_{b}} I_{b}, Y \rangle = 0 \]

\[ \log \E \exp \theta \langle \sum_{t} g_{t} g_{t}^{*}, X_{a} \otimes Y_{b} \rangle = \log \E \exp \theta \sum_{ij} x_{i} y_{j} \chi(\frac{1}{d_{a} d_{b}}, \frac{TD}{d_{a} d_{b}})  \]
\[ = \sum_{ij} \frac{-TD}{2 d_{a} d_{b}} \log \left( 1 - 2 \theta \frac{x_{i} y_{j}}{TD} \right)   \]
\[ \lesssim \theta^{2} \frac{\|X\|_{F}^{2} \|Y\|_{F}^{2}}{2 d_{a} d_{b} TD} \hspace{10mm} \forall \theta < \left( \frac{\|X\|_{op} \|Y\|_{op}}{TD} \right)^{-1}  \]
\[ \Pr[ \langle \sum_{t} g_{t} g_{t}^{*}, X_{a} \rangle \geq \lambda \frac{\|X\|_{F} \|Y\|_{F}}{\sqrt{d_{a} d_{b}}} ] \leq 
\begin{cases} 
\exp( - \lambda^{2} TD ) \hspace{7mm} \lambda  < \frac{\|X\|_{F} \|Y\|_{F}}{\sqrt{d_{a} d_{b}} \|X\|_{op} \|Y\|_{op}} 
\\ \exp ( - \lambda TD \frac{\|X\|_{F} \|Y\|_{F}}{ \sqrt{d_{a} d_{b}} \|X\|_{op} \|Y\|_{op}} )
\end{cases}
\]
So for $\lambda \approx \frac{1}{k}$ we need $TD \gtrsim \max_{a} d_{a}^{3} \log D > \max_{a,b} \sqrt{d_{a} d_{b}} (d_{a}^{2} + d_{b}^{2}) \log D$

\begin{remark}
Note again we lose out when $\frac{d \|X\|_{op}^{2}}{\|X\|_{F}^{2}}$ is large. So we would like to show that w.h.p. the singular vectors of our bipartite operator have e.g. small condition number or large stable rank. Again if we can show the relevant part of the net has $s \|X\|_{F}^{2} \geq d \|X\|_{op}^{2}$ then we incur an $s$ factor loss in samples. This is reminiscent of the fact that eigenvectors for random matrices have many delocalization properties, so will look into that. 
\end{remark}



\subsection{Robust proof of Expansion}
In this section we will show expansion under perturbations of the form $\otimes_{a} e^{\delta_{a}}$. Note if $\|\delta\|_{op} \ll 1$ then we can approximate $e^{\delta} - I \approx O(\delta)$. 
\[ \mu := \E \langle \sum_{t} g_{t} g_{t}^{*}, e^{\delta_{a}} X e^{\delta_{a}} \otimes e^{\delta_{b}} Y e^{\delta_{b}} \otimes_{c} e^{2\delta_{c}} \rangle = \langle \frac{e^{2\delta_{a}}}{d_{a}} , X \rangle \langle \frac{e^{2\delta_{b}}}{d_{b}}, Y \rangle \prod_{c \neq a,b} Tr[\frac{e^{2\delta_{c}}}{d_{c}} ]  \]
\[ \log \E \exp \theta \langle \sum_{t} g_{t} g_{t}^{*}, e^{\delta_{a}} X e^{\delta_{a}} \otimes e^{\delta_{b}} Y e^{\delta_{b}} \otimes_{c \neq a,b} e^{2\delta_{c}} \rangle = \log \E \exp \theta \sum_{\vec{i}} x^{\vec{i}} \chi(\frac{1}{D},T)   \]
\[ \leq \theta \mu + \theta^{2} \frac{\|e^{\delta_{a}} X e^{\delta_{a}}\|_{F}^{2} \|e^{\delta_{b}} Y e^{\delta_{b}}\|_{F}^{2} \prod_{c} \|e^{2\delta_{c}}\|_{F}^{2}}{2 TD^{2}} \]
\[ \forall \theta < \left( \frac{\|e^{\delta_{a}} X e^{\delta_{a}}\|_{op} \|e^{\delta_{b}} Y e^{\delta_{b}}\|_{op} \prod_{c} \|e^{2\delta_{c}}\|_{op}}{TD} \right)^{-1}    \]

\begin{lemma}
For $\|\delta\|_{op} \ll 1$
\[ \frac{1}{d} Tr[e^{2\delta}] \leq 1 + O(\|\delta\|_{op})  \]
\[ \langle \frac{1}{d} I, e^{\delta} X e^{\delta} - X \rangle \leq \frac{O(\|\delta\|_{op})}{\sqrt{d}} \|X\|_{F}   \]
\[ \|e^{\delta} X e^{\delta}\|_{F}^{2} \leq (1 + O(\|\delta\|_{op})) \|X\|_{F}^{2}   \]
\end{lemma}
\begin{proof}
Let $\delta := e^{\delta'} - I$ and note $\|\delta\|_{op} \approx \|\delta'\|_{op}$ for small $\delta'$. So we bound
\[ Tr[e^{2\delta'} - I] = \langle I, e^{2\delta'} - I \rangle \leq d \|2\delta + \delta^{2}\|_{op} \leq d O(\|\delta\|_{op})   \]
For the second line we use Cauchy Schwarz $\|X\|_{1} \leq \sqrt{d} \|X\|_{F}$
\[ \langle I, (I + \delta) X (I + \delta') - X \rangle = \langle 2 \delta + \delta^{2}, X \rangle \leq O(\|\delta\|_{op}) \|X\|_{1} \leq O(\|\delta\|_{op}) \sqrt{d} \|X\|_{F}  \]
The second line is similar but simpler using $\|AB\|_{F} \leq \|A\|_{op} \|B\|_{F}$
\end{proof}

So using the above lemma, we have bounds for $\langle X, I_{a} \rangle = \langle Y, I_{b} \rangle = 0$:
\[ \mu \leq \frac{\|X\|_{F} \|Y\|_{F}}{\sqrt{d_{a} d_{b}}} O(\|\delta_{a}\|_{op} \|\delta_{b}\|_{op}) \prod_{c} (1 + O(\|\delta_{c}\|_{op})  \]
\[ \log \E \exp \theta (... - \mu) \leq \theta^{2} \frac{\|X\|_{F}^{2} \|Y\|_{F}^{2}}{2TD} \prod_{c} (1 + O(\|\delta_{c}\|_{op}))  \]
\[ \forall \theta < \left( \frac{\|X\|_{op}\|Y\|_{op}}{TD} \right)^{-1} \prod_{c} (1 - O(\|\delta_{c}\|_{op}))  \]
So basically, as long as $\|\delta\|_{op} \ll 1$ everything is of the same order as in the unperturbed case, and therefore if we run a net on all parts simultaneously (of size $\exp(\sum_{a} d_{a}^{2})$) we get roughly the same probabilistic bounds as the start. 

This creates a bottleneck though as in general the inequality $\|X\|_{op} \leq \|X\|_{F}$ could be tight, so in order to guarantee a small enough perturbation we can only move in the ball $\|X\|_{F} \ll 1$. This is why we wrote out the conditions required for $\|\nabla_{a}\|_{F} \ll \frac{1}{d_{a}}$, as therefore we can assume $\|X_{a}\|_{F} \ll 1$ and require $\lambda \approx 1$. 

\begin{remark}
Here is where we definitely would like a KLR style analysis to exploit the fact that we actually have robustness in $\|\cdot\|_{op}$. 
\end{remark}

\subsection{Conclusion}
At the end of the day we require $TD \gtrsim \max_{a} d_{a}^{3} \log D$ to get small enough $\nabla$ and robust expansion. Note this is $d \log D$ away from the existence/ uniqueness threshold for the solution. 

\section{Noise}
\TODO{make sure things work under some error in the data}







%If $\langle \cdot , \cdot \rangle_P$ is the metric at a point $P$, then the new metric $\langle \cdot, \cdot \rangle_{R,P}$ will be given by 
%$$ \langle X, Y \rangle_{R,P} = \langle R^{1/2} X R^{1/2}, R^{1/2} Y R^{1/2} \rangle_{P}.$$

%At the identity the metric takes the form $\langle X, Y \rangle_R = \tr \sqrt{R} X  \sqrt{R} Y $. In particular the length of the geodesic $t\mapsto \sqrt{P}e^{Xt}\sqrt{P}$ for $t \in [0,1]$ is $\langle X, X \rangle_R$. 

\bibliographystyle{alpha}
\bibliography{refs}

\end{document}
