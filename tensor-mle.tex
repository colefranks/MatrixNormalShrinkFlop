\documentclass[aos]{imsart}
\pdfoutput=1
\RequirePackage[english]{babel}
\RequirePackage[ascii]{inputenc}
\RequirePackage[T1]{fontenc}
\RequirePackage{microtype,amsthm,amsmath,amsfonts,amssymb,mathtools,braket,bm,xcolor,float}
\RequirePackage[authoryear]{natbib}  % \RequirePackage[numbers]{natbib}
\RequirePackage[bookmarksnumbered,colorlinks,citecolor=blue,urlcolor=blue,linkcolor=blue,anchorcolor=green,breaklinks=true]{hyperref}
\RequirePackage{graphicx}
\RequirePackage[capitalize]{cleveref}
\RequirePackage{silence}
\WarningFilter{latexfont}{Font shape}
\WarningFilter{latexfont}{Size substitutions}
\WarningFilter{caption}{Unknown document class}

\usepackage{caption}
\usepackage{subcaption}

\startlocaldefs
\newtheorem{theorem}{Theorem}[section]
\newtheorem{innercustomthm}{Theorem}
\newenvironment{customthm}[1]{\renewcommand\theinnercustomthm{#1}\innercustomthm}{\endinnercustomthm}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{prop}[theorem]{Proposition}
\crefname{prop}{Proposition}{Propositions}
% \newtheorem{obs}[theorem]{Observation}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{remark}[theorem]{Remark}
\newtheorem{remark*}[theorem]{Remark}
\newtheorem{fact}[theorem]{Fact}
% \newtheorem{claim}[theorem]{Claim}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{example}[theorem]{Example}
\floatstyle{boxed}\newfloat{Algorithm}{ht}{alg}
\crefname{Algorithm}{Algorithm}{Algorithms}
\numberwithin{equation}{section}
% \allowdisplaybreaks[4]
\urlstyle{same}

\DeclareMathOperator{\Lie}{Lie}
\DeclareMathOperator{\Lin}{L}
\DeclareMathOperator{\poly}{poly}
\DeclareMathOperator{\op}{op}
\DeclareMathOperator{\vol}{vol}
\DeclareMathOperator{\cut}{cut}
\DeclareMathOperator{\ch}{ch}

\DeclareMathOperator{\Mat}{Mat}
\DeclareMathOperator{\GL}{GL}
\DeclareMathOperator{\tr}{Tr}
\DeclareMathOperator{\rk}{rk}
\DeclareMathOperator{\PD}{PD}
\DeclareMathOperator{\SSPD}{SPD}
\DeclareMathOperator{\vect}{vec}
\DeclareMathOperator*{\argmin}{arg\,min}

\DeclarePairedDelimiter{\abs}{\lvert}{\rvert}
\DeclarePairedDelimiter{\norm}{\lVert}{\rVert}
\DeclarePairedDelimiter{\ip}{\langle}{\rangle}

\newcommand{\R}{{\mathbb{R}}}
\renewcommand{\P}{{\mathbb{P}}}
\newcommand{\C}{{\mathbb{C}}}
\renewcommand{\H}{{\mathbb{H}}}
\newcommand{\G}{{\mathbb{G}}}
\newcommand{\Q}{{\mathbb{Q}}}
\newcommand{\N}{{\mathbb{N}}}
\newcommand{\Z}{{\mathbb{Z}}}
\renewcommand{\S}{\mathbb{S}}
\newcommand{\otheta}{\overline{\Theta}}
\newcommand{\htheta}{\widehat{\Theta}}
\newcommand{\oZ}{\overline{Z}}
\newcommand{\ot}{\otimes}
\renewcommand{\vec}{\bm}
\newcommand{\E}{\mathbb{E}}
\newcommand{\eps}{\varepsilon}
\newcommand{\cN}{\mathcal{N}}
\newcommand{\SL}{\operatorname{SL}}
\newcommand{\Herm}{\operatorname{Herm}}
\newcommand{\Sym}{\mathcal{S}}
\newcommand{\smallSym}{S}
\newcommand{\SPD}{\mathbb{P}}
\newcommand{\samp}{x}
\newcommand{\rv}{x}
\newcommand{\ef}{f}
\newcommand{\eqdef}{:=}
\newcommand{\maps}{\colon}
\newcommand{\DF}{D_{\operatorname{F}}}
\newcommand{\Dop}{D_{\operatorname{op}}}
\newcommand{\DKL}{D_{\operatorname{KL}}}
\newcommand{\DTV}{D_{\operatorname{TV}}}
\newcommand{\dFR}{d_{\operatorname{FR}}}
% \newcommand{\email}[1]{\href{mailto:#1}{\texttt{#1}}}

\def\dmin{d_{\min}}
\def\dmax{d_{\max}}

\newcommand{\CF}[1]{{\color{purple}[CF: #1]}}
\newcommand{\AR}[1]{{\color{orange}[AR: #1]}}
\newcommand{\RMO}[1]{{\color{olive}[RMO: #1]}}
\newcommand{\MW}[1]{{\color{red}[MW: #1]}}
\newcommand{\TODO}[1]{{\color{blue}[TODO: #1]}}

% uncomment to get rid of colored comments
\iffalse
\newcommand{\CF}[1]{{\iffalse[CF: #1]\fi}}
\newcommand{\AR}[1]{{\iffalse[AR: #1]\fi}}
\newcommand{\RMO}[1]{{\iffalse[RMO: #1]\fi}}
\newcommand{\MW}[1]{{\iffalse[MW: #1]\fi}}
\newcommand{\TODO}[1]{{\iffalse[TODO: #1]\fi}}
\fi


\newcommand{\mitn}{\footnotemark[6]}
\newcommand{\nyun}{\footnotemark[7]}

\endlocaldefs

\begin{document}
%=============================================================================
\begin{frontmatter}
\title{Near optimal sample complexity for matrix and tensor normal models via geodesic convexity}
\runtitle{Near optimal sample complexity for matrix and tensor normal models}
%=============================================================================
\begin{aug}
\author[A]{\fnms{Cole} \snm{Franks}\corref{}\ead[label=e1]{franks@mit.edu}},
\author[B]{\fnms{Rafael} \snm{Oliveira}\corref{}\ead[label=e2,mark]{rafael@uwaterloo.ca}},
\author[B]{\fnms{Akshay} \snm{Ramachandran}\corref{}\ead[label=e3,mark]{a5ramachandran@uwaterloo.ca}} \\ \and
\author[C]{\fnms{Michael} \snm{Walter}\corref{}\ead[label=e4]{m.walter@uva.nl}}
\runauthor{C.\ Franks, R.\ Oliveira, A.\ Ramachandran \and M.\ Walter}
\address[A]{Department of Mathematics, Massachusetts Institute of Technology, \printead{e1}}
\address[B]{Cheriton School of Computer Science, University of Waterloo, \printead{e2,e3}}
\address[C]{Korteweg-de Vries Institute for Mathematics and Institute for Theoretical Physics, University of Amsterdam,
\printead{e4}}
\end{aug}
%=============================================================================
\begin{abstract}
The matrix normal model, the family of Gaussian matrix-variate distributions whose covariance matrix is the Kronecker product of two lower dimensional factors, is frequently used to model matrix-variate data. The tensor normal model generalizes this family to Kronecker products of three or more factors. We study the estimation of the Kronecker factors of the covariance matrix in the matrix and tensor models. We show nonasymptotic bounds for the error achieved by the maximum likelihood estimator (MLE) in several natural metrics. In contrast to existing bounds, our results do not rely on the factors being well-conditioned or sparse. For the matrix normal model, all our bounds are minimax optimal up to logarithmic factors, and for the tensor normal model our bound for the largest factor and overall covariance matrix are minimax optimal provided there are enough samples for any estimator to obtain better than constant Frobenius error. In the same regimes as our sample complexity bounds, we show that an iterative procedure to compute the MLE known as the flip-flop algorithm converges linearly with high probability. Our main tool is geodesic convexity in the geometry on positive-definite matrices induced by the Fisher information metric. We also provide numerical evidence that combining the flip-flop algorithm with a simple shrinkage estimator can improve performance in the undersampled regime.

%this is just a test to see if overleaf deletes my code again
%test again to see if overleaf deletes
%testing the overleaf clone

%testing pulling changes from overleaf
%testing diff thingy

\end{abstract}
%=============================================================================
\begin{keyword}[class=MSC2020]
%Primary 62F12, secondary 62F30
\kwd[Primary ]{62F12}
\kwd[; secondary ]{62F30}
\end{keyword}

\begin{keyword}
%Covariance estimation, matrix normal model, tensor normal model, geodesic convexity, operator scaling, maximum likelihood estimation
\kwd{Covariance estimation}
\kwd{matrix normal model}
\kwd{maximum likelihood estimation}
\kwd{geodesic convexity}
\kwd{operator scaling}
\end{keyword}
\end{frontmatter}
%=============================================================================
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Please use \tableofcontents for articles %%
%% with 50 pages and more                   %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\tableofcontents
%=============================================================================

%=============================================================================
\section{Introduction}
%=============================================================================
Covariance matrix estimation is an important task in statistics, machine learning, and the empirical sciences.
We consider covariance estimation for matrix-variate and tensor-variate Gaussian data, that is, when individual data points are matrices or tensors. Matrix-variate data arises naturally in numerous applications like gene microarrays, spatio-temporal data, and brain imaging.
A significant challenge is that the dimensionality of these problems is frequently much higher than the number of samples, making estimation information-theoretically impossible without structural assumptions.

To remedy this issue, matrix-variate data is commonly assumed to follow the \emph{matrix normal distribution} \citep{dutilleul1999mle,werner2008estimation}.
Here the matrix follows a multivariate Gaussian distribution and the covariance between any two entries in the matrix is a product of an inter-row factor and an inter-column factor.
In spatio-temporal statistics this is referred to as a separable covariance structure.
Formally, if a matrix normal random variable~$X$ takes values in the~$d_1\times d_2$ matrices, then its covariance matrix $\Sigma$ is a $d_1d_2\times d_1 d_2$ matrix that is the Kronecker product~$\Sigma_1 \ot \Sigma_2$ of two positive-semidefinite matrices~$\Sigma_1$ and~$\Sigma_2$ of dimension~$d_1\times d_1$ and~$d_2\times d_2$, respectively.
This naturally extends to the \emph{tensor normal model}, where $X$ is a $k$-dimensional array, with covariance matrix equal to the Kronecker product of $k$ many positive semidefinite matrices~$\Sigma_1, \dots, \Sigma_k$.
In this paper we consider the estimation of $\Sigma_1, \dots, \Sigma_k$ from $n$ samples of a matrix or tensor normal random variable $X$.

A great deal of research has been devoted to estimating the covariance matrix for the matrix and tensor normal models, but gaps in rigorous understanding remain. In unstructured covariance matrix estimation, i.e., $k =1$, it is well-known that the maximum likelihood estimator exists whenever $d \geq n$ and achieves mean-squared Frobenius norm error $O(d^2/n)$ and mean-squared operator norm error $O(d/n)$, which are both minimax optimal. This fact is the starting point for a vibrant area of research attempting to estimate the covariance or precision matrix with fewer samples under structural assumptions. Particularly important is the study of graphical models, which seeks to better estimate the precision matrix under the assumption that it is \emph{sparse} (has few nonzero entries).

For the matrix and tensor normal models, much of the work (apart from an initial flurry of work on the asymptotic properties of the maximum likelihood estimator) has approached the sparse case directly. In contrast to the unstructured problem above, the fundamental problem of determining the optimal rates of estimation \emph{without} sparsity assumptions is still largely open. We study this basic question in order to provide a firmer foundation for the large body of work studying its many variants, including the sparse case. We begin by discussing the related work in detail.


In the asymptotic arena, \cite{dutilleul1999mle} and later \cite{werner2008estimation} proposed an iterative algorithm, known as the \emph{flip-flop algorithm}, to compute the maximum likelihood estimator (MLE).
In the latter work, the authors also showed that the MLE is consistent and asymptotically normal, and showed the same for the estimator obtained by terminating the flip-flop after three steps. For the tensor normal model, a natural generalization of the flip-flop algorithm was proposed to compute the MLE \citep{mardia1993spatial,manceur2013maximum}, but its convergence was not proven. Here we will be interested in non-asymptotic rates.

For the matrix normal model, treating the covariance matrix $\Sigma$ as unstructured and estimating it by the sample covariance matrix (the MLE in the unstructured case) yields a mean-squared Frobenius norm error of $O(d_1^2 d_2^2/n)$ assuming $n \geq C d_1 d_2$ for a large enough constant $C$.
The matrix normal model has only $\Theta(d_1^2 + d_2^2)$ parameters, so it should be possible to do much better. The state of the art towards optimal rates for the matrix normal model without sparsity assumptions is the work of \cite{tsiligkaridis2013convergence}, which showed that a three-step flip-flop estimator has mean-squared Frobenius error of $O((d_1^2 + d_2^2)/n)$ for the full matrix $\Sigma$. However, their result requires the individual factors have constant condition number and that $n$ is at least $\tilde{\Omega}(\max\{d_1,d_2\})$. They also did not state a bound for the individual factors $\Sigma_1, \Sigma_2$, and did not state bounds for estimation in the operator norm. For the tensor normal model, \cite{sun2015nonconvex} present an estimator with tight rates assuming constant condition number of the true covariances and foreknowledge of initializers within constant Frobenius distance of the true precision matrices. In both the matrix and tensor case, no estimator for the Kronecker factors has been proven to have tight rates without additional assumptions on the factors' structure.



Regarding the sparse case, simply setting $\Sigma_2 = I_{d_2}$ or $\Sigma_1 = I_{d_1}$, in which case the matrix normal model reduces to standard covariance estimation with $d_2 n$ (resp.\ $d_1 n$) samples, shows the necessity of additional assumptions like sparsity or well-conditionedness if $n < \max\{d_1/d_2, d_2/d_1\}$. \cite{tsiligkaridis2013convergence} also propose a penalized estimator which obtains tighter rates that hold even for~$n\ll d_i$ under the additional assumption that the precision matrices $\Sigma_i^{-1}$ are sparse. In the extremely undersampled regime, \cite{zhou2014gemini} demonstrated a single-step penalized estimator that converges even for a single matrix $(n=1)$ when the precision matrices have constant condition number, are highly sparse, and have bounded $\ell_1$ norm off the diagonal.
\cite{allen2010transposable} also considered penalized estimators for the purpose of missing data imputation.


Even characterizing the existence of the MLE for the matrix and tensor normal model has remained elusive until recently, in contrast to the unstructured case ($k=1$).
\cite{amendola2020invariant} recently noted that the matrix normal and tensor MLEs are equivalent to algebraic problems about a group action called the \emph{left-right action} and the \emph{tensor action}, respectively.
In the computer science literature these two problems are called \emph{operator} and \emph{tensor scaling}, respectively.
Independently from \cite{amendola2020invariant}, it was pointed out by \cite{FM20} that the Tyler's M estimator for elliptical distributions (which is the MLE for the matrix normal model under the additional promise that~$\Sigma_2$ is diagonal) is a special case of operator scaling.
Using the connection to group actions, exact sample size thresholds for the existence of the MLE were recently determined in \cite{derksen2020matrix} for the matrix normal model and subsequently for the tensor normal model in \cite{derksen2020tensor}.
In the context of operator scaling, \cite{gurvits2004classical} showed much earlier that the flip-flop algorithm converges to the matrix normal MLE whenever it exists.
Recently it was shown that the number of flip-flop steps to obtain a gradient of magnitude~$\eps$ in the log-likelihood function for the tensor and matrix normal model is polynomial in the input size and~$1/\eps$ \citep{GGOW19,burgisser2017alternating,burgisser2019towards}.

%In the context of tensor scaling, it was shown earlier that the flip-flop algorithm converges to the tensor MLE whenever it exists \CF{cite tensor scaling}.

%In the current literature on matrix normal and tensor models, typically the estimators are assessed using Frobenius or spectral norm between the estimated parameter and the truth. However, neither of these metrics bound statistical distances of interest such as the total variation or Kullback-Leibler divergence between the true distribution and that corresponding to the estimated parameter, or the Fischer-Rao distance. These quantities are affinely invariant, meaning that for any invertible matrix $g$ we have $d(\Sigma, \Sigma') = d(g \Sigma g^T, g \Sigma' g^T)$. \CF{not exactly sure how to write this, but I want it to say that we get the right rates with no assumptions and we use more appropriate metrics. Amusingly, to get from these metrics TO the less useful metrics or vice versa, one needs the condition number assumptions. Also, it appears that the reason that the better metrics aren't being used is that the estimators didn't have the equivariance property.}

%-----------------------------------------------------------------------------
\subsection{Our contributions}
%-----------------------------------------------------------------------------
We take a geodesically convex optimization approach to provide rigorous nonasymptotic guarantees for the estimation of the precision matrices, without any assumptions on their structure.
For the matrix normal model we provide high probability bounds on the estimator that are tight up to logarithmic factors.
For the tensor normal model, our bounds are tight up to factors of~$k$ (the number of Kronecker or tensor factors) whenever it is information-theoretically possible to recover the factors to constant Frobenius error.

%our rates are tight in every regime up to logarithmic factors, and for the tensor normal model our rates our tight assuming it is information-theoretically possible to recover the factors up to constant Frobenius error with high probability.

In the current literature on matrix normal and tensor models, typically the estimators are assessed using Frobenius or spectral norm distance between the estimated parameter and the truth.
However, neither of these metrics bound statistical dissimilarity measures of interest such as the total variation or Kullback-Leibler divergence between the true distribution and that corresponding to the estimated parameter, or the Fisher-Rao distance.
The latter statistical measures enjoy an invariance property for multivariate normals -- namely, they are preserved under acting on both random variables by the same invertible linear transformation.
Such transformations only change the basis in which the data is represented; ideally the performance of estimators should not depend on this basis and hence should not require the covariance matrix to be well-conditioned.

Here we consider the \emph{relative Frobenius error} $\DF(A \Vert B) = \norm{I - B^{-1/2} A B^{-1/2}}_F$ of the precision matrices.
This dissimilarity measure is invariant under the linear transformations discussed above, whereas the the Frobenius norm distance is not.
Moreover, the relative Frobenius error~$\DF(\Theta_1 \Vert \Theta_2)$, the total variation distance $\DTV(\mathcal{N}(0, \Theta_1^{-1}), \mathcal{N}(0, \Theta_2^{-1}))$, the square root of the KL-divergence $\DKL(\mathcal{N}(0, \Theta_1^{-1}), \mathcal{N}(0, \Theta_2^{-1}))^{1/2}$, and the Fisher-Rao distance between $\Theta_1$ and $\Theta_2$ all coincide ``locally.''
That is, if any of them is at most a small constant then they are all on the same order.
The estimation of precision and covariance matrices under~$\DKL$ was suggested by \cite{james1992estimation} due to its natural invariance properties, and has been studied extensively (e.g., \cite{ledoit2012nonlinear}).
To obtain the sharpest possible results, we also consider the \emph{relative spectral error} $\Dop(A \Vert B) = \norm{I - B^{-1/2} A B^{-1/2}}_{\op}$, which has been studied in the context of spectral graph sparsification.
%The relative spectral error converges to zero in the regimes where the relative Frobenius error does not, and implies the Frobenius error bounds by the relation $\DF \leq \sqrt{d} \Dop$.
The dissimilarity measure $\DF(A\Vert B)$ (resp.\ $\Dop(A\Vert B)$) can be related to the usual norm $\|A - B\|_F$, (resp.\ $\|A - B\|_{\op}$) by a constant factor assuming the spectral norms of $B, B^{-1}$ are bounded by a constant.
Though we caution that $\DF$ and $\Dop$ are not truly metrics, we will call them distances because they approximately (or ``locally'') obey symmetry and the triangle inequality.
See \cref{sec:rel-error} for a discussion of these properties.

Informally, our contributions are as follows:
\begin{enumerate}
\item Consider the matrix normal model for $d_1 \leq d_2$. We show that for some $n_0 =\widetilde{O}( d_2/d_1)$, if $n \geq n_0$ then the MLE for the precision matrices $\Theta_1, \Theta_2$ has error $\widetilde{O}(\sqrt{{d_1}/{nd_2}})$ for~$\Theta_1$ and $\widetilde{O}(\sqrt{{d_2}/{nd_1}})$ for~$\Theta_2$ in $\Dop$ with probability $1 - O(e^{ - \Omega ( d_1)})$.
For estimating $\Theta_1$ alone, we obtain the error $\widetilde{O}(\sqrt{{d_1}/{n\min\{d_2, n d_1\}}})$ for any $n$. The $\widetilde{O}$ notation suppresses logarithmic factors in $d_1$ and $d_2$.
%As a corollary the MSE in $\DF$ is $O( \max \{ \frac{d_1^2}{nd_2}, \frac{d_2^2}{nd_1} \} )$.
% \item For the matrix normal model, if $n \geq C (\max\{d_1^2,d_2^2\}/d_1 d_2) \log^2 d_1$, then the MLE for the precision matrices $\Theta_1, \Theta_2$ has MSE $O( \max\{ d_1^2, d_2^2 \}/ (n d_1 d_2) )$ in $\Dop$.
% As a corollary the MSE in $\DF$ is $O( \max \{ d_1^3, d_2^3 \} / (nd_1d_2) )$.
\item In the tensor normal model, for $k$ fixed we show that for some $n_0 = O( \max\{d_i^3\}/ \prod_{i=1}^k d_i)$, if $n \geq n_0$ then the MLE for the precision matrix $\Theta$ has error $O( \frac{\dmax}{\sqrt{n}} )$ in $\DF$ with probability $1 - (n D /\max\{d_i^2\})^{-\Omega(\min\{d_i\})}$.
We also give bounds for growing $k$ and for the individual Kronecker factors $\Theta_i$. Our bound for the error of thes Kronecker factor of largest dimension is tight.
\item Under the same sample requirements as above in each case, the flip-flop algorithm of \citep{mardia1993spatial,manceur2013maximum} converges exponentially quickly to the MLE with high probability.
As a consequence, the output of the flip-flop algorithm with $O_k\left(\dmax + \log n \right)$ iterations is an efficiently computable estimator that enjoys statistical guarantees at least as strong (up to constant factors) as those we show for the MLE.
%As a corollary, there is an algorithm to compute the MLE up to precision $\eps$ in $\DF$ or $\Dop$ with runtime polynomial in the input size and~$\log\frac1\eps$ with high probability.
\item To handle the undersampled case, we introduce a shrinkage-based estimator that is much simpler to compute than the LASSO-type estimators of \cite{tsiligkaridis2013convergence,sun2015nonconvex,zhou2014gemini} and give empirical evidence that it improves on them in a generative model for \emph{dense} precision matrices.
\end{enumerate}
%\CF{Mention $\DTV$ again.}

We now discuss the tightness of our results.
Our first result is tight up to logarithmic factors in the sense that it is information-theoretically impossible to obtain an error bound that is smaller by a polylogarithmic factor and holds with constant probability.
%For $n$ a polylogarithmic factor smaller than $n_0$, it is information-theoretically impossible to obtain any finite bound independent of $\Theta$ with high probability.\MW{$\leftarrow$ Do the preceding two sentences say the same thing? How about `constant' vs `high probability'?}
We also show that our results for estimating $\Theta_1$ alone are tight up to logarithmic factors; i.e., that it is impossible to obtain a rate better than $O(\sqrt{d_1/ n \min\{n d_1, d_2\}})$. Similarly, for the second result, provided $n \geq n_0$ it is impossible to obtain an error bound that is smaller than ours by a factor tending to infinity that holds with constant probability. For $n \ll n_0$, no constant error bound on the $\DF$ error of the largest Kronecker factor can hold with constant probability. Apart from the lower bound for estimating $\Theta_1$ alone, which to our knowledge is novel, these tightness results follow by reduction to known results on the Frobenius and operator error for covariance estimation; see \cref{sec:lower}.

For interesting cases of the tensor normal model such as $d\times d \times d$ tensors we just require that $n$ is at least a large constant.
For the matrix normal model, our first result removes the added constraint $n \geq C \max\{d_1,d_2\}$ in \cite{tsiligkaridis2013convergence}.
We leave extending the $\Dop$ bounds for the matrix normal model to the tensor normal model as an open problem.

We now briefly discuss our estimator for the undersampled case, which is described in detail in \cref{sec:numerics}. The MLE is a function of the sample covariance matrix, but in the undersampled case the MLE need not exist. To remedy this, we replace the sample covariance matrix by a shrinkage estimator for it (in particular, by taking a convex combination with the identity matrix) and then compute the MLE for the ``shrunk'' covariance matrix. Though our estimator is empirically outperformed by \cite{zhou2014gemini} for sparse precision matrices, it emprically outperforms \cite{zhou2014gemini} in a natural \emph{dense} generative model of random approximately low-rank Kronecker factors which we refer to as the ``spiked'' model. Moreover, we found that on average our estimator is faster to compute than the estimator of \cite{zhou2014gemini} by a factor of 500 for the matrix model with $d_1 = 100, d_2 = 200$. Given this empirical evidence, we view our shrinkage-based estimator as a potentially useful tool for the undersampled tensor normal model which merits further theoretical attention.

%Our regularizer has a Bayesian interpretation as coming from a Wishart prior for the covariance, and is closer in spirit to the shrinkage estimators considered in \cite{goes2020robust}.
%-----------------------------------------------------------------------------
\subsection{Outline}
%-----------------------------------------------------------------------------
In the next section, \cref{sec:main results}, we precisely describe the model and formally state our results.
In \cref{sec:tensor-normal}, we prove our main sample complexity bound for the tensor normal model (\cref{thm:tensor-frobenius}).
In \cref{sec:matrix-normal} we prove our improved bound for the matrix normal model (\cref{thm:matrix-normal}).
Our results on the flip-flop algorithm for tensor and matrix normal models (\cref{thm:tensor-flipflop,thm:matrix-flipflop}, respectively) are proven in \cref{sec:flipflop}.
\Cref{sec:lower} contains the proofs of our lower bound for the matrix normal model, and \cref{sec:numerics} contains empirical observations about the performance of our regularized estimator.

%-----------------------------------------------------------------------------
\subsection{Notation}
%-----------------------------------------------------------------------------
We write $\Mat(d)$ for the space of $d\times d$ matrices with real entries, $\PD(d)$ for the convex cone of $d\times d$ positive definite symmetric matrices; and $\SSPD(d)$ for the $d\times d$ positive definite symmetric matrices with unit determinant; $\GL(d)$ denotes the group of invertible $d\times d$ matrices with real entries.
For a matrix $A$, $\norm{A}_{\op}$ denotes the operator norm, $\norm{A}_F = (\tr A^T A)^{\frac12}$ the Frobenius norm, and $\braket{A,B} = \tr A^T B$ the Hilbert-Schmidt inner product.
% We extend these definitions to tuples $A=(A_0;A_1,\dots,A_k)$, where~$A_0\in\R$ and the $A_a$ for $a\in[k]$ are matrices and denote them by the same symbol, i.e., $\norm{A}_F = (\abs{A_0}^2 + \sum_{a=1}^k \norm{A_a}_F^2)^{1/2}$ and similarly for the inner product.
For functions $f,g\colon S \to \R$ for any set $S$, we say $f = O(g)$ if there is a constant $C > 0$ such that $f(x) \leq C g(x)$ for all~$x \in S$, and similarly $f = \Omega(g)$ if there is a constant $c > 0$ such that $f(x) \geq c g(x)$ for all $x \in S$.
If~$f = O(g)$ and~$g = O(f)$ we write $f \asymp g$.
We also abbreviate $[k]=\{1,\dots,k\}$ for $k\in\N$.
All other notation is introduced in the remainder.
%If the constant $C, c$ depends on another parameter $k$ we write $O_k, \Omega_k$, respectively.
%\MW{Define $O, \Theta$ notation and say that for us this is always about universal constants.}

%=============================================================================
\section{Model and main results}\label{sec:main results}
%=============================================================================
In this section we define the matrix and tensor normal models and we state our main technical results.

%-----------------------------------------------------------------------------
\subsection{Matrix and tensor normal model}\label{subsec:model}
%-----------------------------------------------------------------------------
The tensor normal model, of which the matrix normal model is a particular case, is formally defined as follows.

\begin{definition}
For positive definite matrices $\Sigma_1,\dots,\Sigma_k$, we define the \emph{tensor normal model} as the centered multivariate Gaussian distribution with covariance matrix given by the Kronecker product $\Sigma = \Sigma_1 \ot \dots \ot \Sigma_k$.
For $k=2$, this is known as the \emph{matrix normal model}.
\end{definition}

\noindent
Note that if each $\Sigma_a$ is a $d_a\times d_a$ matrix then $\Sigma$ is a $D\times D$-matrix, where $D=d_1 \cdots d_k$.
Our goal is to estimate $k$ Kronecker factors $\Sigma_1, \dots, \Sigma_k$ such that $\Sigma \approx \Sigma_1 \ot \cdots \ot \Sigma_k$ given access to $n$ i.i.d.\ random samples $x_1, \dots, x_n \in \R^D$ drawn from the model.

One may also think of each random sample $x_j$ as taking values in the set of $d_1 \times \dots \times d_k$ arrays of real numbers.
There are $k$ natural ways to ``flatten'' $x_j$ to a matrix:
for example, we may think of it as a $d_1 \times d_2d_3\cdots{}d_k$ matrix whose column indexed by $(i_2,\dots, i_k)$ is the vector in $\R^{d_1}$ with $i_1^{\text{th}}$ entry equal to $(x_j)_{i_1, \dots, i_k}$.
In the tensor normal model, the $d_2d_3\cdots{}d_k$ many columns are each distributed as a Gaussian random vector with covariance proportional to~$\Sigma_1$.
In an analogous way we may flatten it to a $d_2 \times d_1d_3\cdots{}d_k$ matrix, and so on.
% Similarly the columns of the $d_2 \times d_1d_3\cdots{}d_k$ flattening have covariance proportional to~$\Sigma_2$, and so on.
As such, the columns of the $a^{\text{th}}$ flattening can be used to estimate~$\Sigma_a$ up to a scalar.
However, doing so na\"ively (e.g., using the sample covariance matrix of the columns) can result in an estimator with very high variance.
This is because the columns of the flattenings are not independent.
In fact they may be so highly correlated that they effectively constitute only one random sample rather than $d_2\cdots{}d_k$ many.
The MLE decorrelates the columns to obtain rates like those one would obtain if the columns were independent.

The MLE is easier to describe in terms of the precision matrices, the inverses of the covariance matrices.

\begin{definition}[Precision matrices]
For a $D\times D$-covariance matrix~$\Sigma$ arising in the tensor normal model, we refer to $\Theta = \Sigma^{-1}$ as the \emph{precision matrix}.
We also define the \emph{Kronecker factor precision matrices} $\Theta_1, \dots, \Theta_k$ as the unique positive-definite matrices such that $\Theta = \Theta_1 \ot \cdots \ot \Theta_k$ and $(\det \Theta)^{1/d_a}$ is the same for each $a \in [k]$.
In other words, we choose $\Theta_a = \lambda \Theta'_a$ where $\det \Theta'_a = 1$ and $\lambda>0$ is a constant (not depending on $a\in [k]$). We make this choice because the Kronecker factors of $\Theta$ are determined only up to a scalar.
\end{definition}

Let~$\P$ denote the manifold of all precision matrices $\Theta$ for the tensor normal model of fixed format $d_1 \times \cdots \times d_k$, i.e.,
% As above, we can fix this by working with tuples of precision matrices with equal determinant:
\begin{align*}
  \P &= \bigl\{ \Theta = \Theta_1 \ot \dots \ot \Theta_k \;:\; \Theta_a \in \PD(d_a) \bigr\}.
\end{align*}
Given a tuple $x$ of samples $\samp_1,\dots,\samp_n\in\R^D$, the following function is proportional to the negative log-likelihood: % $\ell(\Theta|x) = \frac{n}2 \log \det \Theta - \frac12 \sum_{i=1}^n x_i^T \Theta x_i$, which we can rewrite as
\begin{align}\label{eq:neg log likelihood}
  \ef_\samp(\Theta)
=  \frac{1}{nD}\sum_{i = 1}^n \samp_i^T \Theta \samp_i -  \frac{1}{D}\log\det\Theta.
\end{align}
% Though $\Theta_a$ are not identifiable, the above expression is nonetheless well-defined. <- \MW{confusing, since there is no Theta_a in the equation above...}
The \emph{maximum likelihood estimator (MLE)} for $\Theta$ is then
\begin{align}\label{eq:mle}
  \widehat{\Theta} := \underset{\Theta \in \P}{ \argmin} f_x(\Theta)
\end{align}
whenever the minimizer exists and is unique.
We write $\widehat\Theta = \widehat\Theta(x)$ when we want to emphasize the dependence of the MLE on the samples~$x$, and we say $(\htheta_1, \dots, \htheta_k)$ is \emph{an} MLE for~$(\Theta_1, \dots, \Theta_k)$ if $\otimes_{a = 1}^k \htheta_a = \htheta$.
Note that $\P$ is not a convex domain under the Euclidean geometry on the $D\times D$ matrices.

%-----------------------------------------------------------------------------
\subsection{Results on the MLE}\label{subsec:mle-results}
%-----------------------------------------------------------------------------
We may now state our result for the tensor normal models precisely.
As mentioned in the introduction, we use the following natural distance measures.

\begin{definition}[Relative error]\label{dfn:relative-error}
For positive definite matrices $A, B$, define their \emph{relative Frobenius error} (or \emph{Mahalanobis distance}) as
\begin{align}\label{eq:def D_F}
  \DF(A \Vert B) = \norm{I - B^{-1/2} A B^{-1/2}}_F.
\end{align}
Similarly, define the \emph{relative spectral error} as
\begin{align}\label{eq:def D_op}
  \Dop(A \Vert B) = \norm{I - B^{-1/2} A B^{-1/2}}_{\op}.
\end{align}
\end{definition}

To state our results, and throughout this paper, we write $\dmin = \min_a d_a$, $\dmax = \max_a d_a$. Recall also that $D = \prod_{i=1}^k d_a$. Recall that we identify $\Theta_1,\dots,\Theta_k$ from $\Theta$ using the convention $\det\Theta_1^{1/d_1}=\dots=\det\Theta_k^{1/d_1}$, and likewise for the MLE $\htheta$. For the reader interested only in the behavior for constant $k$, we display this special case afterwards in \cref{cor:tensor-constant-k}.

\newcommand{\TensorFrob}[2]{%
There is a universal constant~$C>0$ such that the following holds.
Suppose $t \geq 1$ and
\begin{#1}#2
  n \geq C k^2 \frac{\dmax^2}D \max \{ k, \dmax \} t^2.
\end{#1}
% \begin{align*}
%   \MW{OLD:} n \geq C k^2 \frac{\dmax^2}{D} \max\{k, \dmax\} \max\{1 , t^2\}.
% \end{align*}
Then the MLE~$\htheta = \htheta_1 \ot \cdots \ot \htheta_k$ for $n$ independent samples of the tensor normal model with precision matrix~$\Theta = \Theta_1 \ot \cdots \ot \Theta_k$ satisfies
\begin{align*}
  \DF(\htheta_a\Vert\Theta_a) &= O\left(t \, k^{1/2} \dmax \sqrt{\frac{d_a}{n D}} \right) \quad\text{ for all } a\in[k] \\
\quad\text{and}\quad
  \DF(\htheta\Vert\Theta) &= O\left(t \, k^{3/2} \frac{\dmax}{\sqrt{n}}\right),
\end{align*}
with probability at least
\begin{align*}
  1 - k e^{-\Omega\bigl( t^2 \dmax \bigr)} - k^2 \left( \frac{\sqrt{nD}}{k \dmax} \right)^{-\Omega(\dmin)}.
\end{align*}}

\begin{theorem}[Tensor normal Frobenius error]\label{thm:tensor-frobenius}
\TensorFrob{equation}{\label{eq:eps sqr assm}}
\end{theorem}

The error for the precision matrix~$\Theta_a$ with $d_a = \dmax$ matches that of the MLE for the precision matrix of a single Gaussian with $D/\dmax$ samples, which is the special case when all the other Kronecker factors are the identity.
We also state our result for a constant number of Kronecker factors~$k$, as is often the case in applications.

\begin{corollary}[Tensor normal Frobenius error, constant $k$]\label{cor:tensor-constant-k}
For any fixed $k$, there is a constant~$C = C(k)>0$ such that the following holds.
Suppose $t \geq 1$ and~$n \geq C \frac{\dmax^3}D t^2$.
Then the MLE~$\htheta = \htheta_1 \ot \cdots \ot \htheta_k$ for $n$ independent samples of the tensor normal model with precision matrix~$\Theta = \Theta_1 \ot \cdots \ot \Theta_k$ satisfies
\begin{align*}
  \DF(\htheta_a\Vert\Theta_a) &= O\left(t \, \dmax \sqrt{\frac{d_a}{n D}} \right) \quad\text{ for all } a\in[k] \\
\quad\text{and}\quad
  \DF(\htheta\Vert\Theta) &= O\left(t  \frac{\dmax}{\sqrt{n}}\right),
\end{align*}
with probability at least
\begin{align*}
  1 - e^{-\Omega\bigl( t^2 \dmax \bigr)} -  \left( \frac{\sqrt{nD}}{ \dmax} \right)^{-\Omega(\dmin)}.
\end{align*}
\end{corollary}


\begin{remark*}[Fisher-Rao distance]
Our bounds on $\DF$ in the above theorem follow from a stronger bound on the \emph{Fisher-Rao distance}
\begin{align}\label{eq:fisher rao}
  \dFR(\htheta, \Theta):= \frac{1}{\sqrt{2}}\|  \log \Theta^{-1/2} \htheta \Theta^{-1/2}\|_F = \frac1{\sqrt 2} \norm{\log \Theta^{-1} \Theta'}_F.
\end{align}
As we will discuss, the Fisher-Rao distance arises from the Fisher information metric for centered Gaussians parameterized by their covariance matrices \citep{skovgaard1984riemannian}.
With the same hypotheses and failure probability as \cref{thm:tensor-frobenius}, we have $d(\htheta, \Theta)  = O(\sqrt{k} \, \dmax \eps/\sqrt{nD})$; see \cref{lem:tensor-geodesic}.
\end{remark*}

For the matrix normal model $(k=2)$, we obtain a stronger result.
Recall that we identify~$\Theta_1, \Theta_2$ from~$\Theta$ using the convention $\det\Theta_1^{1/d_1}=\det\Theta_2^{1/d_2}$.
% define the MLE's $\htheta_1, \htheta_2$ to be the minimizers of $f$ restricted to the subset $\{P \in \PD(d_1): \det P = 1\} \times \PD(d_2)$.

\newcommand{\MatrixSpec}{%
There are universal constants~$C, c>0$ with the following property.
Suppose $1 < d_1\leq d_2$, $t\geq c$, and
\begin{align*}
  n \geq C \frac{d_2}{d_1} \max\{\log d_2, t^2 \log^2 d_1\}.
\end{align*}
Then the MLE $\htheta = \htheta_1 \ot \htheta_2$ \ for $n$ independent samples from the matrix normal model with precision matrix $\Theta = \Theta_1 \ot \Theta_2$ satisfies
\begin{align*}
  \Dop(\widehat{\Theta}_1 \Vert \Theta_1) = O\left(t \sqrt{\frac{d_1}{nd_2}} \log d_1\right)
\quad\text{and}\quad
\Dop(\widehat{\Theta}_2 \Vert \Theta_2) = O\left(t \sqrt{\frac{d_2}{nd_1}} \log d_1 \right)
\end{align*}
with probability at least  $1 - O(e^{ - \Omega( d_1 t^2)})$.}

\begin{theorem}[Matrix normal spectral error]\label{thm:matrix-normal}
\MatrixSpec
\end{theorem}

In applications such as brain fMRI, one is interested only in $\Theta_1$, and $\Theta_2$ is treated as a nuisance parameter.
Assuming the nuisance parameter $\Theta_2$ is known, we can compute $(I \ot \Theta_2^{1/2} )X$, which is distributed as $nd_2$ independent samples from a Gaussian with precision matrix $\Theta_1$.
In this case, one can estimate $\Theta_1$ in operator norm with an RMSE rate of $O(\sqrt{ d_1/ n d_2})$ no matter how large $d_2$ is.
One could hope that this rate holds for $\Theta_1$ even when $\Theta_2$ is not known.
In \cref{sec:lower} we show that, to the contrary, the rate for $\Theta_1$ cannot be better than $O(\sqrt{d_1/ n \min(n d_1, d_2)})$.
Thus, for $d_2 > n d_1$, it is impossible to estimate $\Theta_1$ as well as one could if $\Theta_2$ were known.
Note that in this regime there is no hope of recovering $\Theta_2$ even if $\Theta_1$ is known.
As the random variable $Y_i$ obtained by ignoring all but $d_2' \approx nd_1$ columns of each $X_i$ is distributed according to the matrix normal model with covariance matrix $\Sigma_1 \ot \Sigma_2'$ for some $\Sigma_2 \in \PD(d_2')$, the MLE for $Y$ obtains a matching upper bound.

\begin{corollary}[Estimating only $\Theta_1$]
There are universal constants~$C, c>0$ with the following property.
Let $X$ be distributed according to the matrix normal model with precision matrix $\Theta = \Theta_1 \ot \Theta_2$ and suppose that $1 < d_1 \leq d_2$ and $t\geq c$.
Let $Y = (Y_1, \dots, Y_n)$ be the random variable obtained by removing all but
\begin{align*}
  d_2' = \min \left\{ d_2, \frac{nd_1}{C \max \{ \log n, t^2 \log^2 d_1 \}} \right\}
\end{align*}
columns of $X_i$ for each~$i \in [n]$.
Then the MLE $\htheta = \htheta_1 \ot \htheta_2$ for $Y$ satisfies
\begin{align*}
  \Dop(\widehat{\Theta}_1 \Vert \Theta_1) = O\left(t \sqrt{\frac{d_1}{nd_2'}} \log d_1\right).
  \end{align*}
 with probability $1 - O(e^{ - \Omega( d_1 t^2)})$.
 This rate is tight up to factors of $\log d_1$ and $t^2 \log^2 d_1$.
\end{corollary}


%-----------------------------------------------------------------------------
\subsection{Flip-flop algorithm}\label{subsec:flip-flop}
%-----------------------------------------------------------------------------
The MLE can be computed by a natural iterative procedure known as the \emph{flip-flop algorithm} \citep{dutilleul1999mle,gurvits2004classical}. For simplicity, we describe it for the matrix normal model ($k=2$), so that the samples $\samp_i$ can be viewed as $d_1\times d_2$ matrices which we denote by $X_i$.
Initialize $\overline{\Theta}_1 = I_{d_1}$, $\overline{\Theta}_2 = I_{d_2}$, and choose a distance measure, say $\DF$ in the case below, and a tolerance $\eps > 0$.
\begin{enumerate}
\item Set $\overline{\Theta}_1 \leftarrow (\frac{1}{n d_2} \sum_{i = 1}^n X_i \overline{\Theta}_2 X_i^T)^{-1}.$
\item\label{it:sinkhorn second} Set $\Upsilon = \frac{1}{n d_1} \sum_{i = 1}^n X_i^T \overline{\Theta}_1 X_i$.
If $\DF( \Upsilon \Vert  \overline{\Theta}_2^{-1}) > \eps$, set $\overline{\Theta}_2 \leftarrow \Upsilon^{-1}$ and return to Step 1.
\item Output $\overline{\Theta}_1, \overline{\Theta}_2$.
\end{enumerate}

We can motivate this procedure by noting that if in the first step we already have $\overline{\Theta}_2 = \Theta_2$, then $\frac{1}{n d_2} \sum_{i = 1}^n X_i \overline{\Theta}_2 X_i^T$ is simply a sum of outer products of $nd_2$ many independent random vectors with covariance $\Sigma_1 = \Theta_1^{-1}$; as such the inverse is a good estimator for $\Theta_1$.
As we don't know $\Theta_2$, the flip-flop algorithm instead uses $\overline{\Theta}_2$ as our current best guess. We will use a slightly different stopping criterion in our formal description of the algorithm

For the general tensor normal model, in each step the flip flop algorithm chooses one of the dimensions $a \in [k]$ and uses the $a^\text{th}$ flattening of the samples~$x_i$ (which are just $X_i$ and $X_i^T$ in the matrix case) to update $\overline{\Theta}_a$.
%-----------------------------------------------------------------------------
\subsection{Results on the flip-flop algorithm}
%-----------------------------------------------------------------------------
Our next results show that the flip-flop algorithm can efficiently compute the MLE with high probability when the hypotheses of \cref{thm:tensor-frobenius} or \cref{thm:matrix-normal} hold.
We first state our result for the general tensor normal model and then give an improved version for the matrix normal model.
In the theorems that follow, we use the same convention for choosing the Kronecker factors as in the preceding section.

\newcommand{\TensorFlop}{
	Let $\htheta$ denote the MLE estimator for $\Theta$. There exsits a universal constant $C > 0$ such that,  provided $n = C \cdot k^2 \cdot \dmax^3/D$, the flip-flop algorithm computes $\otheta$ with
	$$ \DF(\htheta_a \ \Vert  \ \otheta_a) \leq \eps $$
	in $O(k \log(1/\eps))$ iterations with probability at least
	$$ 1 - k^2 \cdot \left( \dfrac{\sqrt{nD}}{k \dmax} \right)^{-\Omega(\dmin)} - 2k \cdot e^{- \Omega(nD/k \dmax^2)}.$$}

\begin{theorem}[Tensor normal flip-flop]\label{thm:tensor-flipflop}
\TensorFlop
\end{theorem}

\newcommand{\MatrixFlop}{
Let $(\widehat{\Theta}_1,\widehat{\Theta}_2)$ denote the MLE for $(\Theta_1,\Theta_2)$. There exists a universal constant $\Gamma > 0$ such that for $d_1 \leq d_2$ and $\eps > 0$, when given
$$n \geq \Gamma \cdot \dfrac{d_2}{d_1} \cdot \max\left\{ \log d_2, \dfrac{\log^2 d_1}{\eps^2}, \eps^2 \right\}$$
% \CF{the $\log d_2/d_1$ got replaced with $\log d_2$, the $\eps^2$ was added in the max. \RMO{} make sure this doesn't mess anything up with your proofs. I think it should be fine because it is stronger.} samples in the matrix normal model, the flip-flop algorithm computes $(\overline{\Theta}_1,\overline{\Theta}_2)$ with
\MW{sentence missing}
\begin{align*}
  \Dop(\overline{\Theta}_a, \widehat{\Theta}_a) \leq \eps
\end{align*}
for $a\in\{1,2\}$ in $O\left(d_2 +  \log(1/\eps) \right)$ iterations with probability at least $1 - e^{- \Omega(d_1 \eps^2)}$.
}

\begin{theorem}[Matrix normal flip-flop]\label{thm:matrix-flipflop}
\MatrixFlop\end{theorem}

Plugging in the error rates for the MLE from \cref{thm:tensor-frobenius,thm:matrix-normal} into \cref{thm:tensor-flipflop,thm:matrix-flipflop} (with $\eps = 1$) shows that the output of the flip-flop algorithm with $O\left(\dmax + k \log \dmax + \log(n)\right)$ iterations is an efficiently computable estimator with the same statistical guarantees as we have shown for the MLE.

One may wonder why in the above theorems we consider the distances for the individual tensor factors and not the covariance matrix itself, but bounds for the covariance matrix itself follow from the above bounds which are tight apart from the logarithmic factor in the matrix normal case and a constant factor in general.

%=============================================================================
\section{Sample complexity for the tensor normal model}\label{sec:tensor-normal}
%=============================================================================
It was observed by \cite{wiesel2012geodesic} that the negative log-likelihood exhibits a certain variant of convexity known as \emph{geodesic convexity}.
In this section, we use geodesic convexity, following a strategy similar to \cite{FM20}, to prove \cref{thm:tensor-frobenius}.
Our improved result for the matrix normal model, \cref{thm:matrix-normal}, requires additional tools and will
be proved later in \cref{sec:matrix-normal}.

%-----------------------------------------------------------------------------
\subsection{Geodesic convexity}\label{subsec:geom}
%-----------------------------------------------------------------------------
We now discuss the geodesic convexity used here and outline the strategy for our proof.
We start by introducing a Riemannian metric on the manifold $\PD(D)$ of positive-definite real symmetric $D\times D$ matrices.
Rather than simply considering the metric induced by the Euclidean metric on the symmetric matrices, we consider the Riemannian metric whose geodesics starting at a point $\Theta \in \PD(D)$ are of the form $t \mapsto \Theta^{1/2} e^{Ht} \Theta^{1/2}$ for~$t \in \R$ and a symmetric matrix~$H$.
This metric arises from the Hessian of the log-determinant \citep{bhatia2009positive} and also as the Fisher information metric on centered Gaussians parametrized by their covariance matrices \citep{skovgaard1984riemannian}.
If $\Theta$ is positive definite and $A$ an invertible matrix then $A\Theta A^T$ is again  positive definite.
The transformation $\Theta \mapsto A\Theta A^T$ is an isometry with respect to this metric, i.e., it preserves the geodesic distance, as well as the statistical distances~$\DF$ and $\Dop$ that we use (\cref{dfn:relative-error}).
This invariance is natural because changing a pair of precision matrices in this way does not change the statistical relationship between the corresponding Gaussians; in particular the total variation distance, Fisher-Rao, and Kullback-Leibler divergence are unchanged.

As observed by \cite{wiesel2012geodesic}, the negative log-likelihood is convex as the precision matrix moves along the geodesics of the Fisher information metric, and in particular for the tensor normal model it is convex along geodesics in $\P = \{ \Theta_1 \ot \dots \ot \Theta_k \in \PD(d_1) \times \dots \times \PD(d_k) \}$.
This is because the geodesics in $\PD(D)$ between elements of the manifold
\begin{align*}
  \P = \bigl\{ \Theta = \Theta_1 \ot \dots \ot \Theta_k \;:\; \Theta_a \in \PD(d_a) \bigr\}
\end{align*}
remain in $\P$.
That is, $\P$ is a \emph{totally geodesic submanifold} of $\PD(D)$.
Its tangent space can be identified with the real vector space
\begin{align*}
  \H &= \bigl\{ (H_0; H_1,\dots,H_k) \;:\; H_0 \in \R, \; H_a \text{ a symmetric traceless $d_a \times d_a$ matrix} \, \forall a \in [k] \bigr\},
\end{align*}
equipped with the norm and inner product
\begin{align*}
  \norm{H}_F := \braket{H, K}^{1/2}, \quad
  \braket{H,K} := H_0 K_0 + \sum_{a=1}^k \tr H_a^T K_a.
\end{align*}
The direction $(1; 0, \dots, 0)$ changes $\Theta$ by an overall scalar, and tangent directions supported only in the $a^{th}$ component for $a \in [k]$ only change~$\Theta_a$; subject to its determinant staying fixed.
The geodesics on $\P$ are simply the geodesics of the Fisher-information metric on~$\PD(D)$, but we will define them precisely in terms of the tangent space $\H$ as follows to fix our conventions.

\begin{definition}[Exponential map, geodesics, balls]
Let $\Theta=\Theta_1\ot\cdots\ot\Theta_k\in\P$.
The \emph{exponential map} $\exp_\Theta \colon \H \to \P$ at~$\Theta$ is defined by
%\begin{align*}\exp_\Theta(H) &=   e^{\frac{H_0}{k \sqrt{D}}}\cdot ( \Theta_1^{1/2} e^{\sqrt{\frac{d_1}{D}} H_1} \Theta_1^{1/2}) \ot  \dots \ot (\Theta_k^{1/2} e^{\sqrt{\frac{d_k}{D}} H_k} \Theta_k^{1/2}).\end{align*}
\begin{align*}
  \exp_\Theta(H)
&= e^{H_0} \cdot ( \Theta_1^{1/2} e^{\sqrt{d_1} H_1} \Theta_1^{1/2}) \ot \cdots \ot (\Theta_k^{1/2} e^{\sqrt{d_k} H_k} \Theta_k^{1/2}).
 % \\
% &= \Theta^{1/2} e^{H_0 I_D + \sum_{a=1}^k \sqrt{d_a} I_{d_1\cdots d_{a-1}} \ot H_a \ot I_{d_{a+1}\cdots d_k} } \Theta^{1/2}
\end{align*}
The \emph{geodesics} through $\Theta$ are the curves $t \mapsto \exp_\Theta(t H)$ for $t\in\R$ and $H\in\H$.
We take the convention that the geodesics have unit speed if $\norm{H}_F^2 = 1$.

Up to reparameterization, there is a unique geodesic between any two points of~$\P$.
The geodesic distance $d(\Theta,\Theta')$ between two points $\Theta$ and $\Theta'=\exp_\Theta(H)$ is therefore equal to~$\norm H_F$, which is equal to the Fisher-Rao distance between $\Theta$ and $\Theta'$ apart from an overall factor of $\sqrt{2/D}$.
For comparison with \cref{eq:fisher rao},
\begin{align}\label{eq:fisher-vs-geo}
  d(\Theta,\Theta') = \frac1{\sqrt D} \norm{\log \Theta^{-1/2} \Theta' \Theta^{-1/2}}_F = \sqrt{\frac{2}{D}} \cdot \dFR(\Theta, \Theta').
\end{align}
where $\log$ denotes the matrix logarithm.
We choose this normalization because it will make the negative log-likelihood functions typically~$\Omega(1)$ strongly convex while ensuring that the gradients are~$O(1)$.

The closed \emph{(geodesic) ball} of radius~$r>0$ about~$\Theta$ is defined as
\begin{align*}
  B_r(\Theta) = \bigl\{ \exp_\Theta(H) : \norm H_F \leq r \bigr\},
\end{align*}
The manifold $\PD(D)$, and hence $\P$, is a \emph{Hadamard manifold}, i.e., a complete, simply connected Riemannian manifold of non-positive sectional curvature \citep{bacak2014convex}. Thus geodesic balls are \emph{geodesically convex} subsets of~$\P$, that is, if $\gamma(t)$ is a geodesic such that~$\gamma(0),\gamma(1) \in B_r(\Theta)$ then $\gamma(t) \in B_r(\Theta)$ for all $t\in[0,1]$.
% The geodesic distance squared between two points $P,Q\in\P$ is given by
% \begin{align*}
%   \sum_{a=1}^k \frac 1{d_a} \norm{\log(P_a^{-1/2} Q_a P_a^{-1/2})}_F^2.
% \end{align*}}
\end{definition}

Using our definition of geodesics, we obtain the following notion of geodesic convexity of functions.

\begin{definition}[Geodesic convexity]
Given a geodesically convex domain~$D \subseteq \P$, we say that a function $f$ is \emph{(strictly) geodesically convex} on~$D$ if, and only if, the function $t \mapsto f(\gamma(t))$ is (strictly) convex on~$[0,1]$ for any geodesic $\gamma(t)$ with $\gamma(0),\gamma(1)\in D$. It is called $\lambda$-\emph{strongly} geodesically convex if $t \mapsto f(\gamma(t))$ is $\lambda$-strongly convex along every unit-speed geodesic $\gamma$ with endpoints in $D$.
% Geodesic convexity on $D$ is equivalent to $0$-strong convexity on $D$.

For a twice differentiable function $f\colon \P \to \R$, we can ensure that it is $\lambda$-strong geodesically convex on $D$ by requiring that $f$ is $\lambda$-strong geodesically convex \emph{at} $\Theta$, i.e. $\partial^2_{t=0} f(\exp_\Theta(tH)) \geq \lambda \norm H_F^2$, for all~$H\in\H$, for every $\Theta \in D$.
%\MW{This paragraph is not connected to the preceding one, which is strange. We could simply kill the first paragraph, but make sure that it still connects to the discussion of ``convexity vs Hessians'' below.}
\end{definition}

\begin{example}\label{exa:usual-likelihood} It is instructive to consider the case $k = 1$, or $\P = PD(d)$.
The metric in this case is simply a constant ($1/\sqrt{d_1}$) multiple of the metric considered in \citep{bhatia2009positive,skovgaard1984riemannian}; c.f. \cref{eq:fisher-vs-geo}.
The geodesics through $\Theta$ are the curves $t \mapsto \sqrt{\Theta} e^{H t} \sqrt{\Theta}.$ As an example of a geodesically convex function, consider the likelihood for the precision matrix of a Gaussian with data $x_1, \dots, x_n$.
Let $\rho := \frac1{nD}\sum_i x_i x_i^T$ denote the matrix of ``second sample moments'' of the data.
Then we can rewrite the objective function~\eqref{eq:neg log likelihood} as
\begin{align*}%\label{eq:obj via rho}
  f_x(\Theta) = \tr \rho \, \Theta - \frac1D \log \det \Theta.
\end{align*}
We claim that $f_x(\Theta)$ is always geodesically convex, and in fact \emph{strictly} geodesically convex whenever $\rho$ is invertible. Indeed,
$$ \partial^2_{t = 0} f_x(\sqrt{\Theta} e^{t H} \sqrt{\Theta}) = \tr \sqrt{\Theta} \rho \sqrt{\Theta} H^2 \geq 0 $$
with strict inequality whenever $\rho$ is invertible (and~$H$ nonzero).
\end{example}

% A function $f\colon \P \to \R$ is said to be \emph{geodesically convex} at $P\in\P$ if the functions $t \mapsto f(\exp_P(tH))$ are convex in $t\in\R$ for all~$H \in \H$.
% Assuming $f$ is twice differentiable, this holds if, and only if, $\partial^2_t f(\exp_P(tH)) \geq 0$ for all~$H\in\H$.

% Similarly, $f$ is called \emph{$\lambda$-strongly geodesically convex} at $P$ for some $\lambda>0$ if the same is true for the functions $t \mapsto f(\exp_P(tH))$ for all~$H\in \H$.
% Assuming the function is twice differentiable, this holds if, and only if, $\partial^2_t f(\exp_P(tH)) \geq \lambda \norm H_F^2$ for all~$H\in\H$.

The invariance properties described above for $\PD(D)$ are directly inherited to $\P$.
The manifold~$\P$ carries a natural action by the group
\begin{align*}
  \G =  \{ A = A_1 \ot \dots \ot A_k \;:\; G_a \in \GL(d_a) \}
\end{align*}
Namely, if $\Theta \in \P$ and $A \in \G$ then the $A \Theta A^T$ is in $\P$.
Moreover, as discussed above, the mapping $\Theta \mapsto A\Theta A^T$ is an isometry of the Riemannian manifold~$\P$ and it preserves the statistical distances~$\DF$ and $\Dop$.

% We also note that \MW{It's not \dots} the MLE obeys a certain \emph{equivariance} property under such transformations.
% For all $P \in \P$, $A \in \G$, and samples $x=(x_1,\dots,x_n)$, the log-likelihood satisfies
% \begin{align*}
%   \ell_{Ax}(P) = \ell_x(A^T P A),
% \end{align*}
% where we write $A x = ((G_1 \ot \dots \ot G_k) x_1, \dots, (G_1 \ot \dots \ot G_k) x_n)$.
% Thus the MLE $\widehat P=\widehat P(x)$ satisfies
% \begin{align}\label{eq:equivariance}
%   \widehat P(\samp) = P^{1/2} \widehat P(P^{1/2} \samp) P^{1/2}
% \end{align}
% assuming either MLE exists and is unique.

% %-----------------------------------------------------------------------------
% \subsection{Notation}
% %-----------------------------------------------------------------------------
% \CF{some aspects of this seem awfully specific to the tensor normal model and maybe could wait until after we define it, or simply merge the two, i.e., "Notation and model"}
% \MW{I think I like the former best.}
% The letter $n$ will denote a number of samples, and $d_1\leq \dots \leq d_k$ will denote sorted dimensions, and we set $D:=\prod_{i = 1}^k d_i$. Let $\PD_d$ denote the positive definite $d\times d$ matrices with unit determinant, and $\PD_d^1$ the subset of $\PD$ with unit determinant. $\rv$ will denote the random tuple $(\rv_1, \dots, \rv_n)$ where $\rv_i \in \R^{D}$ are drawn i.i.d from the tensor normal model with precision matrix $\Theta \in \PD_D$, and $\samp$ will denote a deterministic tuple of tensors.

% Let $\smallSym_d$ denote the vector space of $d\times d$ real symmetric matrices, and $\smallSym^0_d$ the subspace of traceless matrices in $\smallSym_d$, i.e., the tangent space of $\PD_d^1$. Let~$\SL_d$ denote the group of $d\times d$ matrices with unit determinant.
% %Then, $A^T e^Z A \in \PD_d^1$ for any $A \in \SL_d$ and $Z\in\Sym_d^0$.
% % Any matrix in $\PD_d^1$ can be written as the matrix exponential of a matrix in $\Sym_d^0$.
% Let
% $$\SL = \prod_{a=1}^k \SL_{d_a}, \SPD = \prod_{a = 1}^k \PD_{d_a}^1, \Sym = \bigoplus_{a = 1}^k \smallSym_{d_a}^0.$$ For a $k$-tuple of $A = (A_1, \dots, A_k)$ of matrices, $\|A\|_F^2:=\sum_{i = 1}^k \|A_i\|_F^2$.
%  We denote by $AB=(A_1B_1,\dots,A_kB_k)$ and $e^Z=(e^{Z_1},\dots,e^{Z_k})$ the componentwise product and matrix exponential, respectively, of matrix tuples $A, B \in \SL$ and $Z\in\Sym$. $I$ will denote an identity matrix, and $I_{a}$ a $d_a\times d_a$ identity matrix. $\langle \cdot, \cdot \rangle$ denotes the standard inner product. $C, c$ denote large (resp.\ small) absolute constants that change line to line.


% \CF{further notation to be introduced; delete as is done}
% \begin{itemize}
% %\item Number of samples $n$, dimensions $d_1\leq \dots \leq d_k$. $D$ for product of these.
% %\item $X$ for the tensor random variable, $\samp_1, \dots, \samp_n$ for each , $\samp = (\samp_1, \dots, \samp_n)$ for the random tuple of samples. $\rho = \samp \samp^T/\|\samp\|_F^2$. Lower case $x$ for samples.

% %\item $\rv$ for the random tuple $(\rv_1, \dots, \rv_n)$, $x$ for the tuple of samples $(x_1, \dots, x_n)$ when no longer random. Think of $\rv$ as $D \times n$ matrix, $\rho = \rv\rv^T,xx^T$ etc.
% \item When $x_i$ is a matrix, which unfortunately does happen sometimes, we'll use $x_i^T$ for the matrix transpose (open to suggestions on this one).
% %\item $\braket{\cdot,\cdot}_{\vec d}$ denote modified Hilbert-Schmidt inner products\MW{sadly the corresponding norms look like $\ell_p$ norms},
% \item $f_{\rv}$ for the function in \cref{dfn:function}, mostly drop $\rv$. $\langle \cdot, \cdot \rangle$ is the $\ell_2$ inner product of vectors

% %\item $\smallSym_d$ for $d \times d$ real symmetric (meh), $\PD_d$ for $d \times d$ real positive definite, $\smallSym_d^0$ for traceless symmetric, $\PD_d^1$ for $\det=1$ positive definite?
% \item $\Theta$ for big tensor product pd precision matrix, $\Theta_a$ for individual pd's. \CF{at some point $\Theta$ gets used for the tuple. We need to decide what to do about this.}
% %\item I'm going to call $\SL = \oplus \SL_{d_i}, \SPD = \oplus \PD_{d_i}^1, \Sym = \oplus \smallSym_{d_i}^0$. Explain somewhere how $\Sym$ is the tangent space of $\SPD$.
% %\MW{Suppressing the $^1$ and $^0$ is a bit confusing I think. Maybe $\operatorname{SPD}$ for $\SPD$ with $\det=1$? I still feel that $\Sym$ looks somewhat horrible (with or without subscript, but I am not sure what would be better).}
% %\item $G = \prod_{a=1}^k \SL_{d_a}$, $P = \prod_{a=1}^k \PD_{d_a}^1$, and $S = \bigoplus_{a=1}^k \Sym_{d_a}^0$.

% \item $\nabla, \nabla^2$ for Riemannian Hessians and gradients, $\nabla_a f$, $\nabla^2_{ab} f$ for components. $\nabla f$ means at the identity. $\|\nabla^2_{ab}f\|_{\op}:=\|\nabla^2_{ab}f\|_{F\to F}$. \CF{as operator from $\Sym_{d}^0$ to self?}
% %\item $C, c$ large (resp.\ small) constants that change line to line.
% \item $\rho^{(a)}$, $\rho^{(ab)}$ for marginals.
% %\item $I$ for an identity matrix, $I_a$ for the $d_a \times d_a$ identity matrix
% \end{itemize}


%-----------------------------------------------------------------------------
\subsection{Sketch of proof}\label{subsec:proof-sketch}
%-----------------------------------------------------------------------------
%As discussed above, the MLE problem reduces to minimizing $\sum_{i=1}^n x_i^T ( \bigotimes_{a=1}^k P_a ) x_i$ over $P \in \P$.
%We take as our objective its logarithm, which is also geodesically convex.

%Then the maximum likelihood estimator is given by $\widehat\Theta_a = {\widehat P_0}^{1/k} \widehat P_a$. We write $\widehat P = \widehat P(x)$ and $\widehat\Theta = \widehat\Theta(x)$ when we want to emphasize the dependence of the MLE on the samples~$x$.



%\begin{align}\label{eq:tilde ell}  \tilde\ell_{\samp}(\Theta) = \frac1D\log\det \Theta - \log \sum_{i=1}^n \braket{\samp_i, \Theta \samp_i};\end{align}

% One may think of \cref{eq:f extension} as proportional, up to an additive constant, to the log-likelihood of $[\samp_1, \dots, \samp_n]$ under the tensor normal model.
% Here $[\samp_1, \dots, \samp_n]$ denotes the equivalence class of the tuple of samples $(x_1, \dots, x_n)$ in projective space.


With these definitions in place, we are able to state a proof plan for \cref{thm:tensor-frobenius}. The proof is a Riemannian version of the standard approach using strong convexity.

\begin{enumerate}
\item\label{it:reduce}
\textbf{Reduce to identity:}
We can obtain $n$ independent samples from $\cN(0, \Theta^{-1})$ as $x'_i = \Theta^{-1/2} x_i$, where $x_1,\dots,x_n$ are distributed as $n$ independent samples from a standard Gaussian.
The MLE $\widehat{\Theta}(x')$ for the former is exactly $\Theta^{1/2} \widehat{\Theta}(x) \Theta^{1/2}$.
By invariance of the relative Frobenius error, $\DF(\widehat\Theta(x') \Vert \Theta) = \DF(\widehat\Theta(x) \Vert I_D)$; the same is true for $\Dop$.
This shows that to prove \cref{thm:tensor-frobenius} it is enough to consider the case that $\Theta = I_D$, i.e., the standard Gaussian.
\item\label{it:grad} \textbf{Bound the gradient:}
Show that the gradient $\nabla f_x(I_D)$ (defined below) is small with high probability.
\item\label{it:convexity} \textbf{Show strong convexity:}
Show that, with high probability, $f_x$ is $\Omega(1)$-strongly geodesically convex near $I$.
\end{enumerate}

These together imply the desired sample complexity bounds -- as in the Euclidean case, strong convexity in a suitably large ball about a point implies the optimizer cannot be far. Moreover, it happens that under alternating minimization $f_\samp$ obeys a descent lemma (similar to what is shown in~\cite{burgisser2017alternating}); as such the flip-flop algorithm must converge exponentially quickly by the strong geodesic convexity of~$f_\samp$.

To make this discussion more concrete, we now define the gradient and Hessian formally, and state the lemma that we will use to relate the gradient and strong convexity to the distance to the optimizer as in the plan above.

\begin{definition}[Gradient and Hessian]\label{def:hess grad}
Let $f\colon \P \to \R$ be a once or twice differentiable function and $\Theta \in \P$.
The \emph{(Riemannian) gradient}~$\nabla f(\Theta)$ is the unique element in $\H$ such that
\begin{align*}
  \braket{\nabla f(\Theta), H}
= \partial_{t=0} f(\exp_\Theta(tH))
\qquad \forall H \in \H.
\end{align*}
Similarly, the \emph{(Riemannian) Hessian}~$\nabla^2 f(\Theta)$ is the unique linear operator on~$\H$ such that
\begin{align*}
  \braket{H, \nabla^2 f(\Theta) K}
= \partial_{s=0} \partial_{t=0} f(\exp_{\Theta}(sH + tK))
\qquad \forall H,K \in \H.
\end{align*}
We abbreviate $\nabla f = \nabla f(I_D)$ and $\nabla^2 f = \nabla^2 f(I_D)$ for the gradient and Hessian, respectively, at the identity matrix, and we write $\nabla_a f$ and $\nabla^2_{ab}f$ for the components.
As block matrices,
\begin{align*}
  \nabla f = \left[\begin{array}{c} \nabla_0 f \\ \hline \nabla_1 f \\ \vdots \\ \nabla_k f \end{array}\right],
  \qquad
  \nabla^2 f = \left[\begin{array}{c|ccc}
  \nabla_{00}^2 f & \nabla_{01}^2 f \dots & \nabla_{0k}^2 f \\
    \hline\nabla_{10}^2 f & \nabla_{11}^2 f \dots & \nabla_{1k}^2 f \\
  \vdots  & \vdots & \ddots & \vdots \\
  \nabla_{k0}^2 f & \nabla_{k1}^2 f \dots & \nabla_{kk}^2 f \\
  \end{array}\right].
\end{align*}
Here, $\nabla_0 f \in \R$ and each $\nabla_a f(\Theta)$ is a $d_a \times d_a$ traceless symmetric matrix.
Similarly, for~$a, b \in [k]$ (i.e., for the blocks of the submatrix to the lower-right of the lines) the components $\nabla_{ab}^2f(\Theta)$ of the Hessian are linear operators from the space of traceless symmetric $d_b\times d_b$ matrices to the space of traceless symmetric $d_a \times d_a$ matrices, while $\nabla_{a0}f$ is a linear operator from $\R$ to the space of traceless symmetric $d_a\times d_a$ matrices (hence can itself be viewed as such a matrix), $\nabla_{0a}f$ is the adjoint of this linear operator, and $\nabla^2_{00} f(\Theta)$ is a real number.
\end{definition}

We note that the Hessian is symmetric with respect to the inner product~$\braket{\cdot,\cdot}$ on $\H$.
Just like in the Euclidean case, the Hessian is convenient to characterize strong convexity.
Indeed, $\braket{H, \nabla^2 f(\Theta) H} = \partial^2_{t=0} f(\exp_{\Theta}(tH))$ for all $H\in \H$.
Thus, $f$~is geodesically convex if and only if the Hessian is positive semidefinite, that is, $\nabla^2 f(\Theta) \succeq 0$. % where$\succeq$ is the Loewner order.
Similarly, $f$ is $\lambda$-strongly geodesically convex if and only if~$\nabla^2 f(\Theta) \succeq \lambda I_{\H}$, i.e., the Hessian is positive definite with eigenvalues larger than or equal to~$\lambda$.

We can now state and prove the following lemma, which shows that strong convexity in a ball about a point where the gradient is sufficiently small implies the optimizer cannot be far.

\begin{lemma}\label{lem:convex-ball}
Let $f\colon \P \to \R$ be geodesically convex and twice differentiable.
Assume the gradient at some~$\Theta\in\P$ is bounded as $\norm{\nabla f(\Theta)}_F \leq \delta$, and that $f$ is $\lambda$-strongly geodesically convex in a ball $B_r(\Theta)$ of radius~$r > \frac{2\delta}\lambda$.
Then the sublevel set $\{\Upsilon \in \P : f(\Upsilon) \leq f(\Theta)\}$ is contained in the ball~$B_{2\delta/\lambda}(\Theta)$, $f$ has a unique minimizer $\smash{\htheta}$, this minimizer is contained in the smaller ball~$B_{\delta/\lambda}(\Theta)$, and
\begin{align}\label{eq:minimum lower bound}
  f(\htheta) \geq f(\Theta) - \frac{\delta^2}{2 \lambda}.
\end{align}
\end{lemma}
\begin{proof}
We first show that the sublevel set of~$f(\Theta)$ is contained in the ball of radius~$\frac{2\delta}\lambda$.
Consider $g(t) := f(\exp_\Theta(tH))$, where~$H\in\H$ is an arbitrary vector of unit norm~$\norm H_F = 1$.
Then, using the assumption on the gradient,
\begin{align}\label{eq:grad bound}
  g'(0)
= \partial_{t=0} f(\exp_\Theta(tH))
= \braket{\nabla f(\Theta), H}
\geq -\norm{\nabla f(\Theta)}_F \norm H_F
\geq -\delta.
\end{align}
Since $f$ is $\lambda$-strongly geodesically convex on $B_r(\Theta)$, we have $g''(t) \geq \lambda$ for all $\abs t\leq r$.
It follows that for all $0 \leq t \leq  r$ we have
\begin{align}\label{eq:g convex lower}
  g(t) \geq g(0) - \delta t + \frac12 \lambda t^2.
\end{align}
Plugging in $t = r$ yields
$g(r) \geq  % g(0) - \delta  r + \frac12 \lambda r^2 =
g(0) + \left( \frac{\lambda r}2 - \delta \right)  r
> g(0)$.
Since $g$ is convex due to the geodesic convexity of $f$, it follows that, for any~$t \geq  r$,
\begin{align*}
  g(0) < g( r) \leq \left( 1-\frac{ r}t \right) g(0) + \frac{ r}t g(t),
\end{align*}
hence
\begin{align*}
  f(\Theta) = g(0) < g(t) = f(\exp_\Theta(tH)).
\end{align*}
Thus, since $H$ was an arbitrary unit norm tangent vector, the sublevel set of~$f(\Theta)$ is contained in the ball of radius~$r$ about~$\Theta$.
By replacing~$r$ with any smaller~$r'>\frac{2\delta}\lambda$, we see that the sublevel set is in fact contained in the closed ball of radius~$\frac{2\delta}\lambda$.
In particular, the minimum of $f$ is attained and any minimizer~$\smash{\htheta}$ is contained in this ball.
Moreover, as the right-hand side of \cref{eq:g convex lower} takes a minimum at $t=\frac\delta\lambda$, we have $g(t) \geq g(0) - \frac{\delta^2}{2\lambda}$ for all~$0\leq t\leq r$.
By definition of $g$, this implies \cref{eq:minimum lower bound}.


Next, we prove that any minimizer of~$f$ is necessarily contained in the ball of radius~$\frac\delta\lambda$.
To see this, take an arbitrary minimizer~$\htheta$ and write it in the form $\htheta = \exp_\Theta(TH)$, where~$H\in \H$ is a unit vector and~$T>0$.

%\AR{Is this assuming a minimizer exists? What if all finite points are only approx minimizers?\\ I think I have a strategy to get around this: let $\htheta$ be the minimizer over the $r$ ball (or any convex set containing $\Theta$). Then if we can show $\htheta$ is in the interior, then it is a local min, which by convexity implies it is global min. If $f$ is strongly convex on this set, then the argument below gives uniqueness. \\ Then for the same $g$ defined, we get the condition $g'(T) \leq 0$, as otherwise there would be a smaller value inside the ball. Everything else follows the same way. }

As before, we consider the function $g(t) = f(\exp_\Theta(tH))$.
Then, using \cref{eq:grad bound}, the convexity of~$g(t)$ for all $t\in\R$ and the $\lambda$-strong convexity of~$g(t)$ for~$\abs t \leq  r$, we have
\begin{align*}
  0 = g'(T) = g'(0) + \int_0^T g''(t) \, dt \geq \lambda \min(T,  r) - \delta.
\end{align*}
If $T> r$ then we have a contradiction as $\lambda r - \delta > \lambda r/2 - \delta > 0$.
Therefore we must have~$T\leq r$ and hence $\lambda T - \delta \leq 0$, so $T \leq \frac\delta\lambda$.
Thus we have proved that any minimizer of $f$ is contained in the ball of radius~$\frac\delta\lambda$.

We still need to show that the minimizer is unique; that this follows from strong convexity is convex optimization ``folklore,'' but we include a proof nonetheless.
Indeed, suppose that $\htheta$ is a minimizer and let $H\in \H$ be arbitrary.
Consider $h(t) := f(\exp_{\htheta}(tH))$.
Then the function $h(t)$ is convex, has a minimum at $t=0$, and satisfies $h''(0) > 0$, since $f$ is $\lambda$-strongly geodesically convex near~$\htheta$, as $\htheta \in B_r(\Theta)$ by what we showed above.
It follows that $h(t) > h(0)$ for any~$t\neq0$.
Since $H$ was arbitrary, this shows that $f(\Upsilon) > f(\htheta)$ for any $\Upsilon\neq \htheta$.
\end{proof}

Using the geodesic distance bounds from the previous lemma allows us to obtain bounds on the statistical distance measures $\DF$ and $\Dop$ of the overall precision matrix as well as of the individual Kronecker factors, assuming strong convexity.

\begin{lemma}[From geodesic distance to $\DF, \Dop$] \label{cor:g-convex-components}
Suppose the geodesic distance between $\htheta, \Theta \in \P$ satisfies $d(\htheta, \Theta) \leq \delta$ for $\delta \leq 1/ \sqrt{\dmax}$.
Writing $\htheta = \htheta_1 \ot \cdots \ot \htheta_k$ and $\Theta = \Theta_1 \ot \cdots \ot \Theta_k$ with $(\det\htheta_1)^{1/d_1} = \ldots = (\det\htheta_k)^{1/d_k}$ and $(\det\Theta_1)^{1/d_1} = \ldots = (\det\Theta_k)^{1/d_k}$, we have
\begin{align*}
  \Dop(\htheta_a \Vert \Theta_a)
\leq \DF(\htheta_a \Vert \Theta_a)
\leq 2 \sqrt{d_a} \cdot \delta
\end{align*}
and
\begin{align*}
  \Dop(\htheta \Vert \Theta)
\leq \DF(\htheta \Vert \Theta)
\leq 2 k \sqrt{D} \cdot \delta \, e^{2k\delta}.
\end{align*}
\end{lemma}
\begin{proof}
It suffices to prove the bounds for $\DF$.
By the invariance of $\DF$ and the geodesic distance, we may assume that~$\Theta = I_D$, i.e., $\Theta_a = I_a$ for all $a\in[k]$.
By our assumption on the geodesic distance, we may write $\htheta = \exp_{I_D}(H) $ for $H \in \H$ and $\|H\|_F \leq \delta$.
Then by our convention, the Kronecker factors are given by
\begin{align*}
  \htheta_a
= e^{\frac{H_0}{k}} \cdot e^{\sqrt{d_a} H_a}
= e^{\frac{H_0}{k} I_{d_a} + \sqrt{d_a} H_a}
\end{align*}
for all~$a\in[k]$, since $H_a$ is traceless.
Note that for each $a \in [k]$ we have
\begin{align*}
  \norm{\frac{H_0}{k} I_{d_a} + \sqrt{d_a} H_a}_F^2
= \frac{1}{k^2} \abs{H_0}^2 d_a + d_a \norm{H_a}_F^2
\leq d_a \norm{H}_F^2
\leq d_a \delta^2.
\end{align*}
By assumption, the above is at most one, so by \cref{f:expTaylor} we obtain
\begin{align*}
  \DF(\htheta_a \Vert I_a)
&= \norm{I_a - \hat\Theta_a}_F
= \norm{I_a - e^{\frac{H_0}{k} I_{d_a} + \sqrt{d_a} H_a}}_F \\
&\leq 2 \norm{\frac{H_0}{k} I_{d_a} + \sqrt{d_a} H_a}_F
\leq 2 \sqrt{d_a} \cdot \delta,
\end{align*}
which establishes the first bound.
The second now follows easily by a telescoping sum:
\begin{align*}
  \DF(\htheta\Vert I_D)
&= \norm{I_{d_1} \ot \cdots \ot I_{d_k} - \hat\Theta_1 \ot \cdots \ot \hat\Theta_k}_F \\
&\leq \sum_{a=1}^k \norm{\hat\Theta_1}_F \cdots \norm{\hat\Theta_{a-1}}_F \, \norm{I_{d_a} - \hat\Theta_a}_F \, \norm{I_{d_{a+1}}}_F \cdots \norm{I_{d_k}}_F \\
% &\leq \sum_{a=1}^k (\norm{I_{d_1}}_F + \norm{I_{d_1} - \hat\Theta_1}_F) \cdots (\norm{I_{d_{a-1}}}_F + \norm{I_{d_{a-1}} - \hat\Theta_{a-1}}_F) \, \norm{I_{d_a} - \hat\Theta_a}_F \, \norm{I_{d_{a+1}}}_F \cdots \norm{I_{d_k}}_F \\
% &\leq \sum_{a=1}^k (\sqrt{d_1} + 2 \sqrt{d_1} \cdot \delta) \cdots (\sqrt{d_{a-1}} + 2 \sqrt{d_{a-1}} \cdot \delta) 2 \sqrt{d_a} \cdot \delta \sqrt{d_{a+1}} \cdots \sqrt{d_k} \\
&\leq 2 \sqrt{D} \cdot \delta \sum_{a=1}^k (1 + 2 \delta)^{a-1}
\leq 2 k \sqrt{D} \cdot \delta \, e^{2k\delta}.
\end{align*}
\end{proof}

%-----------------------------------------------------------------------------
\subsection{Bounding the gradient}
%-----------------------------------------------------------------------------
Proceeding according to step~\ref{it:grad} of the plan outlined in \cref{subsec:proof-sketch}, we now compute the gradient of the objective function and bound it using matrix concentration results.

To calculate the gradient, we need a definition from linear algebra.
Recall that our data comes as an $n$-tuple $x=(x_1,\dots,x_n)$ of $k$-tensors. %, which we can also think of as an array of format~$d_1\times\dots\times d_k\times n$.
As in \cref{exa:usual-likelihood}, let $\rho := \frac1{nD}\sum_i x_i x_i^T$ denote the ``second sample moments'', and rewrite the objective function~\eqref{eq:neg log likelihood} as
\begin{align}\label{eq:obj via rho}
  f_x(\Theta) = \tr \rho \, \Theta - \frac1D \log \det \Theta.
\end{align}
We may also consider the ``second sample moments'' of a subset of the coordinates~$J \subseteq [k]$.
For this the following definition is useful.

\begin{definition}[Partial trace]\label{definition:partial-trace}
Let $\rho$ be an operator on $\R^{d_1} \ot \dots \ot \R^{d_k}$, and~$J \subseteq [k]$ an ordered subset.
Define the \emph{partial trace} $\rho^{(J)}$ as the $d_J \times d_J$-matrix, $d_J = \prod_{a\in J} d_a$, that satisfies the property that
\begin{align}\label{eq:partial trace duality}
  \tr \rho^{(J)} H
= \tr \rho \, H_{(J)}
\end{align}
for any $d_J\times d_J$ matrix~$H$, where $H_{(J)}$ denotes the operator on $\R^{d_1} \ot \cdots \ot \R^{d_k}$ that acts as~$H$ on the tensor factors labeled by~$J$ (in the order determined by~$J$) and as the identity on the rest.
This property uniquely determines $\rho^{(J)}$.
We write $\rho^{(a)}$ and $\rho^{(ab)}$ when $J=\{a\}$ and $J=\{a,b\}$, respectively.
\end{definition}

If $\rho$ is positive (semi)definite then so is $\rho^{(J)}$.
Moreover, $\tr \rho = \tr \rho^{(J)}$ and $(\rho^{(J)})^{(K)} = \rho^{(K)}$ for $K \subseteq J$.

Concretely, the partial trace $\rho^{(J)}$ can be calculated as follows:
Analogously to the discussion in \cref{subsec:model}, ``flatten'' the data~$x$ by regarding it as a $d_J \times N_J$~matrix~$x^{(J)}$, where $N_J = \frac{nD}{d_J}$;
then $\rho^{(J)} = \frac1{nD} x^{(J)} (x^{(J)})^T$.

The components of the gradient can be readily computed in terms of partial traces.

\begin{lemma}[Gradient]\label{lem:gradient}
Let $\rho = \frac{1}{nD} \sum_{i=1}^n \samp_i \samp_i^T $.
Then the components of the gradient~$\nabla f_x$ at the identity are given by
\begin{align}
 \nabla_a \ef_{\samp} &= \sqrt{d_a}\left( \rho^{(a)} - \frac{\tr\rho}{d_a} I_{d_a}\right)
  \qquad \text{ for } a \in [k], \label{eq:grad-a}\\
  \nabla_0 \ef_\samp &= \tr \rho - 1.\label{eq:grad-0}
\end{align}
\end{lemma}
\begin{proof}
For all $a\in[k]$ and any traceless symmetric $d_a\times d_a$ matrix~$H$, we have
\begin{align*}
\braket{\nabla_a f_x(I_D), H}
&= \partial_{t=0} f_x(e^{t\sqrt{d_a} H_{(a)}})
= \partial_{t=0} \left( \tr \rho \, e^{t\sqrt{d_a} H_{(a)}} - \frac1D\log\det(e^{t\sqrt{d_a} H_{(a)}}) \right) \\
&= \sqrt{d_a} \tr \rho \, H_{(a)}
= \sqrt{d_a} \tr \rho^{(a)} \, H
\end{align*}
using \cref{eq:obj via rho,eq:partial trace duality} and that $\tr H_{(a)} = 0$ since $\tr H = 0$.
Since $\nabla_a f_{\samp}$ is traceless and symmetric by definition, while $\rho^{(a)}$ is symmetric, this implies that $\nabla_f f_{\samp}$ is the orthogonal projection of $\rho^{(a)}$ onto the traceless matrices, i.e.,
\begin{align*}
  \nabla_a f_{\samp}
= \sqrt{d_a} \left( \rho^{(a)} - \frac{\tr \rho^{(a)}}{d_a} I_{d_a} \right)
= \sqrt{d_a} \left( \rho^{(a)} - \frac{\tr \rho}{d_a} I_{d_a} \right).
\end{align*}
Finally,
\[
  \nabla_0 f_x
= \partial_{t=0} \left( \tr \rho e^t - \frac1D \log \det(e^t I_D) \right)
% = \partial_{t=0} \left( \tr \rho e^t - \frac1D \log e^{tD} \right)
= \partial_{t=0} \left( \tr \rho e^t - t \right)
= \tr \rho - 1.
\]
\end{proof}

\begin{remark}[Gradient at other points]\label{remark:gradient-everywhere}
    In the previous lemma we only computed the gradient at the identity. However, this is without loss of generality, since from the calculations above one easily obtains $\nabla f_{x}(\Theta) = \nabla f_{\Theta^{1/2} x}(I)$. That is, the gradient $\nabla f_{x}(\Theta)$ is given by \cref{eq:grad-a,eq:grad-0} with $\rho$ replaced by $\Theta^{1/2}\rho\; \Theta^{1/2}.$
\end{remark}

Having calculated the gradient of the objective function, we are ready to state our bound:

\begin{prop}[Gradient bound]\label{prop:gradient-bound}
Let $\rv = (\rv_1,\dots,\rv_n)$ consist of independent standard Gaussian random variables in~$\R^D$. % where $D=d_1\cdots{}d_k$,
Suppose that $0<\eps<1$ and $n \geq \frac{\dmax^2}{D \eps^2}$.
Then, the following occurs with probability at least $1 - 2(k+1)e^{-\eps^2 \frac{nD}{8\dmax}}$:
\begin{align*}
  \norm{\nabla_a f_x}_{\op}
% &\leq 9\eps \frac{\sqrt{d_a}}{d_{max}}
&\leq \frac{9\eps}{\sqrt{d_a}}
\qquad\text{ for all $a\in[k]$}, \\
  \abs{\nabla_0 f_x} &\leq \eps.
\end{align*}
As a consequence, %$\norm{\nabla_a f_x}_{\op} \leq \frac{9\eps }{\sqrt{d_{max}}} \leq \frac{9\eps }{\sqrt{d_{a}}}$ and hence
\begin{align*}
  \norm{\nabla f_x}_F^2
% = \abs{\nabla_0 f_x}^2 + \sum_{a=1}^k \norm{\nabla_a f_x}_F^2
% \leq \abs{\nabla_0 f_x}^2 + \sum_{a=1}^k d_a \norm{\nabla_a f_x}_{\op}^2 \\
% \leq \eps^2 + \sum_{a=1}^k 81 \eps^2 =
&\leq (1 + 81 k) \eps^2
\leq 82 k \eps^2.
\end{align*}
\end{prop}

To prove this we will need a standard result in matrix concentration.
By the discussion below \cref{definition:partial-trace}, when the samples $x=(x_1,\dots,x_n)$ are independent standard Gaussians in $\R^D$, then $\rho^{(a)}$ is distributed as $\frac1{nD} Y Y^T$, where~$Y$ is a random $d_a \times N_a$ matrix with independent standard Gaussian entries, where~$N_a = \frac{nD}{d_a}$.
The following result bounds the singular values of such random matrices.

\begin{theorem}[Corollary 5.35 of \cite{vershynin2010introduction}]\label{cor:vershynin}
Let $Y \in \R^{d \times N}$ have independent standard Gaussian entries where $N\geq d$.
Then, for every $t > 0$, the following occurs with probability at least $1 - 2 e^{-t^2/2}$:
\begin{align*}
  \sqrt{N} - \sqrt{d} - t \leq \sigma_d(Y) \leq \sigma_1(Y) \leq \sqrt{N} + \sqrt{d} + t,
\end{align*}
where $\sigma_j$ denotes the $j$-th largest singular value.
\end{theorem}

We will also need to bound $\tr\rho = \frac1{nD} \norm x_2^2$.
Because $\norm x_2^2$ is simply a sum of $nD$ many $\chi^2$ random variables, the next proposition follows from standard concentration bounds.

\begin{prop}[Example~2.11 of \cite{W19}]\label{prp:xnorm}
Let $\rv = (\rv_1,\dots,\rv_n)$ consist of independent standard Gaussian random variables in~$\R^D$.
Then, for $0 < t < 1$, the following occurs with probability at least $1 - 2e^{-t^2 nD/8}$:
\begin{align*}
  (1 - t) nD \leq \norm{x}_2^2 \leq (1 + t) nD.
\end{align*}
\end{prop}

Equipped with these results we now prove our gradient bound.

\begin{proof}[Proof of \cref{prop:gradient-bound}]
For any fixed $a\in[k]$, recall that $\rho^{(a)}$ has the same distribution as~$\frac1{nD} YY^T$, where $Y$ is a $d_a\times N_a$-matrix with standard Gaussian entries where $N_{a} = \frac{nD}{d_{a}}$.
By \cref{cor:vershynin}, we have the following bound with failure probability at most~$2 e^{-t^2/2}$:
\begin{align*}
  \sqrt{N_a} - \sqrt{d_a} - t \leq \sigma_d(Y) \leq \sigma_1(Y) \leq \sqrt{N_a} + \sqrt{d_a} + t.
\end{align*}
This event tells us that the eigenvalues of $d_a \rho^{(a)}$ are in the range $( (1 - \frac{\sqrt{d_a} + t}{\sqrt{N_a}})^2, (1 + \frac{\sqrt{d_a} + t}{\sqrt{N_a}})^2)$.
% Let $t = \eps \sqrt{n D d_{a}/ \dmax^{2}}$.
Let $t = \eps \sqrt{n D / d_a} = \eps \sqrt{N_a}$.
Because $n \geq \dmax^{2}/D\eps^{2}$ and $0 < \eps \leq 1$, we have $\sqrt{d_{a}} \leq t \leq \sqrt{N_a}$.
Hence, the eigenvalues of $d_a \rho^{(a)}$ are contained in $( 1 - 4\frac{t}{\sqrt{N_a}}, 1 + 8 \frac{ t}{\sqrt{N_a}})$, and so the eigenvalues of $d_{a} \rho_{a} - I_{d_{a}}$ are bounded in absolute value by
% \[ \frac{8t}{\sqrt{N_{a}}} = 8 \eps \sqrt{ \frac{ nD d_{a}/ \dmax^{2}}{nD / d_{a}} } = 8\eps \frac{d_{a}}{\dmax}  \]
$8\eps$
% with failure probability at most $2e^{-\eps^{2} n D d_{a}/ 2 \dmax^{2}}$.
with failure probability at most~$2e^{-\eps^{2} n D / 2 d_{a}}$.
Moreover, by \cref{prp:xnorm}, we have that $\abs{\tr \rho - 1} \leq \eps$ with failure probability at most $2e^{-\eps^{2} n D / 8}$.
The formulae in \cref{lem:gradient} and the union bound imply
\begin{align*}
  \norm{\nabla_a f_x}_{\op}
&\leq\frac{1}{\sqrt{d_a}} \norm*{d_{a} \rho^{(a)} - I_{d_a}}_{\op} + \frac{\abs{\tr\rho - 1}}{\sqrt{d_a}}
\leq \frac{8\eps}{\sqrt{d_a}} + \frac{\eps}{\sqrt{d_a}}
\leq \frac{9\eps}{\sqrt{d_a}}
\end{align*}
for all $a\in[k]$, as well as
\begin{align*}
  \abs{\nabla_0 f_x}
= \abs{\tr \rho - 1}
\leq \eps,
\end{align*}
with failure probability at most $2 (k+1) e^{-\eps^{2} n D / 8 \dmax}$.
\end{proof}

%-----------------------------------------------------------------------------
\subsection{Strong convexity}\label{subsec:strong-convex}
%-----------------------------------------------------------------------------
In this section, we prove our strong convexity result, \cref{thm:ball-convexity}, in order to carry out step~\ref{it:convexity} of the plan from \cref{subsec:proof-sketch}.
The theorem states that, with high probability, $f_x$ is strongly convex near the identity.
We will prove it by first establishing strong convexity \emph{at} the identity, \cref{thm:tensor-convexity}, using quantum expansion techniques, and then giving a bound on how the Hessian changes away from the origin, \cref{convexRobustness}.
We then combine these results to prove \cref{thm:ball-convexity} at the end of this subsection.

Similarly as for the gradient, we can compute the components of the Hessian in terms of partial traces, but now we also need to consider two coordinates at a time.

% Recall that given data $x=(x_1,\dots,x_n)$, we defined $\rho = \frac1{nD} \sum_{i=1}^n x_i x_i^T$.
% For any $a \neq b \in [k]$, we may ``flatten'' $x$ by thinking of it as a $d_a d_b \times \frac{nD}{d_ad_b}$ matrix $x^{(ab)}$, and define $\rho^{(ab)} := x^{(ab)} (x^{(ab)})^T$.
% Then $\rho^{(ab)}$ is a positive semidefinite $d_ad_b \times d_ad_b$ matrix, and we have
% \begin{align}\label{eq:two body partial trace duality}
%   \tr \rho^{(ab)} (H \ot K) = \tr \rho \, H_{(a)} K_{(b)}
% \end{align}
% for any $d_a\times d_a$ matrix $H$ and any $d_b\times d_b$ matrix $K$, with $H_{(a)}$, $K_{(b)}$ defined as in \cref{eq:partial trace duality}.
% The matrices $\rho^{(ab)}$ are the \emph{partial trace} of $\rho$ over all but two of the coordinates.
% By comparing \cref{eq:partial trace duality,eq:two body partial trace duality}, we see that this definition is consistent with the definitions of~$\rho^{(a)}$,~$\rho^{(b)}$ in the sense that $\tr \rho^{(ab)} (H \ot I_{d_b}) = \tr \rho^{(a)} H$ and $\tr \rho^{(ab)} (I_{d_a} \ot K) = \tr \rho^{(b)} K$ for all~$H,K$.

\begin{lemma}[Hessian]\label{lem:hessian}
Let $\rho = \frac{1}{nD}\sum_{i=1}^n \samp_i \samp_i^T$.
Then the components of the Hessian~$\nabla^2 f_{\samp}$ at the identity are given by
\begin{align*}
  \braket{H, (\nabla^2_{aa} f_x) H} &= d_a \tr \rho^{(a)} H^2 \\
  \braket{H, (\nabla^2_{ab} f_x) K} &= \sqrt{d_a d_b} \tr \rho^{(ab)} \left( H \ot K \right)
\end{align*}
for all $a\neq b\in[k]$ and traceless symmetric $d_a\times d_a$ matrices $H$, $d_b\times d_b$ matrices~$K$, and
\begin{align*}
  \nabla^2_{0a} f_x&~\widehat= \sqrt{d_a} \left( \rho^{(a)} - \frac{\tr \rho}{d_a} I_{d_a} \right) \widehat=~ \nabla^2_{a0} f_x, \\
  \nabla^2_{00} f_x&= \tr \rho.
\end{align*}
for all $a \in [k]$.
\end{lemma}

\noindent
Here we use the conventions discussed in \cref{def:hess grad}.
In particular, we identify $\nabla^2_{a0} f_x$, which is a linear operator from the real numbers to the traceless symmetric matrices, with a traceless symmetric matrix, and similarly for its adjoint $\nabla^2_{0a} f_x$.
The notation $\widehat=$ reminds us of these identifications.

\begin{proof}
  Note that the Hessian of~$f_x$ coincides with the one of $\tr\rho\,\Theta$.
  This follows from \cref{eq:obj via rho}, since the Hessian of $\log\det\Theta$ vanishes identically.
  % Indeed, for any $H\in\H$,
  % \begin{align*}
  %   \log\det(\exp_\Theta(tH))
  % = \log\det(e^{tH_0} \Theta)
  % = \log \det(\Theta) + t H_0 D,
  % \end{align*}
  % which is an affine-linear function in $t\in\R$.
  Accordingly, we will compute the Hessian of~$\tr\rho\,\Theta$.
  For $a\in[k]$ and any traceless symmetric $d_a\times d_a$ matrix $H$, we have
  \begin{align*}
    \braket{H, (\nabla^2_{aa} f_x) H}
  = \partial_{s=0} \partial_{t=0} \tr \rho \, e^{(s+t) \sqrt{d_a} H_{(a)}}
  = d_a \tr \rho H_{(a)}^2
  = d_a \tr \rho^{(a)} H^2
  \end{align*}
  using \cref{eq:partial trace duality}.
  Similarly, for $a\neq b\in[k]$, any traceless symmetric $d_a\times d_a$ matrix $H$, and any traceless symmetric $d_b\times d_b$ matrix $K$, we find that
  \begin{align*}
    \braket{H, (\nabla^2_{ab} f_x) K}
  &= \partial_{s=0} \partial_{t=0} \tr \rho \, e^{s \sqrt{d_a} H_{(a)} + t \sqrt{d_b} K_{(b)}} \\
  &= \sqrt{d_a d_b} \tr \rho \, H_{(a)} K_{(b)}
  = \sqrt{d_a d_b} \tr \rho^{(ab)} \left( H \ot K \right)
  \end{align*}
  using \cref{eq:partial trace duality}.
  Next, for $a\in[k]$ and any traceless symmetric $d_a\times d_a$ matrix $H$, we have
  \begin{align*}
    \braket{H, \nabla^2_{a0} f_x}
  = \partial_{s=0} \partial_{t=0} \tr \rho \, e^{s\sqrt{d_a} H_{(a)} + t}
  = \sqrt{d_a} \tr \rho \, H_{(a)}
  = \sqrt{d_a} \tr \rho^{(a)} H.
  \end{align*}
  As we identify $\nabla^2_{a0} f_x$ with a traceless symmetric $d_a\times d_a$ matrix;
  this shows that
  \begin{align*}
    \nabla^2_{a0} f_x = \sqrt{d_a} \left( \rho^{(a)} - \frac{\tr \rho}{d_a} I_{d_a} \right),
  \end{align*}
  and similarly for the transpose.
  Finally,
  \[
    \nabla^2_{00} f_x
  = \partial_{s=0} \partial_{t=0} \tr \rho \, e^{s+t}
  = \tr \rho.
  \]
\end{proof}

\begin{remark}[Hessian at other points]\label{remark:hessian-everywhere}
Analogously to \cref{remark:gradient-everywhere}, we can compute the Hessian at other points using $\nabla^2 f_{x}(\Theta) = \nabla^2 f_{\Theta^{1/2} x}$. That is, the Hessian $\nabla^2 f_{x}(\Theta)$ is given by \cref{lem:hessian} with $\rho$ replaced by $\Theta^{1/2}\rho\; \Theta^{1/2}.$
\end{remark}

The most interesting part of the Hessian are the off-diagonal components for $a\neq b\in[k]$, which up to an overall factor $\sqrt{d_a d_b}$ can be seen as the restrictions of the linear maps
\begin{align}\label{eq:hessian channel}
  \Phi^{(ab)} \colon \Mat(d_b) \to \Mat(d_a), \quad \braket{H, \Phi^{(ab)}(K)} &= \tr \rho^{(ab)} \left( H \ot K \right)
\end{align}
to the traceless symmetric matrices.
\Cref{eq:hessian channel} is a special case of a \emph{completely positive map}, which is a linear map of the form
\begin{align}\label{eq:def cp}
  \Phi_A \colon \Mat(d_b) \to \Mat(d_a), \quad \Phi_A(X) = \sum_{i=1}^N A_i X A_i^T
\end{align}
for $d_a\times d_b$ matrices $A_1,\dots,A_N$.
To see the connection, note that since $\rho^{(ab)}$ is positive semidefinite, it can be written in the form $\sum_{i=1}^N \vect(A_i) \vect(A_i)^T$; then $\Phi^{(ab)} = \Phi_A$ follows.
The matrices $A_1,\dots,A_N$ are known as \emph{Kraus operators}.
\Cref{eq:def cp} can also be written as
\begin{align}\label{eq:vec rep}
  \vect(\Phi_A(X)) = \sum_{i=1}^N (A_i \ot A_i) \vect(X).
\end{align}
We denote by $\Phi^*\colon\Mat(d_a)\to\Mat(d_b)$ the adjoint of a completely positive map~$\Phi$ with respect to the Hilbert-Schmidt inner product; this is again a completely positive map, with Kraus operators $A_1^T,\dots,A_N^T$.
% \begin{align*}
%   \braket{H, \Phi^{(ab)}(K)}
% = \tr \rho^{(ab)} \left( H \ot K \right)
% = \sum_{i=1}^N \vect(A_i)^T \left( H \ot K \right) \vect(A_i) \\
% = \sum_{i=1}^N \sum_{a,a',b,b'} \braket{a|A_i|b} \braket{a,b|H \ot K|a',b'} \braket{a'|A_i|b'} \\
% = \sum_{i=1}^N \sum_{a,a',b,b'} \braket{a|A_i|b} \braket{a|H|a'} \braket{b|K|b'} \braket{a'|A_i|b'} \\
% = \sum_{i=1}^N \sum_{a,a',b,b'} \braket{a'|H^T|a} \braket{a|A_i|b} \braket{b|K|b'} \braket{b'|A_i^T|a'} \\
% = \sum_{i=1}^N \tr H^T A_i K A_i^T
% = \braket{H, \sum_{i=1}^N A_i K A_i^T}
% \end{align*}
% and
% \begin{align*}
%   \vect(A_i K A_i^T)
% = \sum_{a,a'} \ket{a,a'} \braket{a|A_i K A_i^T|a'}
% = \sum_{a,a',b,b'} \ket{a,a'} \braket{a|A_i|b} \braket{b|K|b'} \braket{b'|A_i^T|a'} \\
% = \sum_{a,a',b,b'} \ket{a,a'} \braket{a|A_i|b} \braket{a'|A_i|b'} \braket{b|K|b'} \\
% = \sum_{a,a',b,b'} (A_i \ot A_i) \vect(K)
% \end{align*}
%
In our proof of strong convexity, we will show that strong convexity follows if the completely positive maps $\Phi^{(ab)}$ are good \emph{quantum expanders}.

\begin{definition}[Quantum expansion]\label{def:expansion}
Let $\Phi\colon\Mat(d_b) \to \Mat(d_a)$ be a completely positive map.
Say $\Phi$ is \emph{$\eps$-doubly balanced} if
\begin{align}\label{eq:doubly balanced}
  \norm*{\frac{\Phi(I_{d_b})}{\tr \Phi(I_{d_b})} - \frac{I_{d_a}}{d_a}}_{\op} \leq \frac\eps{d_a}
\quad\text{and}\quad
  \norm*{\frac{\Phi^*(I_{d_a})}{\tr \Phi^*(I_{d_a})} - \frac{I_{d_b}}{d_b}}_{\op} \leq \frac\eps{d_b}.
\end{align}
The map $\Phi$ is an \emph{$(\eps, \lambda)$-quantum expander} if $\Phi$ is $\eps$-doubly balanced and
\begin{align}\label{eq:expansion}
  \norm{\Phi}_0 := \max_{\substack{H \text{ traceless symmetric} \\ \norm H_F=1}} \norm{\Phi(H)}_F
\leq \lambda \frac{\tr \Phi(I_{d_b})}{\sqrt{d_ad_b}}
\end{align}
A $(0, \lambda)$-quantum expander is called a \emph{$\lambda$-quantum expander}.
\end{definition}

\noindent
Quantum expanders originate in quantum information theory and quantum computation~\cite{H07}.
There one typically takes $d_a=d_b$ and $\eps=0$, so that \cref{eq:expansion} simplifies to~$\norm{\Phi}_0 \leq \lambda$.
\Cref{def:expansion} is invariant under rescaling $\Phi \mapsto c\Phi$ for $c>0$.
Here we follow the definitions of~\cite{KLR19,FM20}, who recognized the connection between the quantum expansion and spectral gaps of the Hessian for operator scaling (but we note that some of the following can be slightly simplified if one opts for a non-scale invariant definition).
For us, the following lemma will allow us to translate quantum expansion properties into strong convexity.

\begin{lemma}[Strong convexity from expansion]\label{lem:expansion-convexity}
If the completely positive maps $\Phi^{(ab)}$ defined in \cref{eq:hessian channel} are $(\eps,\lambda)$-quantum expanders for every $a\neq b\in[k]$, then
\begin{align*}
  \norm*{\frac{\nabla^2 f_x}{\tr \rho} - I_\H}_{\op}
\leq (k-1)\lambda + (\sqrt k + 1) \eps.
\end{align*}
Assuming $k\geq2$, the right-hand side is at most $k (\lambda + \eps)$.
\end{lemma}
\noindent
It suffices to verify the hypothesis for $a<b$.
Indeed, since $\tr \Phi^*(I_{d_a}) = \tr \Phi(I_{d_b})$, any $\Phi$ is an $(\eps,\lambda)$-quantum expander if and only if this is the case for the adjoint $\Phi^*$, but note that the adjoint of~$\Phi^{(ab)}$ is~$\Phi^{(ba)}$.
To prepare the proof, we also note that
\begin{align}\label{eq:channel to single marginals}
   \Phi^{(ab)}(I_{d_b}) = \rho^{(a)}
\quad\text{and}\quad
   (\Phi^{(ab)})^*(I_{d_a}) = \Phi^{(ba)}(I_{d_a}) = \rho^{(b)},
\end{align}
hence in particular $\tr \Phi^{(ab)}(I_{d_b}) = \tr \rho$.

\begin{proof}
We wish to bound the operator norm of $M = \frac{\nabla^2 f_\samp}{\tr \rho} - I_\H$, which we consider as a block matrix as in \cref{def:hess grad}.
For this, we use the following basic estimate of the norm of a block matrix in terms of the norm of the matrix of block norms:
\begin{align}\label{eq:baby norm bounds}
  \norm{M}_{\op} \leq \norm{m}_{\op},
\quad \text{ where } m=(\norm{M_{ab}}_{\op})_{a,b\in\{0,1,\dots,k\}}.
\end{align}
We first bound the individual block norms, using that the blocks can be computed using \cref{lem:hessian}.
Recall that the off-diagonal blocks of the Hessian, $\nabla^2_{ab} f_x$ for $a \neq b\in[k]$, are given by the restriction of $\sqrt{d_a d_b} \Phi^{(ab)}$ to the traceless symmetric matrices.
Since $\Phi^{(ab)}$ is an $(\eps,\lambda)$-quantum expander, we have
\begin{align*}
  \norm{M_{ab}}_{\op}
= \frac{\norm{\nabla^2_{ab} f_x}_{\op}}{\tr\rho}
= \frac{\sqrt{d_a d_b}}{\tr \Phi^{(ab)}(I_{d_b})} \norm{\Phi^{(ab)}}_0
\leq \lambda,
\end{align*}
using that $\tr \Phi^{(ab)}(I_{d_b}) = \tr \rho$.
The remaining off-diagonal blocks can be bounded as
\begin{align*}
\norm{M_{a0}}
= \frac{\norm{\nabla^2_{a0} f_x}_{\op}}{\tr \rho}
&= \norm*{\sqrt{d_a} \left( \frac{\rho^{(a)}}{\tr \rho} - \frac{I_{d_a}}{d_a} \right)}_F
= \sqrt{d_a} \norm*{\frac{\Phi^{(ab)}(I_{d_b})}{\tr \Phi^{(ab)}(I_{d_b})} - \frac{I_{d_a}}{d_a}}_F \\
&\leq d_a \norm*{\frac{\Phi^{(ab)}(I_{d_b})}{\tr \Phi^{(ab)}(I_{d_b})} - \frac{I_{d_a}}{d_a}}_{\op}
\leq \eps,
\end{align*}
using the fact that the operator norm of a linear functional $\braket{K, -}$ is the same as the Frobenius norm of~$K$, and \cref{eq:channel to single marginals}.
On the other hand, the diagonal blocks for $a\in[k]$ can be bounded by observing that, for any traceless Hermitian $H$,
\begin{align*}
  \abs{\braket{H, M_{aa} H}}
&= \abs*{\braket{H, \left( \frac{\nabla^2_{aa} f_x}{\tr \rho} - I \right) H}}
= d_a \abs*{\tr \left( \frac{\rho^{(a)}}{\tr \rho} - \frac{I_{d_a}}{d_a} \right) H^2} \\
&\leq d_a \norm*{\frac{\rho^{(a)}}{\tr \rho} - \frac{I_{d_a}}{d_a}}_{\op} \norm{H}_F^2
\leq \eps \norm H_F^2,
\end{align*}
hence $\norm{M_{aa}}_{\op} \leq \eps$, while $\abs{M_{00}} = \abs{\frac{\nabla^2_{00} f_x}{\tr \rho} - 1} = 0$.
% Since $m$ is a symmetric matrix, it follows that
% \begin{align*}
%   \norm m_{\op} \leq \max_i \sum_j m_{ij} = \max \{ k \eps, \eps + (k-1) \lambda \}
% \end{align*}
To conclude the proof, decompose
\begin{align*}
  m
% = \left[\begin{array}{c|cccc}
%   0 & m_{01} & m_{02} & \cdots & m_{0k} \\
%   \hline
%   m_{10} & m_{11} & m_{12} & \cdots & m_{1k} \\
%   m_{20} & m_{21} & m_{22} & & m_{2k} \\
%   \vdots & \vdots & & \ddots & \hdots \\
%   m_{k0} & m_{k1} & m_{k2} & \cdots & m_{kk}
%   \end{array}\right]
= \left[\begin{array}{c|cccc}
  0 & 0 & 0 & \cdots & 0 \\
  \hline
  0 & 0 & m_{12} & \cdots & m_{1k} \\
  0 & m_{21} & 0 & & m_{2k} \\
  \vdots & \vdots & & \ddots & \vdots \\
  0 & m_{k1} & m_{k2} & \cdots & 0
  \end{array}\right]
+ \left[\begin{array}{c|cccc}
  0 & 0 & 0 & \cdots & 0 \\
  \hline
  0 & m_{11} & 0 & \cdots & 0 \\
  0 & 0 & m_{22} & & 0 \\
  \vdots & \vdots & & \ddots & \vdots \\
  0 & 0 & 0 & \cdots & m_{kk}
  \end{array}\right]
+ \left[\begin{array}{c|cccc}
  0 & m_{01} & m_{02} & \cdots & m_{0k} \\
  \hline
  m_{10} & 0 & 0 & \cdots & 0 \\
  m_{20} & 0 & 0 & & 0 \\
  \vdots & \vdots & & \ddots & \vdots \\
  m_{k0} & 0 & 0 & \cdots & 0
  \end{array}\right].
\end{align*}
The nonzero entries of the first matrix are bounded by $\lambda$, hence its operator norm is at most $(k-1)\lambda$.
The second matrix is diagonal with diagonal entries bounded by $\eps$, hence its operator norm is at most~$\eps$.
The third matrix has nonzero entries bounded by $\eps$, hence its operator norm is bounded by~$\sqrt k \eps$.
Using \cref{eq:baby norm bounds} we obtain the desired bound.
% \begin{align*}
%   \norm M_{\op} \leq \norm m_{\op} \leq (k-1)\lambda + (1 + \sqrt k) \eps.
% \end{align*}
\end{proof}

% We note that a slightly more complicated proof yields the improved estimate $\leq (k-1) \lambda + (\sqrt k + 1) \eps$.

% The proof of \cref{lem:expansion-convexity} uses a basic lemma that gives upper and lower bounds on a block matrix in terms of block diagonal matrices and the Loewner order, and a resulting norm bound.

% \begin{lemma}\label{lem:block-matrix}
% Let $M$ be a symmetric block matrix with blocks $M_{ij}$ of size~$d_i \times d_j$, where~$i,j\in[N]$.
% Then,
% \begin{align*}
%   \bigoplus_{i=1}^N \left(M_{ii} - I_{d_i} \cdot \sum_{j \neq i} \norm{M_{ij}}_{\op} \right)
% \preceq M
% \preceq \bigoplus_{i=1}^N \left(M_{ii} + I_{d_i} \cdot \sum_{j \neq i} \norm{M_{ij}}_{\op} \right)
% \end{align*}
% and hence \MW{Isn't the following all we need?}
% \begin{align*}
%   \norm{M}_{\op} \leq \max_i \sum_j \norm{M_{ij}}_{\op}.
% \end{align*}
% \end{lemma}
% \begin{proof}
% We use the inequality for block matrices
% \begin{align}\label{eq:psd-bound}
%   -\begin{bmatrix} A & 0 \\ 0 & B \end{bmatrix}
% \preceq \begin{bmatrix} 0 & K \\ K^{*} & 0 \end{bmatrix}
% \preceq \begin{bmatrix} A & 0 \\ 0 & B \end{bmatrix}
% \end{align}
% for any $A,B \succ 0$ such that $\norm{A^{-1/2} K B^{-1/2}}_{\op} \leq 1$, which may be proved by computing Schur complements.
% We first apply the upper bound in \cref{eq:psd-bound} to the upper-left two-by-two block matrix.
% Taking $A = I_{d_1} \cdot \norm{M_{12}}_{\op}$ and $B = I_{d_2} \cdot \norm{M_{12}}_{\op}$,
% we find that
% \begin{align*}
%   M \preceq \begin{bmatrix}
%   M_{11} + I_{d_1} \cdot \norm{M_{12}}_{\op} & 0 & \quad M_{13} & \cdots & M_{1N} \\
%   0 & M_{22} + I_{d_2} \cdot \norm{M_{12}}_{\op} & \quad M_{23} & \cdots & M_{2N} \\
%   M_{31} & M_{32} & \quad M_{33} & \cdots & M_{3N} \\
%   \vdots & \vdots & \vdots & \;\;\;\;\ddots & \vdots \\
%   M_{N1} & M_{N2} & \quad\; M_{N3} & \cdots & M_{NN}
%   \end{bmatrix}.
% \end{align*}
% By sequentially apply this inequality to all other $2\times 2$ principal block submatrices, we obtain the desired upper bound.
% The lower bound is proved completely analogously.
% \end{proof}
% \begin{proof}[Proof of \cref{lem:expansion-convexity}]
% We apply \cref{lem:block-matrix} with $M = \frac{\nabla^2 f_\samp}{\tr \rho} - I_\H$, which we consider as a block matrix as in \cref{def:hess grad}.
% Recall from \cref{lem:hessian} that the off-diagonal blocks of the Hessian for $a \neq b\in[k]$ are given by $\nabla^2_{ab} f_x = \sqrt{d_a d_b} \Phi^{(ab)}$.
% Thus, since $\Phi^{(ab)}$ is an $(\eps,\lambda)$-quantum expander, we have
% \begin{align*}
%   \norm{M_{ab}}_{\op}
% = \frac{\norm{\nabla^2_{ab} f_x}_{\op}}{\tr\rho}
% = \frac{\sqrt{d_a d_b}}{\tr \Phi^{(ab)}(I_{d_b})} \norm{\Phi^{(ab)}}_0
% \leq \lambda,
% \end{align*}
% using that $\tr \Phi^{(ab)}(I_{d_b}) = \tr \rho$.
% The remaining off-diagonal elements can be bounded as
% \begin{align*}
% \norm{M_{a0}}
% = \frac{\norm{\nabla^2_{a0} f_x}_{\op}}{\tr \rho}
% &= \norm*{\sqrt{d_a} \left( \frac{\rho^{(a)}}{\tr \rho} - \frac{I_{d_a}}{d_a} \right)}_F
% = \sqrt{d_a} \norm*{\frac{\Phi^{(ab)}(I_{d_b})}{\tr \Phi^{(ab)}(I_{d_b})} - \frac{I_{d_a}}{d_a}}_F \\
% &\leq d_a \norm*{\frac{\Phi^{(ab)}(I_{d_b})}{\tr \Phi^{(ab)}(I_{d_b})} - \frac{I_{d_a}}{d_a}}_{\op}
% \leq \eps,
% \end{align*}
% using that $\Phi^{(ab)}(I_{d_b}) = \rho^{(a)}$.
% On the other hand, the diagonal blocks for $a\in[k]$ can be bounded by observing that, for any traceless Hermitian $H$,
% \begin{align*}
%   \abs{\braket{H, M_{aa} H}}
% &= \abs*{\braket{H, \left( \frac{\nabla^2_{aa} f_x}{\tr \rho} - I_\H \right) H}}
% = d_a \abs*{\tr \left( \frac{\rho^{(a)}}{\tr \rho} - \frac{I_{d_a}}{d_a} \right) H^2} \\
% &\leq d_a \norm*{\frac{\rho^{(a)}}{\tr \rho} - \frac{I_{d_a}}{d_a}}_{\op} \norm{H}_F^2
% \leq \eps \norm H_F^2,
% \end{align*}
% hence $\norm{M_{aa}}_{\op} \leq \eps$, while $\abs{M_{00}} = \abs{\frac{\nabla^2_{00} f_x}{\tr \rho} - 1} = 0$.
% Applying the bound from \cref{lem:block-matrix}, we obtain
% \begin{align*}
%   \norm{M}_{\op} = \max \{ k \eps, \eps + (k-1) \lambda \} \leq k(\eps+\lambda).
% \end{align*}
% % We next bound $\nabla_{aa}^2 f_\samp$. Recall again from \cref{lem:hessian} that the diagonal blocks of the Hessian are given by \begin{align}
% %  \langle Y,  \left( \nabla^2_{aa} f_{\samp} \right) Y \rangle=  d_a \tr \rho^{(a)} Y^2 .\label{eq:on-diag-hess-ii}
% % \end{align}
% % Again, the $\eps$-doubly balancedness of $\Phi^{ab}$ implies $ |\tr (d_a \rho^{(a)}  - (\tr \rho) I_a)Y^2| \leq \eps (\tr \rho) \tr Y^2$, or that the first term of \cref{eq:on-diag-hess-ii} is in $(1 \pm \eps) \|Y\|_F^2 (\tr \rho)$. Hence $\|\nabla_{aa}^2 f_\samp - (\tr \rho) I\|_{\op} \leq \eps^2 + \eps$. Finally, $\nabla_{00}^2 f_\samp$ is exactly $\tr \rho$. Using \cref{lem:block-matrix}, we find that the submatrix of $\nabla^2 f$ excluding the $0$ row and column tells us that dominates the block diagonal matrix where the $aa$ block is at least $(1 - \eps) (\tr \rho) I - (k-1) \lambda (\tr \rho) I $ for $a \in [k]$. Furthermore, the 00 block is exactly $ \tr \rho$. The analogous upper bound on $\nabla^2 f_\samp$ follows similarly. We now wish to apply \cref{lem:block-matrix} one more time, this time with only two diagonal blocks, one of which is the 00 block. It remains to bound the operator norm of the off diagonal block of this matrix, which we call $M_{\text{off}}$, consisting of $\nabla_{a 0}^2 f$ vertically concatenated together for $a \in [k]$.

% % First we bound $\|\nabla_{a0} f\|_{\op}$. This is simply $\sqrt{d_a}\sup_{\|Z\|_F = 1} |\tr Z \rho^{(a)}|$ where $Z$ ranges over traceless Hermitians. Using that $\rho^{(a)} = \Phi^{ab}(I_{d_b})$, we have $\|d_a \rho^{(a)} - (\tr \rho) I_{d_a}\|_{\op} \leq \eps \tr \rho $ by quantum expansion. For traceless $Z$ we have
% % \begin{align}\tr Z \rho^{(a)} = \frac{1}{d_a} \tr Z (d_a \rho^{(a)} - (\tr \rho) I_{d_a}) \leq \frac{\tr \rho}{d_a} \eps \|Z\|_1 \leq \frac{\tr \rho}{\sqrt{d_a}} \eps \|Z\|_F,\label{eq:second-term-hess}\end{align}
% % thus $\|\nabla_{a0} f\|_{\op} \leq \eps \tr \rho$. It follows that $\|M_{\text{off}}\|_{\op} \leq \sqrt{k} \eps \tr \rho$. Applying \cref{lem:block-matrix} to these two blocks shows $\nabla^2 f_\samp$ dominates a block diagonal matrix where each block is dominates $(1 - \eps) (\tr \rho) I - (k-1) \lambda (\tr \rho) I  - \sqrt{k} \eps (\tr \rho) I.$
%  \end{proof}

We are concerned with $\Phi^{(ab)}$ that arise from random Gaussians.
Just like random graphs can give rise to good expanders, it is known that random completely positive maps, namely~$\Phi$ constructed by choosing Kraus operators at random from well-behaved distributions, yield good quantum expanders.
When the Kraus operators are chosen to be standard Gaussian we have the following result:

\begin{theorem}[\cite{pisier2012grothendieck,P14}]\label{thm:hess-pisier}
Let $A_1,\dots,A_N$ be independent $d_a\times d_b$ random matrices with independent standard Gaussian entries.
Then, for every $t \geq 2$, with probability at least~$1 - t^{-\Omega(d_a + d_b)}$, the completely positive map $\Phi_A$, defined as in \cref{eq:def cp}, satisfies
\begin{align*}
  \norm{\Phi_A}_0 \leq O\left(t^{2} \sqrt N \left( d_a + d_b \right)\right).
\end{align*}
\end{theorem}

Pisier's actual result is slightly different.
% As stated, \cref{thm:hess-pisier} is an easy consequence of Theorem~16.6 in~\cite{pisier2012grothendieck}, together with a standard symmetrization trick (see, e.g., the proof of Lemma~4.1 in~\cite{P14}).
We present the details in \cref{sec:pisier}.

When the samples $x=(x_1,\dots,x_n)$ are independent standard Gaussians in $\R^D$,
% then~$\rho^{(ab)}$ is distributed as $\frac1{nD} ZZ^T$, where $Z$ is a random $d_a d_b \times \frac{nD}{d_ad_b}$ matrix with independent standard Gaussian entries, as follows from the discussion above \cref{eq:two body partial trace duality}.
% Therefore,
the random completely positive maps $\Phi^{(ab)}$ we are interested in have the same distribution as~$\frac1{nD}\Phi_A$, where the Kraus operators~$A_1,\dots,A_N$ are $d_a \times d_b$ matrices with independent standard Gaussian entries and~$N=\frac{nD}{d_ad_b}$.
Accordingly, strong convexity at the identity follows quite easily from \cref{thm:hess-pisier} once double balancedness can be controlled.
For the latter, observe that
\begin{align*}
  \norm*{\frac{\Phi^{(ab)}(I_{d_b})}{\tr \Phi^{(ab)}(I_{d_b})} - \frac{I_{d_a}}{d_a}}_{\op}
= \frac1{\tr\rho} \norm*{\rho^{(a)} - \frac{\tr \rho}{d_a} I_{d_a}}_{\op}
% = \frac1{\tr\rho} \frac1{\sqrt{d_a}} \sqrt{d_a} \norm*{\rho^{(a)} - \frac{\tr \rho}{d_a} I_{d_a}}_{\op}
% = \frac1{\tr\rho} \frac1{\sqrt{d_a}} \norm*{\nabla_a f_x}_{\op}
= \frac1{1 + \nabla_0 f_x} \frac1{\sqrt{d_a}} \norm*{\nabla_a f_x}_{\op},
\end{align*}
by \cref{lem:gradient}, and similarly for the adjoint.
Therefore, the completely positive maps $\Phi^{(ab)}$ are $\eps$-doubly balanced if and only if, for all $a\in[k]$,
\begin{align}\label{eq:balanced via grad}
  % \frac1{1 + \nabla_0 f_x} \frac1{\sqrt{d_a}} \norm*{\nabla_a f_x}_{\op} \leq \frac\eps{d_a}
  \sqrt{d_a} \norm*{\nabla_a f_x}_{\op} \leq \eps \tr \rho = \left( 1 + \nabla_0 f_x \right) \eps,
\end{align}
hence double balancedness can be controlled using the gradient bounds in \cref{prop:gradient-bound}.

We now state and prove our strong convexity result at the identity:

\begin{prop}[Strong convexity at identity]\label{thm:tensor-convexity}
There is a universal constant $C>0$ such that the following holds.
Let $x = (x_1,\dots,x_n)$ be independent standard Gaussian random variables in~$\R^D$, where $n \geq C k \frac{\dmax^2}D$.
Then, with probability at least~$1 - k^2 ( \frac{\sqrt{nD}}{k \dmax} )^{-\Omega(\dmin)}$,
\begin{align*}
  \norm{\nabla^2 f_x - I_\H}_{\op} \leq \frac14;
\end{align*}
in particular, $f_x$ is $\frac34$-strongly convex at the identity.
\end{prop}
\begin{proof}
By \cref{lem:expansion-convexity}, it is enough to prove that with the desired probability all $\Phi^{(ab)}$ are $(\eps,\lambda):=(\frac1{40 k^{1/2}},\frac1{20k})$-quantum expanders for $a\neq b\in[k]$ and $\tr \rho \in (\frac78,\frac98)$.
If that is the case, then
\begin{align*}
  \norm*{\nabla^2 f_x - I_\H}_{\op}
&\leq \tr \rho \cdot  \norm*{\frac{\nabla^2 f_x}{\tr \rho} - I_\H}_{\op}  + \abs{1 - \tr\rho} \\
&\leq \left( (k-1)\lambda + (\sqrt k + 1) \eps \right) \tr\rho + \abs{1 - \tr\rho}
% &\leq \left( \frac1{20} + \frac1{20} \right) \frac98 + \frac18
% = \frac1{10} \frac98 + \frac18
\leq \frac14.
\end{align*}
Firstly, $\tr \rho = \frac{1}{nD} \|X\|^2$ is in $(\frac78, \frac98)$ with failure probability $e^{-\Omega(nD)}$ by \cref{prp:xnorm}.

Next, we describe an event that implies the $\Phi^{(ab)}$ are all $\eps$-doubly balanced for $\eps=\frac1{40k^{1/2}}$.
By \cref{eq:balanced via grad}, this is equivalent to the condition $\sqrt{d_a} \norm{\nabla_a f_{\rv}}_{\op} \leq \eps \tr \rho$ for all $a \in [k]$.
By \cref{prop:gradient-bound}, and assuming the bound $\tr \rho \geq \frac78$ from above, the latter occurs with failure probability~$k \smash{e^{-\Omega(\frac{nD}{k \dmax})}}$ provided $n \geq C k \smash{\frac{\dmax^2}D}$ for a universal constant~$C>0$.

Finally, we describe an event that ensures that $\norm{\Phi^{(ab)}}_0 \leq \lambda \smash{\frac{\tr\rho}{\sqrt{d_a d_b}}}$ for $\lambda=\frac1{20k}$ for any fixed~$a \neq b$, which is the other condition needed for quantum expansion.
Recall that each~$\Phi^{(ab)}$ is distributed as $\frac1{nD} \Phi_A$, where $A$ is a tuple of $\frac{nD}{d_ad_b}$ many $d_a \times d_b$ matrices with independent standard Gaussian entries.
Thus, taking $t^2 = O(\smash{\frac{\lambda \sqrt{nD}}{d_a + d_b}})$ and again assuming that $\tr\rho \geq \frac78$, we have $\norm{\Phi^{(ab)}}_0 \leq \lambda \frac{\tr\rho}{\sqrt{d_a d_b}}$ by \cref{thm:hess-pisier},
% Take t^2 = \frac1{X} \frac78 \frac{\lambda \sqrt{nD}}{d_a + d_b}, with $X$ the universal constant from Pisier's theorem
% \begin{align*}
%   \norm{\Phi^{(ab)}}_0
% \leq \frac1{nD} \norm{\Phi_A}_0
% \leq \frac1{nD} X t^{2} \sqrt N \left( d_a + d_b \right) \\
% \leq \frac78 \frac1{nD} \lambda \sqrt{nD} \sqrt N
% \leq \frac78\frac1{nD} \lambda \sqrt{nD} \sqrt{nD} \frac1{\sqrt{d_ad_b}}
% = \frac78\lambda \frac1{\sqrt{d_ad_b}}
% \leq \lambda \frac{\tr\rho}{\sqrt{d_ad_b}}
% \end{align*}
with failure probability at most~$\smash{( \frac{\sqrt{nD}}{k \dmax} )^{-\Omega(\dmin)}}$.
% \begin{align*}
%   \left( \frac{\lambda \sqrt{nD}}{d_a + d_b} \right)^{-\Omega(d_a + d_b)}
% \leq \left( \frac{20 k (d_a + d_b)}{\sqrt{nD}} \right)^{\Omega(d_a + d_b)}
% \leq \left( \frac{40 k \dmax}{\sqrt{nD}} \right)^{\Omega(d_a + d_b)} \\
% \leq \left( \frac{40 k \dmax}{\sqrt{nD}} \right)^{\Omega(2\dmin)} \\
% \lesssim \left( \frac{k \dmax}{\sqrt{nD}} \right)^{\Omega(\dmin)}
% \end{align*}

By the union bound, we conclude that all $\Phi^{(ab)}$ for $a\neq b$ are $(\eps,\lambda)$-quantum expanders and that $\tr\rho \in (\frac78,\frac98)$, up to a failure probability of at most
\begin{align*}
  e^{-\Omega(nD)}
+ k \smash{e^{-\Omega\bigl(\frac{nD}{k \dmax}\bigr)}}
+ k^2 \left( \frac{\sqrt{nD}}{k \dmax} \right)^{-\Omega(\dmin)}.
\end{align*}
The final term dominates, which implies the desired failure probability. To see that the final term dominates compare exponents: it suffices to show that $nD/ k \dmax \geq \dmin \log (\frac{\sqrt{nD}}{k \dmax})$ by our assumption on $n$, which states that $\alpha:= nD/k \dmax^2 \geq C$. Writing the desired inequality in terms of $\alpha$, we need $\dmax \alpha \geq \dmin \log(\sqrt{\alpha/k})$. This holds for $C$ large enough.
\end{proof}

We now show our second strong convexity result, namely that if our function is strongly convex at the identity then it is also strongly convex in an \emph{operator norm} ball about the identity.
The proof is given in \cref{app:robust}.

\begin{lemma}[Robustness of strong convexity]\label{convexRobustness}
There is a universal constant $0 < \eps_0 < 1$ such that if $\norm{\nabla_a f_x(I_D)}_{\op} \leq \eps_0/\sqrt{d_a}$ for all $a\in[k]$ and $\abs{\nabla_{0} f_{\samp}(I_{D})} \leq \eps_0$, then
$$\|\nabla^{2} f_{\samp}(\Theta) - \nabla^{2} f_{\samp}\|_{\op} = O(\delta)$$
for any $\Theta\in\P$ such that $\delta := \norm{\log\Theta}_{\op}(I_D) \leq \eps_0$. In particular, for any $\lambda > 0$, if~$f_x$ is $\lambda$-strongly convex at $I_D$ then $f_x$ is $(\lambda-O(\delta))$-strongly convex at $\Theta$.
\end{lemma}

Finally we obtain our strong convexity result near the identity.

\begin{prop}[Strong convexity near identity]\label{thm:ball-convexity}
There are universal constants $C,c>0$ such that the following holds.
Let $x = (x_1,\dots,x_n)$ be independent standard Gaussian random variables in~$\R^D$, where $n \geq C k \frac{\dmax^2}D$.
Then, with probability at least~$1 - k^2 ( \frac{\sqrt{nD}}{k \dmax} )^{-\Omega(\dmin)}$,
the function~$f_x$ is $\frac12$-strongly convex at any point $\Theta\in\P$ such that $\norm{\log\Theta}_{\op} \leq c$.
\end{prop}
\begin{proof}
We can choose $C>0$ such that both \cref{prop:gradient-bound,thm:tensor-convexity} apply (the former with $\eps\leq\eps_0/9\; $, where $\eps_0$ is the universal constant from \cref{convexRobustness}).
Then the assumptions of \cref{convexRobustness} are satisfied for $\lambda=\frac34$ with failure probability at most
\begin{align*}
  2(k+1)e^{-\eps^2 \frac{nD}{8\dmax}} + k^2 \left( \frac{\sqrt{nD}}{k \dmax} \right)^{-\Omega(\dmin)},
\end{align*}
where the latter term dominates, and there exists a constant $0<c\leq\eps_0$ such that $f$ is $\frac12$-strongly convex at any point $\Theta$ such that $\norm{\log\Theta}_{\op} \leq c$.
\end{proof}

\begin{remark}\label{remark-strong-convexity-balls}
While \cref{thm:ball-convexity} uses the operator norm to quantify closeness to the identity, we can easily translate it into a statement in terms of the geodesic distance on~$\P$.
Namely, under the same hypotheses it holds that~$f_x$ is $\frac12$-strongly convex on the geodesic ball~$B_r(I_D)$ of radius
\begin{align*}
  r= \frac c{\sqrt{(k+1)\dmax}},
\end{align*}
where $c>0$ is the universal constant from \cref{thm:ball-convexity}.
Indeed, if $\Theta = \exp_{I_D}(H)$, then
\begin{align*}
  \norm{\log\Theta}_{\op}
% = \norm{H_0 I_D + \sum_{a=1}^k \sqrt{d_a} H_{(a)}}_{\op}
\leq \abs{H_0} + \sum_{a=1}^k \sqrt{d_a} \norm{H_a}_{\op}
\leq \sqrt{\dmax} \left( \abs{H_0} + \sum_{a=1}^k \norm{H_a}_{\op} \right) \\
\leq \sqrt{\dmax} \left( \abs{H_0} + \sum_{a=1}^k \norm{H_a}_F \right)
\leq \sqrt{\dmax} \sqrt{k+1} \norm{H}_F,
\end{align*}
so if $\norm{H}_F \leq r$ then $\norm{\log\Theta}_{\op} \leq c$.
\end{remark}

%-----------------------------------------------------------------------------
\subsection{Proof of Theorem~\ref{thm:tensor-frobenius}}
%-----------------------------------------------------------------------------
We are now ready to prove the main result of this section according to the plan outlined in \cref{subsec:proof-sketch}.
We first state a result that bounds the geodesic distance between the precision matrix and the MLE.
The theorem then follows from bounds for $\Dop$ and $\DF$ in terms of geodesic distance.

\begin{prop}[Tensor normal geodesic error]\label{lem:tensor-geodesic}
There is a universal constant~$C>0$ such that the following holds.
Suppose that $t \geq 1$ and
\begin{align}\label{eq:proposition eps sqr assm}
  n \geq C k^2 \frac{\dmax^3}D t^2.
\end{align}
% \begin{align*}
%   \MW{OLD:}
%   n \geq C k^2 \frac{\dmax^2}{D} \max\{k, \dmax\} \max\{1 , t^2\}.
% \end{align*}
Then the MLE~$\htheta$ for $n$ independent samples of the tensor normal model with precision matrix~$\Theta$ satisfies
\begin{align*}
  d(\htheta,\Theta) &= O\left(\frac{\sqrt{k} \, \dmax }{\sqrt{nD}} t\right),
\end{align*}
with probability at least
\begin{align*}
  1 - k e^{-\Omega\bigl( t^2 \dmax \bigr)} - k^2 \left( \frac{\sqrt{nD}}{k \dmax} \right)^{-\Omega(\dmin)}.
\end{align*}
\end{prop}

\begin{proof}
By step~\ref{it:reduce} in \cref{subsec:proof-sketch}, it is enough to prove the theorem assuming $\Theta = I_D$.
Assuming this, we now show that the minimizer of $f_\rv$ exists and is close to $\Theta = I_D$ with high probability.
Let~$c>0$ be the constant from \cref{thm:ball-convexity}.
Consider the following two events:
\begin{enumerate}
\item\label{it:grad-bd} $\norm{\nabla f_x}_F \leq \delta := \sqrt{82 k} \frac{\dmax }{\sqrt{nD}} t$.
\item\label{it:sc-ball} $f_\rv$ is $\lambda$-strongly convex on the geodesic ball $B_r(I_D)$, where $\lambda=\frac12$ and radius $r := \smash{\frac c{\sqrt{(k+1) \dmax}}}$.
\end{enumerate}
Now, \cref{prop:gradient-bound} (with $\eps=\smash{\frac\delta{\sqrt{82 k}}}$) shows that, assuming~$\smash{\frac\delta{\sqrt{82 k}}} < 1$ and $n \geq \smash{\frac{\dmax^2}{D (\frac\delta{\sqrt{82 k}})^2}}$, that is,
$n > \frac{\dmax^2}D t^2$ and $t \geq 1$, the first event holds up to a failure probability of at most
\begin{align*}
  2(k+1)e^{-(\frac\delta{\sqrt{82 k}})^2 \frac{nD}{8\dmax}}
% = 2(k+1)e^{-(\frac{\dmax}{\sqrt{nD}} t)^2 \frac{nD}{8\dmax}}
% = 2(k+1)e^{-\frac{\dmax}{8} t^2}
= k e^{-\Omega(t^2 \dmax)}.
\end{align*}
On the other hand, \cref{thm:ball-convexity,remark-strong-convexity-balls} show that, assuming $n \geq C k \frac{\dmax^2}D$ for a universal constant $C>0$, the second event holds up to a failure probability of at most
\begin{align*}
  k^2 \left( \frac{\sqrt{nD}}{k \dmax} \right)^{-\Omega(\dmin)}.
\end{align*}
Note that both assumptions follow from \cref{eq:proposition eps sqr assm} for some appropriate choice of~$C$.
Thus, by the above and the union bound, both events hold simultaneously with the advertised success probability.
For $C$ large enough, \cref{eq:proposition eps sqr assm} moreover implies
$r > \frac{2\delta}\lambda$, since the latter is equivalent to
\begin{align*}
  n > \frac{16 \cdot 82}{c^2} k (k+1) \frac{\dmax^3}{D} t^2.
\end{align*}
Thus, if the above two events hold, \cref{lem:convex-ball} applies (with our choice of $\delta$ and $\lambda$) and shows that the MLE~$\htheta$ exists, is unique, and has geodesic distance at most $\frac\delta\lambda = 2 \delta$ from $\Theta = I_D$.
\end{proof}

The theorem now follows as a corollary.
We restate it for convenience.

\begin{customthm}{\ref{thm:tensor-frobenius}}[Tensor normal Frobenius error, restated]
\TensorFrob{equation*}{\tag{\ref{eq:eps sqr assm}}}
\end{customthm}

\begin{proof}
By \cref{lem:tensor-geodesic}, noting that \cref{eq:eps sqr assm} implies \cref{eq:proposition eps sqr assm}, we have with the desired failure probablity that~$\htheta$ is at most geodesic distance~$\delta$ from~$\Theta$, where~$\delta = O(\sqrt{k} \, \dmax t / \sqrt{nD})$.
% In the proof of \cref{lem:tensor-geodesic} we already verified that $4 \delta \leq r$ for $r \leq c/\sqrt{\dmax}$ if \eqref{eq:eps sqr assm} holds and $C, 1/c$ large enough. <--- \MW{I agree that this is basically right. Rephrased because here we want delta d_max <= 1, not <= some constant, which is of course ensure by possibly choosing C larger.}
Moreover, \cref{eq:proposition eps sqr assm} also implies $\delta \leq 1 / \sqrt{\dmax}$ if we choose~$C$ large enough.
% Indeed, this condition is equivalent to
% \begin{align*}
%   C' k \, \dmax^3 t^2 / (nD) \leq 1.
% \end{align*}
Thus, \cref{cor:g-convex-components} applies, and we have that for each $a\in [k]$,
\begin{align*}
  \DF(\htheta_a\Vert \Theta_a)
\leq 2 \sqrt{d_a} \cdot \delta
= O\left(t \, k^{1/2} \dmax \sqrt{\frac{d_a}{n D}} \right)
\end{align*}
as well as
\begin{align*}
  \DF(\htheta\Vert \Theta)
\leq 2 k \sqrt{D} \cdot \delta \, e^{2k\delta}
= O\left(t \, k^{3/2} \frac{\dmax}{\sqrt{n}}\right).
\end{align*}
In the last step, we used that $\delta k = O(1)$, which also follows from \cref{eq:eps sqr assm}.
Indeed, it is equivalent to $n \geq C' k^3 \frac{\dmax^2}D t^2$ for some $C'>0$.
\end{proof}





%=============================================================================
\section{Improvements for the matrix normal model}\label{sec:matrix-normal}
%=============================================================================
We now prove \cref{thm:matrix-normal}, which improves over \cref{thm:tensor-frobenius} in the case of the matrix normal model ($k=2$).
Throughout this section we assume without loss of generality that $d_1 \leq d_2$.
Our results for the matrix normal model are stronger in that:
\begin{enumerate}
\item the MLE is shown to be close to the truth \emph{in spectral norm} ($\Dop$) rather than the looser Frobenius norm ($\DF$),
\item the errors are \emph{tight for the individual factors}, and
\item the failure probability is \emph{inverse exponential} in the number of samples rather than inverse polynomial.
\end{enumerate}

The proof plan is similar to that in \cref{subsec:proof-sketch}, the main difference being that we now work directly with quantum expansion instead of translating into strong convexity.
An important tool will be a bound by~\cite{KLR19} which uses the notion of a \emph{spectral gap}, which is closely related to quantum expansion.

\begin{definition}[Spectral gap]\label{def:gap}
Let $\Phi\colon\Mat(d_b) \to \Mat(d_a)$ be a completely positive map.
Say $\Phi$ has \emph{spectral gap} $\gamma>0$ if
\begin{align}\label{eq:spectral-gap}
  \sigma_2(\Phi) \leq (1 - \gamma) \frac{\tr \Phi(I_{d_b})}{\sqrt{d_a d_b}}
\end{align}
where $\sigma_2$ denotes the second largest singular value of~$\Phi$.
Note that $\gamma\leq1$.
Moreover, the definition is invariant under rescaling $\Phi \mapsto c\Phi$ for $c>0$.
\end{definition}

\noindent

Recall that by the variational formula for singular values, if we let $K \in \Mat(d_{b})$ be the first (right) singular vector of $\Phi$, we can rewrite the above condition as
%an optimization over $H \in \Mat(d_{b})$:
\begin{align*}
  \sigma_2(\Phi) = \max_{\langle H, K \rangle = 0} \frac{\norm{\Phi(H)}_{F}}{\norm{H}_{F}} \leq (1 - \gamma) \frac{\tr \Phi(I_{d_b})}{\sqrt{d_a d_b}} .
\end{align*}
On the other hand, the definition of an $(\eps,\lambda)$-quantum expander is given in \cref{eq:expansion} as
\begin{align*}
      \norm{\Phi}_0 := \max_{\langle H, I_{d_{b}} \rangle = 0} \frac{\norm{\Phi(H)}_F}{\norm{H}_{F}}
\leq \lambda \frac{\tr \Phi(I_{d_b})}{\sqrt{d_a d_b}}  .
\end{align*}
Due to the $\eps$-doubly balanced condition in \cref{eq:doubly balanced}, it turns out that these two notions are closely related, as the following lemma shows.

\begin{lemma}[Lemma~A.3 in \cite{FM20}]\label{lem:fm20}
There exists a universal constant~$c>0$ with the following property.
If $\Phi$ is an $(\eps,\lambda)$-quantum expander and $\eps \leq c(1-\lambda)$, then~$\Phi$ has spectral gap~$1-\lambda-O(\eps)$.
\end{lemma}

We now state the bound of~\cite{KLR19} in our language.
Because~$k = 2$, the gradient and Hessian are completely described by the single completely positive map~$\Phi^{(12)}$ (compare the formulas in \cref{lem:gradient,lem:hessian} with \cref{eq:hessian channel,eq:channel to single marginals}).
Suppose we are given samples $y_1,\dots,y_n$, which we can identify with $d_1\times d_2$ matrices $Y_1,\dots,Y_n$.
Then~$\Phi^{(12)} = \frac1{nD}\Phi_Y$, as discussed below \cref{thm:hess-pisier}.
Moreover, the double balancedness and spectral gap are invariant under rescaling.
This explains why the following bound can be purely stated in terms of~$\Phi_Y$.
Recall that $\SSPD(d)$ denotes the $d\times d$ positive definite matrices of unit determinant.

\begin{theorem}[Theorem 1.8, Proof of Theorem~3.22 in \cite{KLR19}]\label{thm:klr}
There is a universal constant $C>0$ such that the following holds.
If $d_1 \leq d_2$ and the completely positive map $\Phi_Y$ is $\eps$-doubly balanced and has spectral gap~$\gamma$, where~$\gamma^2 \geq C \eps \log d_1$, then, restricted to $\SSPD(d_1) \ot \SSPD(d_2)$, the function~$f_y$ has a unique minimizer~$P = P_1 \ot P_2$ such that
\begin{align*}
  \max\ \Bigl\{ \norm{P_1 - I_{d_1}}_{\op}, \norm{P_2 - I_{d_2}}_{\op} \Bigr\}
= O\left(\frac{\eps \log d_1}\gamma\right).
\end{align*}
Moreover, $f_y(P) \geq (1 - \frac{4 \eps^2}{\gamma}) \tr\rho$.
%\frac{\|x\|^2}{nD}$.
%\AR{Both statements seems correct to me because we optimize over SPD, so then $f$ is just norm-squared and KLR grad flow remains in SPD, and defintions of spectral gap/$\eps$-doubly balanced are homogenous.}
\end{theorem}

We can immediately translate this into a statement about the MLE.

\begin{corollary}[Spectral gap implies MLE nearby]\label{cor:klr}
There is a universal constant $C>0$ such that the following holds.
Let~$\eps,\gamma\in(0,1)$, $1 < d_1 \leq d_2$, and suppose the completely positive map $\Phi_Y$ is $\eps$-doubly balanced and has spectral gap~$\gamma$, where $\gamma^2 \geq C \eps \log d_1$.
Further assume that $\norm{y}_2^2 = nD$.
\MW{In case we need to relax this condition, comment other version back in (and change the proof of Theorem 2.6.}
% Further suppose that $\abs{\tr\rho - 1} \leq \delta$ for some $\delta \leq \frac12$.
Then the MLE $\htheta = \htheta_1 \ot \htheta_2$ exists, is unique, and satisfies (using our conventions)
\begin{align*}
  \max \, \Bigl\{ \norm{\htheta_1 - I_{d_1}}_{\op}, \norm{\htheta_2 - I_{d_2}}_{\op} \Bigr\}
= O\Bigl(\frac{\eps \log d_1}\gamma\Bigr).
% = O\Bigl(\delta + \frac{\eps \log d_1}\gamma\Bigr).
\end{align*}
\end{corollary}
\begin{proof}
To compute the MLE, we reparameterize by $\htheta_1 = \lambda P_1$ and $\htheta_2 = \lambda P_2$ where $P_1 \in \SSPD(d_1)$, $P_2 \in \SSPD(d_2)$, and $\lambda \in \R_{> 0}$.
Plugging this reparametrization into the formula~\eqref{eq:neg log likelihood} for~$f_y$ shows that $(\lambda, P_1, P_2)$ solve
\begin{align*}
  \argmin_{\lambda, P_1, P_2} \lambda^2 f_x(P_1 \ot P_2) - \log(\lambda^2).
\end{align*}
In particular, the MLE $\htheta_1, \htheta_2$ exists uniquely if $f_y$ has a unique minimizer~$P = P_1 \ot P_2$ when restricted to $\SSPD(d_1) \ot \SSPD(d_2)$.
Such unique minimizers exist by \cref{thm:klr}.
Given $P_1$, $P_2$, solving the simple one-dimensional optimization problem for $\lambda$ yields
\begin{align*}
  \lambda = \frac1{\sqrt{f_y(P_1)}}.
\end{align*}
By \cref{thm:klr} and using the assumption that $\tr\rho=\frac{\norm{y}_2^2}{nD}=1$, $f_y(P) \geq 1 - \frac{4 \eps^2}{\gamma}$, and we also have $f_y(P) \leq f_y(I_D) = \tr\rho = 1$ since~$P$ is the minimizer in $\SSPD(d_1) \ot \SSPD(d_2)$.
Therefore,
\begin{align*}
  1
\leq \lambda
\leq \left( 1 - \frac{4 \eps^2}{\gamma}\right)^{-1/2}.
\end{align*}
By our assumptions on $\gamma$ and $\eps$, we have $\frac{\eps^2}\gamma \leq \frac\eps\gamma \leq \frac\eps{\gamma^2} \leq \frac1{C \log d_1}$.
Thus, choosing $C>0$ large enough, we obtain
\begin{align*}
  \abs{\lambda - 1}
= O\left(\frac{\eps^2}{\gamma}\right)
\leq O\left(\frac{\eps \log d_1}{\gamma}\right).
\end{align*}
hence in particular $\lambda = O(1)$.
Since also $\norm{P_a - I_{d_a}}_{\op} = O(\eps\log d_1/\gamma)$ by \cref{thm:klr}, we conclude that
\begin{align*}
  \norm{\htheta_a - I_{d_a}}_{\op}
% = \norm{\lambda P_a - \lambda I_{d_a} + \lambda I_{d_a} - I_{d_a}}_{\op}
\leq \lambda \norm{P_a - I_{d_a}}_{\op} + \abs{\lambda - 1}
= O\Bigl(\frac{\eps \log d_1}\gamma\Bigr)
\end{align*}
for $a\in\{1,2\}$.
This completes the proof.
% By \cref{thm:klr}, $f_x(P) \geq (1 - \frac{4 \eps^2}{\gamma}) \tr\rho$, and we also have $f_x(P) \leq f_x(I_D) = \tr\rho$, since~$P$ is the minimizer in $\SSPD(d_1) \ot \SSPD(d_2)$.
% Therefore,
% \begin{align*}
%   (\tr\rho)^{-1/2}
% \leq \lambda
% \leq \left( 1 - \frac{4 \eps^2}{\gamma}\right)^{-1/2} (\tr\rho)^{-1/2}.
% \end{align*}
% By our assumptions on $\gamma$ and $\eps$, we have $\frac{\eps^2}\gamma \leq \frac\eps\gamma \leq \frac\eps{\gamma^2} \leq \frac1{C \log d_1}$.
% Thus, choosing $C>0$ large enough and using the assumption that $\abs{\tr\rho - 1} \leq \delta \leq \frac12$, we obtain
% \begin{align*}
%   \abs{\lambda - 1}
% = O\left(\delta + \frac{\eps^2}{\gamma}\right)
% \leq O\left(\delta + \frac{\eps \log d_1}{\gamma}\right).
% \end{align*}
% hence in particular $\lambda = O(1)$.
% Since also $\norm{P_a - I_{d_a}}_{\op} = O(\delta + \eps\log d_1/\gamma)$ by \cref{thm:klr}, we conclude that
% \begin{align*}
%   \norm{\htheta_a - I_{d_a}}_{\op}
% % = \norm{\lambda P_a - \lambda I_{d_a} + \lambda I_{d_a} - I_{d_a}}_{\op}
% \leq \lambda \norm{P_a - I_{d_a}}_{\op} + \abs{\lambda - 1}
% = O\Bigl(\delta + \frac{\eps \log d_1}\gamma\Bigr)
% \end{align*}
% for $a\in\{1,2\}$.
% This completes the proof.
\end{proof}

\Cref{lem:fm20,thm:klr}, along with what we have shown so far, already imply a preliminary version of \cref{thm:matrix-normal}.
Indeed, similarly to the proof of \cref{thm:tensor-convexity}, one can use \cref{prop:gradient-bound,prp:xnorm} to show that under suitable assumptions on $n$, $t$, the completely positive map~$\Phi^{(12)}$ is a $(t \sqrt{{d_2}/{n d_1}}, \lambda)$-quantum expander for some constant~$\lambda\in(0,1)$ with failure probability
\[ O(e^{ - \Omega( d_2 t^2)}) + \left( \frac{\sqrt{nD}}{d_2} \right)^{ - \Omega(d_1)}. \]
By \cref{thm:klr}, we immediately have that with the above failure probability the MLEs satisfy
\begin{align*}
  \Dop(\Theta'_a \Vert \Theta_a) = O\left(t \sqrt{\frac{d_2}{n d_1}} \log d_1\right),
\end{align*}
which matches \cref{thm:matrix-normal} for the larger Kronecker factor.

One of the main results of this section is the following theorem, which shows that the expansion constant $\lambda$ of $\Phi$ can be made constant with \emph{exponentially small} failure probability.
% , albeit with a worse constant $\lambda$.
Recall that for the matrix model, the samples $x_i$ can be viewed as $d_1 \times d_2$-matrices, which we denote by $X_i$.

\begin{theorem}[Expansion]\label{thm:operator-cheeger}
There are universal constants $C > 0$ and $\lambda\in(0,1)$ such that the following holds.
For $d_1 \leq d_2$, $d_2>1$, let $X=(X_1,\dots,X_n)$ be random $d_1 \times d_2$ matrices with independent standard Gaussian entries, where $n \geq C \frac{d_2}{d_1} \max\{\log d_2, t^2\}$ and $t\geq1$.
Then, $\Phi_X$ is a $(t \smash{\sqrt{\frac{d_2}{n d_1}}}, \lambda)$-quantum expander with probability at least $1 - O(e^{ - \Omega( d_2 t^2)})$.
\end{theorem}

We will prove \cref{thm:operator-cheeger} in \cref{app:cheeky} using techniques similar to \cite{FM20} using Cheeger's inequality.
This also improves our result on strong convexity (\cref{thm:ball-convexity}), which will be useful in the analysis of the flip-flop algorithm.
Indeed, for $k = 2$, using \cref{thm:operator-cheeger} (in place of \cref{thm:hess-pisier}) with \cref{lem:expansion-convexity} in the proof of \cref{thm:tensor-convexity} improves the failure probability in \cref{thm:tensor-convexity} to $1 - e^{ - \Omega( d_2 t^2)}$.
As in the proof of \cref{thm:ball-convexity}, combining this failure probability bound with \cref{convexRobustness} yields the next corollary.

\begin{corollary}\label{cor:matrix-convexity}
There are universal constants $C, c > 0$ and $\lambda\in(0,1)$ such that the following holds.
For $d_1 \leq d_2$, let $x=(x_1,\dots,x_n)$ be independent standard Gaussian random variables in~$\R^{d_1d_2}$, where $n \geq C \frac{d_2}{d_1} \max\{\log d_2, t^2 \}$ and $t\geq1$.
Then, with probability at least $1 - e^{ - \Omega( d_2 t^2)}$, the function~$f_x$ is $(1-\lambda)$-strongly convex at any point $\Theta\in\P$ such that $\norm{\log\Theta}_{\op} \leq c$.
\end{corollary}



%-----------------------------------------------------------------------------
\subsection{Proof of Theorem~\ref{thm:matrix-normal}}
%-----------------------------------------------------------------------------
We now use \cref{thm:operator-cheeger} as well as some more refined concentration inequalities to prove \cref{thm:matrix-normal}.
The additional concentration is required to obtain the tighter bounds on the smaller Kronecker factor.
Throughout this section, we still assume without loss of generality that $d_1 \leq d_2$.
%\AR{\cref{prop:gradient-bound} shows bounds on $\|(\nabla f_\samp)_{a}\|_{\op} $ which are inversely proportional to $\dmax$, so in the case when $d_{2} \gg d_{1}$ we can actually prove much better bounds on  $\|(\nabla f_\samp)_{1}\|_{\op}$ than on $\|(\nabla f_\samp)_{2}\|_{\op}$.  }

The idea of the proof is to apply one step of the flip-flop algorithm to ``renormalize'' the samples such that the second partial trace is proportional to $I_{d_2}$.
This has the effect of making the second component of the gradient~$\nabla f_\samp$ equal to zero.
We will show that the first component still enjoys the same concentration exploited in \cref{prop:gradient-bound} even after the step of flip-flop -- thus the total gradient has become smaller, but only the second component of the MLE estimate has changed.
Thus, intuitively, the total change in the first component will be small.
Using \cref{convexRobustness} to control the change induced in the minimum eigenvalue of the Hessian by the first step of the flip-flop and applying \cref{lem:convex-ball} results in a geodesic distance error proportional to the new gradient after flip-flop, which gives us the tighter bound.
To obtain a relative spectral norm error bound, we employ a similar strategy but with \cref{cor:klr} instead of \cref{lem:convex-ball}.

We now discuss the concentration bound.
Let $X_1,\dots,X_n$ be random $d_1 \times d_2$ matrices with independent standard Gaussian entries.
% Note that
% \begin{align*}
%   \Phi^{(12)}(\cdot) = \frac1{nD} \sum_{i=1}^n X_i (\cdot) X_i^T.
% \end{align*}
Consider new random variables $Y_1,\dots,Y_n$ obtained by one step of the flip-flop algorithm applied to the second, larger Kronecker factor (cf.\ \cref{subsec:flip-flop}).
That is, for $i\in[n]$:
\begin{align}\label{eq:one-step}
  Y_i = X_i \left( \frac1{nd_1} \sum_{i=1}^n X_i^T X_i \right)^{-1/2}.
\end{align}
The completely positive map $\Phi^{(12)}$ corresponding to the ``renormalized'' samples $Y_1,\dots,Y_n$ is $\frac1{nD} \Phi_Y$.
By construction, it satisfies
\begin{align*}
  \frac1{n D} \Phi_Y(I_{d_2}) = \frac1{d_2} \sum_{i=1}^{n} X_i \left( \sum_{i=1}^n X_i^T X_i \right)^{-1} X_i^T
\quad\text{and}\quad
  \frac1{n D} \Phi^*_Y(I_{d_1}) = \frac{I_{d_2}}{d_2}.
\end{align*}
Note also that $\tr \Phi_Y(I_{d_2}) = \tr \Phi_Y^*(I_{d_1}) = \|Y\|^2 = nD$. Thus $\Phi_Y$ is $\delta$-doubly balanced if and only if $\|\frac{1}{nD} \Phi_Y(I_{d_2}) - \smash{\frac{I_{d_1}}{d_1}}\|_{\op} \leq \frac{\delta}{d_1}$.

\begin{lemma}[Concentration after flip-flop]\label{lem:flipflop-concentration}
There is a universal constant $c>0$ such that the following holds.
Let $X_1,\dots,X_n$ be random $d_1 \times d_2$ matrices with independent standard Gaussian entries, where $d_1 \leq d_2$.
If $n \geq \frac{d_2}{d_1}$ and $t\geq c$, then for $\Phi_Y$ with $Y$ as in \cref{eq:one-step} we have, with probability at least $1 - e^{- \Omega( d_1 t^2)}$,
\begin{align*}
  \norm*{\frac1{n D} \Phi_Y(I_{d_2}) - \frac{I_{d_1}}{d_1}}_{\op} \leq t \sqrt{\frac1{nD}}.
\end{align*}
By the remarks preceding the lemma, this condition implies $\Phi_Y$ is $t \sqrt{\frac{d_1}{nd_2}}$-doubly balanced.
\end{lemma}

The proof of this lemma uses a result of \cite{hayden2006aspects} on the overlap of two random projections, combined with a standard net argument.
The details can be found in \cref{app:flipflop-concentration}.

We need as one final ingredient the following robustness result for quantum expansion which will play a role analogous to our \cref{convexRobustness}.
% The result is essentially Lemma~4.4 in \cite{FM20}, along with the observation that quantum expansion is defined in a scale-invariant way (cf.\ the discussion below \cref{def:expansion}).

\begin{lemma}[Robustness of quantum expansion, Lemma~4.4 in \cite{FM20}]\label{lem:robust expansion}
There exists a universal constant~$c>0$ with the following property:
Let $X=(X_1,\dots,X_n)$, $Y=(Y_1,\dots,Y_n)$ be tuples of $d_1\times d_2$ matrices such that $Y_i = X_i R$ for some~$R \in \GL(d_2)$.
Let $0<\eps,\lambda<1$.
If $\Phi_X$ is an $(\eps,\lambda)$-quantum expander and $\norm{R^TR - I_{d_2}}_{\op} \leq \delta$ for some $\delta\leq c$, then $\Phi_Y$ is an $(\eps + O(\delta), \lambda + O(\delta))$-quantum expander.
\end{lemma}

With the above tools in hand, we may now prove \cref{thm:matrix-normal}.

\begin{customthm}{\ref{thm:matrix-normal}}[Matrix normal spectral error, restated]
\MatrixSpec
\end{customthm}
\begin{proof}
As discussed in \cref{subsec:proof-sketch}, we may assume without loss of generality that~$\Theta_a = I_a$ for~$a\in\{1,2\}$.
Let $x=(x_1,\dots,x_n)$ be our tuple of samples, which we can identify with a tuple $X=(X_1,\dots,X_n)$ independent random $d_1\times d_2$ matrices with independent standard Gaussian entries.
Define $Y=(Y_1,\dots,Y_n)$ as in \cref{eq:one-step}.
Consider the following three events:
\begin{enumerate}
\item The operator $\Phi_X$ is a $(t\sqrt{d_2/nd_1},\lambda)$-quantum expander for $\lambda\in(0,1)$ as in \cref{thm:operator-cheeger}.
\item The operator $\Phi_Y$ is $t\sqrt{d_1/nd_2}$-doubly balanced.
\item $\abs{\frac{\norm{x}_2^2}{nD} - 1} \leq t\sqrt{d_2/nd_1}$.
\end{enumerate}
By \cref{thm:operator-cheeger} and our assumptions, the first event occurs with probability at least $1 - O(e^{ - \Omega( d_2 t^2)})$ provided we choose~$C,c$ large enough.
% Indeed, we need $t \geq 1$ and $n \geq C' \frac{d_2}{d_1} \max\{\log d_2, t^2\}$ for some universal constant C'.
By \cref{lem:flipflop-concentration} and our assumptions, the second event occurs with probability at least $1 - e^{ - \Omega( d_1 t^2)}.$
% Indeed, we need $t \geq c$ and $n \geq \frac{d_2}{d_1}$.
Finally, the third event occurs with probability at least $1 - e^{-\Omega(d_2^2 t^2)}$ by \cref{prp:xnorm} and our assumptions.
% Note that we apply the Proposition with t_{prop} = t\sqrt{d_2/nd_1}, so we require t_{prop} < 1, that is, n \geq \frac{d_2}{d_1}.
By the union bound, all three events occur simultaneously with probability at least $1 - O(e^{-\Omega(d_1 t^2)})$, which is the desired success probability.

We now show that the three events together imply the desired properties.
We first want to use \cref{lem:robust expansion} to relate the quantum expansion of~$\Phi_X$ and~$\Phi_Y$.
By definition, $Y_i = X_i R$ for $R := ( \frac1{nd_1} \sum_{i=1}^n X_i^T X_i )^{-1/2} = R^T$.
Now note that
\begin{align*}
  R^{-2} - I_{d_2}
= \frac1{nd_1} \sum_{i=1}^n X_i^T X_i - I_{d_2}
= \frac{\norm{x}_2^2}{nD} \left( d_2 \frac{\Phi_X^*(I_{d_1})}{\tr \Phi_X^*(I_{d_1})} - I_{d_2} \right)
+ \left( \frac{\norm{x}_2^2}{nD} - 1 \right) I_{d_2}.
\end{align*}
Therefore, by the first and the third event,
\begin{align*}
  \norm{R^{-2} - I_{d_2}}_{\op}
% \leq (1 + t\sqrt{d_2/nd_1}) t\sqrt{d_2/nd_1} + t\sqrt{d_2/nd_1}
= O\left(t\sqrt{\frac{d_2}{nd_1}}\right),
% \leq (1 + t\sqrt{d_2/nd_1}) t\sqrt{d_2/nd_1} + t\sqrt{d_2/nd_1}
\end{align*}
noting that $t\sqrt{\frac{d_2}{nd_1}} \leq \frac1{\sqrt C}$ can be made smaller than any constant by choosing~$C$ large enough.
This also implies that
\begin{align}\label{eq:RR bound}
  \norm{R^TR - I_{d_2}}_{\op}
= \norm{R^2 - I_{d_2}}_{\op}
= O\left(t\sqrt{\frac{d_2}{nd_1}}\right).
\end{align}
Noting again that the right-hand side can be made smaller than any universal constant, we can now apply \cref{lem:robust expansion} to see that $\Phi_Y$ is a $(t\sqrt{d_1/nd_2},\lambda')$-quantum expander for some universal constant~$\lambda'\in(0,1)$ (the double balancedness follows from the second event!), and then \cref{lem:fm20}, which shows that $\Phi_Y$ has spectral gap~$\gamma$ for a universal constant~$\gamma\in(0,1)$.

Finally, noting that $\norm{y}_2^2 = \sum_{i=1}^n \tr Y_i^T Y_i = nD$ and using our assumption on $n$, provided we choose $C$ large enough we may apply \cref{cor:klr} with $\eps = t\sqrt{d_1/nd_2}$.
% Indeed, $\gamma^2 \geq C_{KLR} \eps \log d_1$ is equivalent to $n \geq C t^2 \frac{d_1}{d_2} \log^2 d_1$, and this is implied by our assumption (note that $\frac{d_2}{d_1} > \frac{d_1}{d_2}$).
We obtain:
\begin{align}\label{eq:MLE Y bound}
  \max \, \Bigl\{ \norm{\htheta_1(Y) - I_{d_1}}_{\op}, \norm{\htheta_2(Y) - I_{d_2}}_{\op} \Bigr\}
% = O\Bigl(\frac{\eps \log d_1}\gamma\Bigr)
= O\Bigl(t\sqrt{\frac{d_1}{nd_2}} \log d_1\Bigr),
\end{align}
where $\htheta_a(Y)$ denotes components the MLE for the samples $Y=(Y_1,\dots,Y_n)$.
By equivariance, the components of the MLE for the samples $X=(X_1,\dots,X_n)$ are then given by~$\htheta_1(X) = \htheta_1(Y)$ and $\htheta_2(X) = R \, \htheta_2(Y) R$.
This immediately yields the bound
\begin{align*}
  \Dop(\htheta_1(X)\Vert\Theta_1)
= \Dop(\htheta_1(X)\Vert I_{d_1})
= O\Bigl(t\sqrt{\frac{d_1}{nd_2}} \log d_1\Bigr).
\end{align*}
To bound $\Dop(\htheta_2(X)\Vert\Theta_2)$, we use invariance of $\Dop$ and the approximate triangle inequality (\cref{lem:triangle-ineq}) to write
\begin{align*}
  \Dop(\htheta_2(X)\Vert\Theta_2)
&= \Dop(\htheta_2(X)\Vert I_{d_2})
= \Dop(R \, \htheta_2(Y) R\Vert I_{d_2})
= \Dop(\htheta_2(Y)\Vert R^{-2}) \\
&= O\left(\Dop(\htheta_2(Y)\Vert I_{d_2}) + \Dop(I_{d_2}\Vert R^{-2})\right) \\
&= O\left(t\sqrt{\frac{d_1}{nd_2}} \log d_1\right)
+ O\left(t\sqrt{\frac{d_2}{nd_1}}\right)
= O\left(t\sqrt{\frac{d_2}{nd_1}} \log d_1\right)
\end{align*}
using \cref{eq:MLE Y bound,eq:RR bound}; by choosing $C$ large enough we can ensure that the right-hand side is smaller than any universal constant, which justifies the application of \cref{lem:triangle-ineq}.
\end{proof}

%=============================================================================
\section{Convergence of the flip-flop algorithm}\label{sec:flipflop}
%=============================================================================

In this section we prove that the flip-flop algorithms for the matrix and tensor normal models converge quickly to the MLE with high probability.
Throughout this section, we will use $\log$ to denote logarithm on base 2. \MW{$\leftarrow$ Aren't we mixing? E.g. when we talk about the objective + Lemma 5.1 of Garg.}
We begin by describing the general flip-flop algorithm in \cref{alg:flip-flop} (cf.\ \cref{subsec:flip-flop}).

\begin{Algorithm}
\begin{description}
\item[\hspace{.2cm}\textbf{Input}:] Samples $\samp = (\samp_1, \ldots, \samp_n)$, where each $\samp_i \in \R^D = \R^{d_1} \ot \cdots \ot \R^{d_k}$.
 % is sampled from a (unknown) centered normal distribution $\cN(0, \Sigma_1\ot \dots \ot \Sigma_k)$ , where each entry of $\samp_i$ is encoded in binary, with bit size $\le b$.
Parameters $T\in\N$ and $\delta>0$.
% Approximation parameter $0 < \delta < 1$, given in binary representation. \\%[.3ex]}

\item[\hspace{.2cm}\textbf{Output}:]
An estimate $\otheta = \otheta_1 \ot \cdots \ot \otheta_k \in \SPD$ of the MLE.

\item[\hspace{.2cm}\textbf{Algorithm}:]
\end{description}
\begin{enumerate}
\item\label{it:flip-flop step 1} Set $\otheta_a = I_{d_a}$ for each $a \in [k]$.

\vspace{10pt}

\item\label{it:flip-flop step 2} For $t=1,\dots,T$, where $T$ is a function of $\dmax, \delta, k$, repeat the following:

\vspace{5pt}

\begin{itemize}
\item Compute $\rho_t = \dfrac{1}{nD} \cdot  \otheta^{1/2} \left( \sum_{i=1}^n x_ix_i^T \right) \otheta^{1/2}$.

\item Compute each component of the gradient using the formula
 % $\nabla f_{\samp}(\otheta_1, \ldots, \otheta_k)$, denoting $\nabla_a := \nabla_a f_{\samp}(\otheta_1, \ldots, \otheta_k)$, and find the index $a \in [k]$ for which $\norm{\nabla_a}_F$ is largest.
$\nabla_a f_x(\otheta) = \sqrt{d_a} ( \rho_t^{(a)} - \tr(\rho_t) \smash{\frac{I_{d_a}}{d_a}} )$,
where $\smash{\rho_t^{(a)}}$ denotes the partial trace (\cref{definition:partial-trace}),
and find the index $a \in [k]$ for which $\norm{\nabla_a f_x(\otheta)}_F$ is largest.

\vspace{5pt}

\item
If $\norm{\nabla_a f_x(\Theta)}_F \leq \delta$, output $\otheta$ and return.
\MW{One might be confused into thinking that the $\otheta_a$ satisfy our conventions.}

% \vspace{5pt}

\item Otherwise, update $\otheta_a \leftarrow \frac{1}{d_a} \otheta_a^{1/2} \left(\rho_t^{(a)}\right)^{-1} \otheta_a^{1/2}$.
\end{itemize}
\end{enumerate}
\caption{Flip-flop algorithm}\label{alg:flip-flop}
\end{Algorithm}

Before we analyze the convergence of the flip-flop algorithms for the tensor and matrix normal models (for suitable choices of the parameters~$T$ and~$\delta$), we discuss the straightforward generalization of standard convergence results for descent methods under strong convexity to the geodesically convex setting.

The next lemma shows that any descent method which manages to decrease the value of the function with respect to the gradient by a constant (more precisely the parameter $\alpha$), if starting from a sublevel set where the function is strongly convex, will converge quickly to the optimum.
The proof of the lemma is a straightforward translation of a corresponding proof in \cite{FM20}; we give it here for completeness.

\begin{lemma}[Proof of Lemma~4.11 in \cite{FM20}]\label{lem:descent-sublevel-set}
	Let $f \colon \SPD \rightarrow \R$ be $\lambda$-strongly geodesically convex on a sublevel set~$S$.
  Let $x_0 \in S$ and let $\alpha,\beta>0$ such that $\norm{\nabla f(x_0)}_F^2 \leq \beta$ and $\{x_t\}_{t\in[T]}$ be a sequence satisfying
  \begin{align}\label{eq:descent prop}
    f(x_t) \leq f(x_{t-1}) - \alpha \cdot \min \bigl\{\beta, \, \norm{\nabla f(x_{t-1})}^2_F \bigr\},
  \end{align}
  for $t\in[T]$.
  Then,
  \begin{align*}
    \min_{0\leq t\leq T} \norm{\nabla f(x_t)}^2_F \leq \norm{\nabla f(x_0)}_F^2 \cdot 2^{- T \alpha \lambda}.
  \end{align*}
\end{lemma}

\begin{proof}
	Let $f^*$ be the minimum value of the function $f$.
  Since $f$ is $\lambda$-strongly geodesically convex on~$S$, we have
  \begin{align}\label{eq:g convex nonsense}
    f^* \geq f(x) - \frac{1}{2\lambda} \norm{\nabla f(x)}_F^2
  \end{align}
	for any $x \in S$.
  % To see \cref{eq:g convex nonsense}, write $f^* = f(\exp_x(H))$ and consider $g(t) := f(\exp_x(tH))$.
  % Then, $g(0) = f(x)$, $g'(0) = \braket{\nabla f(x),H} \geq -\norm{\nabla f(x)}_F \norm{H}_F$, and $g''(t) \geq \lambda \norm{H}_F^2$ for~$t\in[0,1]$, since the geodesic segment~$\exp_x(tH)$, $t\in[0,1]$, is contained in the geodesically convex sublevel set~$S$.
  % Thus,
  % \begin{align*}
  %   f^* = g(1)
  % &\geq f(x) - \norm{\nabla f(x)}_F \norm{H}_F + \frac\lambda2\norm{H}_F^2 \\
  % &\geq \min_{s\in\R} f(x) - \norm{\nabla f(x)}_F s + \frac\lambda2 s^2 \\
  % &= f(x) - \frac1{2\lambda} \norm{\nabla f(x)}_F^2.
  % \end{align*}
  Since $\{x_t\}$ is a descent sequence, i.e., $f(x_t) \leq f(x_{t-1})$ for all $t\in[T]$, we know that each $x_t \in S$.
  Therefore, \cref{eq:g convex nonsense} holds for any $x_t$, $0 \leq t \leq T$.

  We claim that for any $x_t$ such that $\eps := \norm{\nabla f(x_t)}_F^2 \leq \beta$, there exists $\ell \leq 1/\alpha \lambda$ such that $\norm{\nabla f(x_{t + \ell})}_F^2 \leq \eps/2$. This is enough to conclude the proof of the lemma, as with this claim we see that we halve the squared norm of the gradient at every sequence of $1/\alpha \lambda$ steps.
  % and will remain less than $\beta$.

	To prove the claim, we assume that $\norm{\nabla f(x_{t+\ell})}_F^2 \geq \eps/2$ for all $\ell\in[m]$ (this is also true for $\ell=0$).
  We wish to show that $m \leq 1/\alpha\lambda$.
  To see this, note that from \cref{eq:descent prop} we have
  \begin{align*}
    f(x_{t+\ell})
  \leq f(x_{t+\ell-1}) - \alpha \cdot \min \bigl\{\beta, \, \norm{\nabla f(x_{t+\ell-1})}^2_F \bigr\}
  \leq f(x_{t+\ell-1}) - \frac{\alpha\eps}2
  \end{align*}
  for all $\ell\in[m]$, and therefore
  \begin{align*}
    f(x_{t+m}) \leq f(x_t) - \frac{\alpha\eps m}2.
  \end{align*}
  On the other hand, \cref{eq:g convex nonsense} implies that
  \begin{align*}
    f(x_{t+m})
  \geq f^* \geq f(x_t) - \frac{1}{2\lambda} \norm{\nabla f(x_t)}_F^2
  \geq f^* \geq f(x_t) - \frac{\eps}{2\lambda}.
  \end{align*}
  Together, we find that $m \leq 1/\alpha\lambda$ as claimed.
  This concludes our proof.
\end{proof}

We now consider the flip-flop algorithm in \cref{alg:flip-flop}.
Note that at the end of each iteration, we update only a single Kronecker factor~$\otheta_a$.
This update has the following property.
% is chosen such that~$\rho^{(a)} = \frac{I_{d_a}}{d_a}$ and hence $\tr\rho=1$ in the subsequent iteration, as we record in the following lemma.

\begin{lemma}[Flip-flop update]\label{lemma:flip-flop-update}
  Let $t\in\{1,\dots,T-1\}$ and assume the flip-flop algorithm has not terminated before the $(t+1)$-st iteration.
  Then $\rho_{t+1}^{(a)} = \frac{I_{d_a}}{d_a}$, where $a\in[k]$ denotes the index chosen in the $t$-th iteration.
  As a consequence, $\tr\rho_t=1$ for $t=2,\dots,T$.
\end{lemma}

\noindent
In view of \cref{lem:gradient,remark:gradient-everywhere}, the above means that in each iteration but the first, $\nabla_0 f_x(\otheta) = 0$ as well as $\nabla_a f_x(\otheta) = 0$ for the $a\in[k]$ chosen in the preceding iteration.
Thus the flip-flop algorithm can be understood as carrying out an alternating minimization or coordinate descent of the objective function~$f_x$.

\begin{proof}
Let $\otheta$ denote the precision matrix at the beginning of the $t$-th iteration.
Then,
\begin{align*}
  \rho_{t+1}^{(a)}
&= \left( \frac1{d_a} \otheta_a^{1/2} \left( \rho_t^{(a)} \right)^{-1} \otheta_a^{1/2} \right)^{1/2} \otheta_a^{-1/2}
\rho_t^{(a)}
\otheta_a^{-1/2} \left( \frac1{d_a} \otheta_a^{1/2} \left( \rho_t^{(a)} \right)^{-1} \otheta_a^{1/2} \right)^{1/2} \\
&= \frac1{d_a} \left( \otheta_a^{1/2} \left( \rho_t^{(a)} \right)^{-1} \otheta_a^{1/2} \right)^{1/2}
\!\!\left( \otheta_a^{1/2} \left( \rho_t^{(a)} \right)^{-1} \otheta_a^{1/2} \right)^{-1}\!\!
\left( \otheta_a^{1/2} \left( \rho_t^{(a)} \right)^{-1} \otheta_a^{1/2} \right)^{1/2} \\
&= \frac1{d_a} I_{d_a}.
\end{align*}
\end{proof}

We now show that the flip-flop algorithm produces a descent sequence as in \cref{eq:descent prop}.

\MW{Michael continue here.}

\begin{lemma}[Descent]\label{lem:tensor-descent-lemma}
  Let $k \geq 2$ and $t\in\{2,\dots,T\}$ and assume that the flip-flop algorithm has not terminated before the $(t+1)$-st iteration.
  Let $\otheta$ and $\otheta'$ denote the precision matrices at the beginning of the $t$-th and the $(t+1)$-st iteration, respectively.
  Then,
  \begin{align*}
    f_x(\otheta') \leq f_x(\otheta) - \frac1{6(k-1)} \min \left\{ \frac{k-1}{\dmax}, \norm{\nabla f_x(\otheta)}_F^2 \right\}.
  \end{align*}
\end{lemma}
\begin{proof}
  Recall that
  \begin{align*}
    f_x(\otheta) &= \tr\rho_t - \frac1D\log\det\otheta.
    % f_x(\otheta') &= \tr\rho_{t+1} - \frac1D\log\det\otheta'.
  \end{align*}
  and similarly for $f_x(\otheta')$.
  By \cref{lemma:flip-flop-update}, we have $\tr\rho_t = \tr\rho_{t+1} = 1$.
  Moreover, by definition of the update step
  \begin{align*}
    \frac1D\log\det\otheta' = \frac1D\log\det\otheta - \frac1{d_a}\log\det\left(d_a \rho_t^{(a)}\right).
  \end{align*}
  It follows that
  \begin{align*}
    f_x(\otheta') = f_x(\otheta) + \frac1{d_a}\log\det\left(d_a \rho_t^{(a)} \right).
  \end{align*}
  Lemma 5.1 in \cite{GGOW19} states that for any $d\times d$ positive semidefinite matrix~$Z$ of trace~$d$, the following inequality holds:
  \begin{align*}
    \log\det(Z) \leq -\frac16 \min \, \Bigl\{ \norm{Z - I_d}_F^2, 1 \Bigr\}.
  \end{align*}
  Applying this with $Z = d_a \rho_t^{(a)}$, we obtain
  \begin{align*}
    \frac1{d_a}\log\det\left(d_a \rho_t^{(a)} \right)
  &\leq -\frac16 \min\left\{ \norm{\rho_t^{(a)} - \frac{I_{d_a}}{d_a}}_F^2, \frac1{d_a} \right\} \\
  &\leq -\frac16 \min\left\{ \norm{\nabla_a f_x(\otheta)}_F^2, \frac1{d_a} \right\} \\
  &\leq -\frac16 \min\left\{ \frac{\norm{\nabla f_x(\otheta)}_F^2}{k-1}, \frac1{d_{\max}} \right\}.
  \end{align*}
  The equality follows from \cref{lem:gradient} and \cref{remark:gradient-everywhere}.
  In the last inequality we used that $\nabla_0 f(\otheta) = 0$ and at least one other component of the gradient is zero, as follows from \cref{lemma:flip-flop-update}, and that $a\in[k]$ is the index where the gradient has largest norm.
\end{proof}

\MW{Michael continue here.}
We now have all the tools we need to prove that, given appropriate initial conditions on the input samples, the flip-flop algorithm will converge quickly to the MLE.

\begin{prop}[Fast convergence from initial conditions]\label{lem:fast-convergence-initial-generic}
	If the samples $x_1, \ldots, x_n \in \R^D$ and the numbers $\lambda, r, \nu > 0$ satisfy the following conditions:
	\begin{enumerate}
		\item $f_x$ is $\lambda$-strongly geodesically convex on the ball $B_r(I_D)$, and
		\item $\norm{\nabla f_x (I_D)}_F \leq \nu^{1/2} < \dfrac{r \lambda}{2} \leq \min\left\{1, \sqrt{\frac{k}{\dmax}}\right\}.$

	\end{enumerate}
	Then, in $T = \dfrac{12k}{\lambda} \cdot \log\left( \dfrac{4 \nu^{1/2} \dmax^{1/2} }{\eps \lambda} \right)$ iterations, \cref{alg:flip-flop} outputs an estimator $\otheta$ such that the \MW{fixfix} equal-determinant factors $\htheta_a, \otheta_a$ satisfy
	$$ \DF(\htheta_a \ \Vert  \ \otheta_a) \leq \eps, \ \ \ \forall \ a \in [k].$$
\end{prop}

\begin{proof}
	The initial conditions above imply that \cref{lem:convex-ball} applies, and therefore we have that the sublevel set $\{ \Upsilon \ \mid \ f(\Upsilon) \leq f(I_D)  \}$ is contained in the ball $B_r(I_D)$. In particular, the above condition on the sublevel set implies that \cref{lem:tensor-descent-lemma} applies, and thus we have that each step of the flip-flop algorithm will decrease the value of the objective function in accordance with the requirements of \cref{lem:descent-sublevel-set}, with parameters $\alpha = 1/6k$, $\beta = \frac{k}{\dmax}$ and $\nu > 0$.

	Thus, after $T = \dfrac{12k}{\lambda} \cdot \log(\nu^{1/2}/\delta)$ steps, \cref{lem:descent-sublevel-set} guarantees us that we will encounter a point $\otheta$ such that
	$$ \norm{\nabla f_x(\otheta)}_F \leq \delta.$$
	Setting $\delta = \dfrac{\lambda \eps}{4 \dmax^{1/2}}$, when we find such a point with gradient $\leq \delta$, \cref{lem:convex-ball,cor:g-convex-components} imply that for each $a \in [k]$, the component-wise distance from $\otheta$ to $\htheta$ is bounded by
	$$ \DF(\htheta_a \ \Vert  \ \otheta_a) \leq \eps. $$
	This in particular implies that $\Dop(\htheta_a \ \Vert  \ \otheta_a) \leq \eps.$
\end{proof}

%-----------------------------------------------------------------------------
\subsection{Tensor flip-flop: Proof of Theorem~\ref{thm:tensor-flipflop}}
%-----------------------------------------------------------------------------

We are now ready to state the tensor flip-flop algorithm and prove its fast convergence to the MLE.

\begin{Algorithm}
\begin{description}
\item[\hspace{.2cm}\textbf{Input}:] Samples $\samp = (\samp_1, \ldots, \samp_n)$ where $\samp_i \in \R^D$ is sampled from a (unknown) centered normal distribution $\cN(0, \Sigma_1 \ot \dots \ot \Sigma_k)$, where each entry of $\samp_i$ is encoded in binary, with bit size $\le b$. Approximation parameter $0 < \eps < 1$, given in binary representation. \\[.3ex]

\item[\hspace{.2cm}\textbf{Output}:] $\otheta \in \SPD$ such that $\DF(\htheta_a \ \Vert  \ \otheta_a) < \eps$, for each $a \in [k]$, where $\htheta$ is the MLE for the precision matrix. \\[.3ex]

\item[\hspace{.2cm}\textbf{Algorithm}:]
\end{description}
\begin{enumerate}
\item\label{it:tensor-flip-flop step 1} Set $\otheta_a = I_a$ for each $a \in [k]$, and
$\delta = \dfrac{\eps}{8 \sqrt{k}}$.

\vspace{5pt}
%test
\item\label{it:tensor-flip-flop step 2} For $t=1,\dots,T = 24 k \cdot \log(1/\delta)$, repeat the following:

\vspace{5pt}

\begin{itemize}
\item Set $\rho = \dfrac{1}{nD} \cdot \otheta^{1/2} \left( \sum_{i=1}^n x_ix_i^T \right) \otheta^{1/2}.$

\item Compute each component of the gradient $\nabla f_{\samp}(\otheta_1, \ldots, \otheta_k)$, denoting $\nabla_a := \nabla_a f_{\samp}(\otheta_1, \ldots, \otheta_k)$, and find the index $a \in [k]$ for which $\norm{\nabla_a}_F$ is largest.
The components of the gradient can be computed as follows:

$$ \nabla_a f_x(\otheta) = \sqrt{d_a} \left( \rho^{(a)} - \dfrac{\tr[\rho]}{d_a} I_{d_a} \right) $$

\vspace{5pt}

\item
If $\norm{\nabla_a}_F < \delta$, output $\otheta$ and return.
% S\Bigl(\frac {I_{n_{k^{t}}}{n_{k^{t}}}} \big\Vert \rho^{t}_{k^{t}}\Bigr)$ is largest.

\vspace{5pt}

\item Otherwise, set $\otheta_a \leftarrow  \dfrac{1}{d_a} \cdot \otheta_a^{1/2} (\rho^{(a)})^{-1} \cdot \otheta_a^{1/2}$.
\end{itemize}
\end{enumerate}
\caption{Tensor flip-flop algorithm}\label{alg:tensor-flip-flop}
\end{Algorithm}




\begin{lemma}[Initial conditions for tensor normal model]\label{lem:tensor-initial-conditions}
	There exist absolute constants $\Gamma > 0,$ $4 \geq \gamma > 0$ such that the following holds.
	When the number of samples $n \geq \Gamma \cdot k^2 \cdot \dmax^3/D$, with probability at least $1 - k^2 \cdot \left( \dfrac{\sqrt{nD}}{k \dmax} \right)^{-\Omega(\dmin)} - 2k \cdot e^{- \Omega(nD/k \dmax^2)}$ we have that the following conditions hold:
	\begin{enumerate}
		\item $\norm{\nabla f_x(I)}_F < \dfrac{\gamma}{4 \sqrt{(k+1)\dmax}}$
		\item $f_x$ is $\frac{1}{2}$-strongly geodesically convex at
		$B_r(I_D)$, where $r = \dfrac{\gamma}{\sqrt{(k+1) \dmax}}$
	\end{enumerate}
\end{lemma}

\begin{proof}
	The lemma follows from the observation that \cref{prop:gradient-bound} implies condition 1, and \cref{thm:ball-convexity} implies condition 2. So all we need to do is to check the parameters.

	By \cref{thm:ball-convexity}, if we set $\gamma = c$ and if the number of samples $n \geq C k \dfrac{\dmax^2}{D}$, where $c, C > 0$ are the constants from \cref{thm:ball-convexity}, then the second condition fails to hold with probability at most
	$$k^2 \cdot \left( \dfrac{\sqrt{nD}}{k \dmax} \right)^{-\Omega(\dmin)}.$$


	By \cref{prop:gradient-bound} with parameter
	$\eps = \frac{\gamma}{100 k \dmax^{1/2}}$,
	if the number of samples satisfies
	$n \geq \dfrac{10^4 k^2 \dmax^3}{\gamma^2 \cdot D}$ then the first condition fails to hold probability at most
	$$2 k \cdot \exp\left(- \frac{n D \gamma^2}{128 (k+1) \dmax^2}\right) = 2k \cdot e^{- \Omega(nD/k \dmax^2)}.$$

	Letting $\Gamma = \max\{10^4/\gamma^2, C \}$, having $n \geq \Gamma k^2 \dmax^3/D$ samples gives a sample upper bound that holds for both situations above.
	Thus, by the union bound, one of the conditions 1 or 2 fails with probability at most
	$$ k^2 \cdot \left( \dfrac{\sqrt{nD}}{k \dmax} \right)^{-\Omega(\dmin)} + 2k \cdot e^{- \Omega(nD/k \dmax^2)}.$$
	This concludes the proof.
\end{proof}

\begin{customthm}{\ref{thm:tensor-flipflop}}[Tensor flip-flop convergence, restated]
\TensorFlop
\end{customthm}


\begin{proof}
	Let $\Gamma$ be the universal constant from \cref{lem:tensor-initial-conditions}, and pick $C \geq \Gamma$. When the number of samples is $n = C \cdot k^2 \cdot \dmax^3/D$, with probability
	$$ 1 - k^2 \cdot \left( \dfrac{\sqrt{nD}}{k \dmax} \right)^{-\Omega(\dmin)} - 2k \cdot e^{- \Omega(nD/k \dmax^2)}$$
	we have that the hypothesis of \cref{lem:tensor-initial-conditions} applies, which implies that there exists a constant $4 \geq \gamma > 0$ such that our objective function $f_x$ is $\frac{1}{2}$-strongly geodesically convex at a ball $B_r(I)$ for $r = \dfrac{\gamma}{\sqrt{(k+1)\dmax}}$ and $\norm{\nabla f_x(I)}_F \leq \nu^{1/2}:=\dfrac{\gamma}{4 \sqrt{(k+1)\dmax}} < \sqrt{\frac{k}{\dmax}}$.

Thus, by \cref{lem:fast-convergence-initial-generic}, we have that in $T = 24k \cdot \log(8 \nu \dmax^{1/2}/\eps) = O(k \log( 1/\eps))$ iterations the flip-flop algorithm converges to an estimator such that $\DF(\htheta_a \ \Vert  \ \otheta_a) \leq \eps$ for all $a \in [k]$. This concludes the proof.
\end{proof}

%-----------------------------------------------------------------------------
\subsection{Matrix flip-flop: Proof of Theorem~\ref{thm:matrix-flipflop}}
%-----------------------------------------------------------------------------

We are now ready to state the matrix flip-flop algorithm and prove its fast convergence to the MLE.
The proof strategy of this section is a bit different from the tensor normal model case, as now the number of samples is not large enough to guarantee that the initial conditions from \cref{lem:fast-convergence-initial-generic} will apply with high probability.

However, we can proceed as in~\cite{FM20} and use the results from~\cite{KLR19} to show that the MLE is in a constant size operator norm ball around the identity. Hence, by \cref{thm:ball-convexity}, the log-likelihood is strongly geodesically convex in a small geodesic ball around the MLE. This implies~\cite[Lemma 4.7]{FM20} that any point with sufficiently small gradient of the log-likelihood function is contained in a sublevel set on which the log-likelihood is strongly geodesically convex. Such a point can be found by applying the flip-flop algorithm for polynomially many iterations, at which point \cref{lem:descent-sublevel-set} applies to yield an $\eps$-minimizer in $O(\log(1/\eps))$ further iterations.

Throughout this subsection, we will assume that $d_1 \leq d_2$, just as we assumed in the introduction.
For convenience of the reader, we will also remind in each statement about this assumption, whenever we need it.


\begin{Algorithm}
\begin{description}
\item[\hspace{.2cm}\textbf{Input}:] Samples $\samp = (\samp_1, \ldots, \samp_n)$ where $\samp_i \in \R^{d_1 \times d_2}$ is sampled from a (unknown) centered normal distribution $\cN(0, \Sigma_1 \ot \Sigma_2)$, where each entry of $\samp_i$ is encoded in binary, with bit size $\le b$. Approximation parameter $\eps > 0$, given in binary representation. \\[.3ex]

\item[\hspace{.2cm}\textbf{Output}:] $\otheta \in \SPD$ such that $\Dop(\htheta_a \ \Vert  \  \otheta_a) < \eps$, for $a \in \{1,2\}$, where $\htheta$ is the MLE for the precision matrix $\Theta$. \\[.3ex]

\item[\hspace{.2cm}\textbf{Algorithm}:]
\end{description}
\begin{enumerate}
\item\label{it:flip-flop step 1 matrix} Let $\gamma, \lambda$ be the constants from \cref{lem:matrix-normal-initial-conditions}. Set $\otheta_a = I_a$ for each $a \in \{1,2\}$, and $\delta = \dfrac{\eps \lambda}{4 \sqrt{d_2}}$.

\vspace{5pt}

\item\label{it:flip-flop step 2 matrix} For $t=1,\dots,T = 30 d_2 \cdot \left( \dfrac{32}{\lambda \gamma} \right)^2 + \dfrac{6}{\lambda} \cdot \log\left( \dfrac{1}{d_2 \cdot \delta^2} \right)$, repeat the following:

\vspace{5pt}

\begin{itemize}
\item Set $\rho = \dfrac{1}{n d_1 d_2} \otheta^{1/2} \cdot \left( \sum_{i=1}^n x_i x_i^T \right) \cdot \otheta^{1/2}$.

\item Compute each component of the gradient $\nabla f_{\samp}(\otheta_1, \otheta_2)$, denoting $\nabla_a := \nabla_a f_{\samp}(\otheta_1, \otheta_2)$, and find the index $a \in \{1,2\}$ for which $\norm{\nabla_a}_{F}$ is non-zero.
The components of the gradient can be computed as follows:

$$ \nabla_a f_x(\otheta) = \sqrt{d_a}\left( \rho^{(a)} - \dfrac{\tr[\rho]}{d_a} I_{d_a} \right) $$

\vspace{5pt}

\item
If $\norm{\nabla_a}_{F} < \delta$, output $\otheta$ and return.

\vspace{5pt}

\item Otherwise, set $\otheta_a \leftarrow \dfrac{1}{d_a} \cdot \otheta_a^{1/2} \cdot (\rho^{(a)})^{-1} \cdot \otheta_a^{1/2}$.
\end{itemize}
\end{enumerate}
\caption{Matrix flip-flop algorithm}\label{alg:flip-flop matrix}
\end{Algorithm}

\begin{remark} Though \cref{alg:flip-flop matrix} looks different than the description in \cref{subsec:flip-flop}, it is identical exept in \cref{alg:flip-flop matrix} we check the stopping criterion at every iteration rather than every other one. The stopping criteria agree because for the quantity $\Upsilon$ described in \cref{subsec:flip-flop}, $d_F(\Upsilon, \otheta_2^{-1})$ is precisely $\frac{1}{\sqrt{d_2}} \|\nabla_2 f_\samp(\otheta_1, \otheta_2) \|_F$ provided $\tr \rho = 1$ (which holds at every iteration apart from the first by \cref{lemma:flip-flop-update}).
\end{remark}

\begin{lemma}[Initial conditions for matrix normal model]\label{lem:matrix-normal-initial-conditions}
    Let $d_1 \leq d_2$.
    There exist absolute constants $\Gamma > 0$ and $\gamma, \lambda \in (0,1]$ such that the following holds.
	When the number $n$ of samples satisfies
	$$n \geq \Gamma \cdot \dfrac{d_2}{d_1} \cdot \max\left\{ \log d_2, \ \dfrac{\log^2 d_1}{\eps^2} ,\eps^2 \right\},$$ \MW{fix assumptions}
	then with probability at least $1 - e^{- \Omega(d_1 \eps^2)}$ \MW{fix prob} the following conditions hold:
	\begin{enumerate}
		\item $|\nabla_0 f_x(I)| \leq \frac{\gamma}{2}$
		\item The MLE's $\htheta_1, \htheta_2$ satisfy $\ \ \norm{\htheta_i - I_{d_i}}_{\op} \leq \gamma/2, \ \ $ for $i \in \{1,2\}$
		\item $f_x$ is $\lambda$-strongly geodesically convex at any $\Theta \in \SPD$ such that $\norm{\log \Theta}_{\op} \leq \gamma$.
	\end{enumerate}
\end{lemma}

\begin{proof}
By \cref{prp:xnorm} and the fact that $|\nabla_0 f_x(I)| = |1 - \norm{x}_2^2/nD|$, Condition 1 holds with probability $1 - 2e^{-\gamma^2 nD/32}$.

If $\Gamma$ is larger than the universal constant $C$ of \cref{thm:matrix-normal}, this theorem implies that condition 2 fails to hold with probability at most $O(e^{-\Omega(d_1 \eps^2)})$.
Finally, if $\Gamma$ is larger than the universal constant $C$ of \cref{cor:matrix-convexity} and $\gamma$ is smaller than the universal constant $c$ from \cref{cor:matrix-convexity}, this corollary implies that there is a universal constant $\lambda \in (0,1)$ such that condition 3 fails to hold with probability at most $e^{-\Omega(d_1 \eps^2)}$.
Thus, choosing $\Gamma$ larger than the foregoing universal constants, as well as $\Gamma > 4/\gamma^2$, by a union bound we satisfy the hypotheses for all three results with the desired probability.
\end{proof}



To prove that flip-flop converges once the initial conditions are satisfied, we need the following general lemma \cite[Lemma 4.7]{FM20} on strongly geodesically convex functions, which tells us that once the gradient is small then the point must be inside a sublevel set of our function which is contained in a ball where our function is strongly convex. Though their lemma is stated for the manifold of positive definite matrices of determinant one, the proof uses no specific properties of this manifold beyond the fact that it is a Hadamard manifold. Thus their lemma holds for the manifold $\SPD$ as well.

\begin{lemma}[\cite{FM20}]\label{lem:gradient-strong-convexity-fm}
Suppose that $f : \SPD \to \R$ is geodesically $\lambda$-strongly convex on the geodesic ball of radius $\kappa$ about $\theta$, and that $\nabla f(\theta) = 0$.
If $\norm{\nabla f(\Upsilon)}_F < \lambda \kappa/8$, then $\Upsilon$ is contained in a sublevel set of $f$ on which $f$ is geodesically $\lambda$-strongly convex.
\end{lemma}
Now we must show that the flip-flop algorithm reaches a point with small enough gradient relatively quickly, which is given by the following lemma, which follows the analysis given by~\cite{GGOW19}:

\begin{lemma}\label{lem:flip-flop-sinkhorn}
Let $d_1 \leq d_2$.
Given $\delta >0$ and samples $x = (x_1, \ldots, x_n)$, let $f^* := \min_{\otheta \in \SPD} f_x(\otheta)$ be the minimizer of the log-likelihood function. The flip-flop algorithm, in at most
$$T = 12 (f_x(I_D) - f^* + |\nabla_0 f_x(I)|) \cdot \max\left\{\dfrac{d_2}{2}, \dfrac{1}{\delta^2}\right\} \ \text{ iterations,}$$
outputs $\otheta$ such that $\norm{\nabla f_x(\otheta)}_F < \delta$.
\end{lemma}
\begin{proof}
Let $\otheta^{(i)}$ be the iterates of the flip-flop algorithm, where $\otheta^{(0)} = I_D$.
After we perform the first normalization step of the flip-flop algorithm, we obtain a scaling $\otheta^{(1)}$ such that $\nabla_0 f_x (\otheta^{(1)}) = 0$.
By \cref{lemma:flip-flop-update}, after each subsequent step, that is $\otheta^{(i)}$ for $i \geq 2$, we maintain that $\nabla_0 f_x (\otheta^{(i)}) = 0$.
Thus, \Cref{lem:tensor-descent-lemma} applies and we obtain that
$$ f_x(\otheta^{(1)}) - f_x(\otheta^{(T)}) \geq \dfrac{1}{12} \sum_{i=1}^{T-1} \min\left\{ \frac{2}{d_2} , \norm{\nabla f_x(\otheta^{(i)})}_F^2 \right\}. $$
Let $\alpha^2 := \dfrac{1}{nD} \sum_{i=1}^n x_i^T x_i$.
\Cref{lem:tensor-descent-lemma} and $\nabla_0 f_x(\alpha I) = 0$ imply $f_x(\alpha I_D) - f_x(\otheta^{(1)}) \geq 0$.
Moreover, since $f_x(I_D) - f_x(\otheta^{(1)}) = f_x(I_D) - f_x(\alpha I_D) + f_x(\alpha I_D) - f_x(\otheta^{(1)})$, we have
$$ f_x(I_D) - f_x(\otheta^{(1)}) \geq f_x(I_D) - f_x(\alpha I_D) \geq - |\nabla_0 f_x(I)|. $$
The above inequalities imply that
$$ f_x(I_D) - f^* \geq f_x(I_D) - f_x(\otheta) \geq - |\nabla_0 f_x(I)| + \dfrac{1}{12} \sum_{i=1}^{T-1} \min\left\{ \frac{2}{d_2} , \norm{\nabla f_x(\otheta^{(i)})}_F^2 \right\}. $$
Thus, for some $1 \leq t \leq T = 12 (f_x(I_D) - f^* + |\nabla_0 f_x(I)|) \cdot \max\left\{\dfrac{d_2}{2}, \dfrac{1}{\delta^2}\right\}$ steps, we must reach a scaling $\otheta^{(t)}$ such that $\norm{\nabla f_x(\otheta^{(t)})}_F^2 < \delta^2$.
\end{proof}

We are now ready to prove our main theorem of this subsection: it says that under the hypotheses of \cref{thm:matrix-normal}, the flip-flop converges exponentially fast to the minimizer.

\begin{customthm}{\ref{thm:matrix-flipflop}}[Matrix flip-flop convergence, restated]
\MatrixFlop
\end{customthm}

\begin{proof}
If we take $\Gamma$ to be the universal constant according to \cref{lem:matrix-normal-initial-conditions}, with probability at least $1 - e^{- \Omega(d_1 \eps^2)}$ we have that the conditions of \cref{lem:matrix-normal-initial-conditions} are satisfied.
Thus, there exist constants $\lambda, \gamma \in (0, 1]$ such that:
\begin{enumerate}
	\item $|\nabla_0 f_x(I)| \leq \frac{\gamma}{2}$
	\item The MLE's $\htheta_1, \htheta_2$ satisfy $\ \ \norm{\htheta_i - I_{d_i}}_{\op} \leq \gamma/2, \ \ $ for $i \in \{1,2\}$
	\item $f_x$ is $\lambda$-strongly geodesically convex at any $\Theta \in \SPD$ such that $\norm{\log \Theta}_{\op} \leq \gamma$.
\end{enumerate}
In particular, we have that our function $f_x$ is lower bounded by
$$f^* := f_x(\htheta_1 \ot \htheta_2) = 1 - \dfrac{1}{D} \log\det(\htheta_1 \otimes \htheta_2) \geq 1 - \dfrac{1}{D} \log\det((1+\gamma/2) I_D) \geq - \gamma/2,  $$
and we also know that $f_x(I_D) = 1 + \nabla_0 f_x(I_D) \leq 1 + \gamma/2$. Thus, by \cref{lem:flip-flop-sinkhorn} with parameter $\lambda \gamma/32 \sqrt{d_2}$, we know that in at most
$30  \cdot d_2 \cdot \left(32/\lambda \gamma\right)^2 = O(d_2)$ steps, we will find a point $\otheta$ with $\norm{\nabla f_x(\otheta)}_F < \lambda \gamma /32 \sqrt{d_2}$.

For the $\otheta$ we just obtained, we will now show that \cref{lem:gradient-strong-convexity-fm} applies.
Note that conditions 2 and 3 imply that $f_x$ is $\lambda$-strongly geodesically convex in an operator norm ball of radius $\gamma/2$ around the MLE $\htheta$.
Thus, \cref{remark-strong-convexity-balls} implies that $f_x$ is $\lambda$-strongly geodesically convex in the ball $B_\kappa(\htheta)$ with $\kappa = \dfrac{\gamma}{2\sqrt{3d_2}}$.
Since $\norm{\nabla f_x(\otheta)}_F < \dfrac{\lambda \gamma}{32 \sqrt{d_2}} < \kappa \lambda/8$, the conditions of \cref{lem:gradient-strong-convexity-fm} apply.

Therefore, $f_x$ is $\lambda$-strongly convex in a ball that contains the sublevel set $\{ \Upsilon \ \mid \ f_x(\Upsilon) \leq f_x(\otheta) \}$. By \cref{lem:tensor-descent-lemma}, \cref{lem:descent-sublevel-set} applies with $\alpha = \frac{1}{6}, \beta = 1/d_2$, and $0 \leq \nu \leq \beta$. Thus in $T = \dfrac{6}{\lambda} \log_2(\nu/\delta^2)$ we obtain a  $\otheta'$ such that $\norm{\nabla_x(\otheta')}_F \leq \delta$. Setting $\delta = \dfrac{\eps \lambda}{4 \sqrt{d_2}}$, \cref{lem:convex-ball,cor:g-convex-components} imply that $\Dop(\htheta_a \ \Vert  \ \otheta_a') \leq \eps$ for $a \in \{1, 2\}$. In particular, \cref{alg:flip-flop matrix} correctly returns a scaling $\eps$-close to the MLE. The number of iterations follows from plugging in the chosen values for $\delta, \nu$.
\end{proof}

%=============================================================================
\section{Lower bounds}\label{sec:lower}
%=============================================================================
In this section we discuss known lower bounds for estimating unstructured precision matrices (i.e., the case $k= 1$ of the tensor normal model). Afterwards we prove a new lower bound on the matrix normal model.

%-----------------------------------------------------------------------------
\subsection{Lower bounds for unstructured precision matrices}
%-----------------------------------------------------------------------------
Here we briefly recall and, for completeness, prove well-known lower bounds on the accuracy of any estimator for the precision matrix in the Frobenius and operator error from independent samples of a Gaussian. The lower bounds follow from Fano's method and the relationship between the Frobenius error and the relative entropy (which is proportional to Stein's loss).
% We then use these bounds to show bounds on the relative Frobenius and relative operator error.
Informally, both bounds imply that no estimator for a $d\times d$ precision matrix from $n$ samples can have accuracy smaller than $d/\sqrt{n}$ (resp.\ $\sqrt{d/n}$) in Frobenius error or relative Frobenius error (resp.\ operator norm error or relative operator norm error) with probability more than $1/2$.
%\CF{define minimax rate}


\begin{prop}[Frobenius and operator error]\label{prp:standard-lower}
There is $c > 0$ such that the following holds.
Let $X \in \R^{d \times n}$ denote $n$ independent random samples from a Gaussian with precision matrix $\Theta \in \PD(d)$.
Consider any estimator $\htheta = \htheta(x)$ for the precision matrix~$\Theta$, and let $B\subset \PD(d)$ denote the ball about $I_d$ of radius $1/2$ in the operator norm.
\begin{enumerate}
\item Let $\delta^2 = c \, \min \left\{1,d^2/n\right\}$. Then,
\begin{align}
\sup_{\Theta \in B} \Pr\left[ \| \htheta - \Theta\|_F \geq \delta\right] \geq \frac{1}{2}.\label{eq:frob-lower}
\end{align}
\item Let $\delta^2 = c \, \min \left\{1,d/n\right\}$. Then,
 \begin{align}
\sup_{\Theta \in B} \Pr\left[ \| \htheta - \Theta\|_{\op} \geq \delta\right] \geq \frac{1}{2}. \label{eq:op-lower}
\end{align}
\end{enumerate}
As a consequence, we have
\begin{align*}
  \sup_{\Theta \in B}\E[\| \htheta - \Theta\|_F^2] =\Omega\left( \min \left\{\frac{d^2}{n},1\right\}\right)
\text{ and } \sup_{\Theta \in B}\E[\| \htheta - \Theta\|_{\op}^2] = \Omega\left( \min \left\{\frac{d}{n},1\right\}\right).
\end{align*}
\end{prop}
The proof uses Fano's inequality with mutual information bounded by relative entropy, as in \cite{yang1999information}.

\begin{lemma}[Fano's inequality]\label{lem:fano}
Let $\{P_i\}_{i \in [m]}$ be a finite set of probability distributions over a set $\mathcal X$, and let $T: \mathcal X \to [m]$ be an estimator for $i$ from a sample of $P_i$. Then
\[ \max_{i\in [m]} \Pr_{X \sim P_i}[T(X) \neq i] \geq 1 - \frac{ \log 2 + \max_{i,j \in [m]} \DKL(P_i\Vert  P_j)}{\log m}. \]
\end{lemma}

\begin{proof}[Proof of \cref{prp:standard-lower}]
We first prove \cref{eq:frob-lower}, the lower bound on estimation in the Frobenius norm.
We begin by the standard reduction from estimation to testing.
Let $V_0$ be a $1$-separated set in the Frobenius ball $B_F$ of radius $1$ in the $d\times d$ symmetric matrices, i.e., the set $B_F = \{A: A \text{ Symmetric}, \|A\|_F \leq 1\}$.

We may take $V_0$ to have cardinality $m \geq 2^{d(d+1)/2}$ because $B_F$ is a Euclidean ball of radius $1$ in the linear subspace of $d\times d$ symmetric matrices, which has dimension $d(d+1)/2$, and hence any maximal Frobenius $1/2$-packing (collection of disjoint radius $1/2$ Frobenius balls) in $B_F$ has cardinality at least $2^{d(d+1)/2}$.
Let $0 \leq \delta \leq 1/2$, and let $V = I_d + \delta V_0 = \{I_d + \delta v: v \in V_0\}$.
Write $V = \{\Theta_1, \dots, \Theta_m\}$.
Note that $V$ is contained within the operator norm ball $B$.
Let $P_i =\mathcal{N}(0, \Theta^{-1}_i)^{\ot n}$ for $i\in[m]$, and define the estimator~$T$ by
\[ T(x) = \argmin_{i \in [m]} \|\Theta_i - \htheta(x)\|_F. \]
Then, because $V$ is $2\delta$-separated,
\begin{align}\label{eq:pr vs est}
  \Pr_{X \sim P_i} \left[T(X) = i\right] \geq \Pr\left[\|\htheta -  \Theta_i\|_F \leq \delta\right].
\end{align}
In order to apply Fano's inequality, we use the well-known fact that $\DKL(P_i\Vert  P_j) = n \DKL(\mathcal{N}(0, \Theta_i^{-1})\ \Vert  \ \mathcal{N}(0, \Theta_j^{-1})) = O(n\DF(\Theta_j \ \Vert  \ \Theta_i)^2)$ when $\Theta_i^{-1}\Theta_j$ has eigenvalues uniformly bounded away from zero by the proof of \cref{prop:dissimilarities}.
This condition on the eigenvalues holds because $I_d/2 \preceq \Theta_j, \Theta_j \preceq 3I_d/2$ for $i,j \in [m]$ by our assumption that $\delta \leq 1/4$.

Moreover, for $i \in [m]$, we have $\|\Theta_i^{-1}\|_{\op} \leq 2$ and so $\DF(\Theta_j\Vert  \Theta_i) = O( \|\Theta_i - \Theta_j\|_F) = O(\delta)$ by \cref{eq:D_F rel vs abs}.
Thus we have $\DKL(P_i\Vert  P_j) \leq Cn \delta^2$ for some absolute constant $C$.
Then, by \cref{lem:fano},
\begin{align*}
  \min_{i \in [m]} \Pr_{X \sim P_i}[T(X) = i] \leq \frac{  \log 2 + C n \delta^2}{d(d+1)(\log 2)/2 }.
\end{align*}
If $\delta^2 = c \, \min\{ \frac{d^2}{n}, 1\}$, the right-hand side of the inequality above is bounded by $\frac{1}{2}$ and the assumption $\delta \leq 1/4$ is satisfied provided $c$ is a small enough absolute constant.
In view of \cref{eq:pr vs est}, it follows that
\[ \min_{i \in [m]} \Pr\left[ \|\htheta - \Theta_i\|_F \leq \delta\right] \leq 1/2. \]
Because $V \subset B$, this proves \cref{eq:frob-lower}.

To obtain \cref{eq:op-lower}, the lower bound in operator norm, instead start with a packing $V_0$ of the unit operator norm ball of cardinality $m \geq 2^{d(d+1)/2}$ and define $V = \{ \Theta_1, \dots, \Theta_m \}$ as above.
We modify the proof by bounding $\DKL(P_i \Vert  P_j) = O(n \| \Theta_i - \Theta_j\|_F^2) = O(n d \|\Theta_i - \Theta_j\|_{\op}^2) \leq C nd \delta^2.$
Proceeding as before, we find that for $\delta = c \, \min \{\frac{d}{n}, 1\}$,
\[ \min_{i \in [m]} \Pr\left[ \|\htheta - \Theta_i\|_{\op} \leq \delta\right] \leq 1/2. \]
Again, we have $V \subset B$, so \cref{eq:op-lower} follows.
\end{proof}

We remark that the above proof shows the necessity of a scale-invariant dissimilarity measure to obtain error bounds that are independent of the ground truth precision matrix $\Theta$.
Indeed, replacing the packing $V$ by $\kappa V$ for $\kappa \to \infty$ in the proof shows that $\sup_{\Theta \in \kappa B} \Pr[ \| \htheta - \Theta\|_F \geq \kappa \delta ] \geq \frac{1}{2}$.
That is, no fixed bound can be obtained with probability $1/2$.

We now use the result just obtained to prove bounds on the relative Frobenius and operator error. Because $I_d/2 \preceq \Theta \preceq 3I_d/2$ for $\Theta \in B$, the bounds $\|\Theta - \htheta\|_F \leq \|\Theta\|_{\op} \DF(\htheta\Vert  \Theta)$ and $\|\Theta - \htheta\|_{\op} \leq \|\Theta\|_{\op} \Dop(\htheta\Vert  \Theta)$ together with \cref{prp:standard-lower} imply the following corollary.
\begin{corollary}[Relative Frobenius and operator error]\label{prp:relative-lower}
There is $c > 0$ such that the following holds for $X, \htheta, B$ as in \cref{prp:standard-lower}.
\begin{enumerate}
\item Let $\delta^2 = c \, \min \left\{1,d^2/n\right\}$. Then
\begin{align}
\sup_{\Theta \in B} \Pr\left[ \DF(\htheta\Vert  \Theta)  \geq \delta\right] \geq \frac{1}{2}.\label{eq:df-lower}
\end{align}
\item Let $\delta^2 = c \, \min \left\{1,d/n\right\}$. Then
 \begin{align}
\sup_{\Theta \in B} \Pr\left[ \Dop(\htheta\Vert  \Theta) \geq \delta\right] \geq \frac{1}{2}. \label{eq:dop-lower}
\end{align}
\end{enumerate}
As a consequence, we have
\begin{align*}
\sup_{\Theta \in B}\E[\DF(\htheta\Vert  \Theta)^2] =\Omega\left( \min \left\{\frac{d^2}{n},1\right\}\right)
\text{ and }
\sup_{\Theta \in B}\E[\Dop(\htheta\Vert  \Theta)^2] = \Omega\left( \min \left\{\frac{d}{n},1\right\}\right).
\end{align*}
\end{corollary}

%-----------------------------------------------------------------------------
\subsection{Lower bounds for the matrix normal model}
%-----------------------------------------------------------------------------
%If $\Theta_2$ is identity, then $X$ is simply $nd_2$ independent samples from a Gaussian with precision matrix $\Theta_1$.

If $\Theta_2$ is known, then we can compute $(I \ot \Theta_2^{1/2} )X$, which is distributed as $nd_2$ independent samples from a Gaussian with precision matrix $\Theta_1$. In this case, one can estimate $\Theta_1$ in operator norm with an RMSE rate of $O(\sqrt{ d_1/ n d_2})$. One could hope that this rate holds for $\Theta_1$ even when $\Theta_2$ is not known. Here we show that, to the contrary, the rate for $\Theta_1$ cannot be better than $O(\sqrt{d_1/ n \min(n d_1, d_2)})$. Thus, for $d_2 > n d_1$, it is impossible to estimate $\Theta_1$ as well as one could if $\Theta_2$ were known. Note that, in this regime, there is no hope of recovering $\Theta_2$ even if $\Theta_1$ is known.

%Here we show that our bounds for the matrix normal model are best possible for estimating the individual matrices $\Theta_a$. The bound for $\Theta_2$ is clearly best possible because it is what we would obtain even if $\Theta_1$ were known, but in fact even the bound on $\Theta_1$ is tight.

\begin{theorem}[Lower bound for matrix normal models]\label{thm:matrix-lower}
There is $c > 0$ such that the following holds. Let $\htheta_1$ be any estimator for $\Theta_1$ from a tuple $X$ of $n$ samples of the matrix normal model with precision matrices $\Theta_1, \Theta_2$. Let $B\subset \PD(d_1)$ denote the ball about $I_{d_1}$ of radius $1/2$ in the operator norm.
\begin{enumerate}
\item \label{it:frob-lower} Let $\delta^2 = c \, \min \left\{1,\frac{d_1^2}{n \min \{n d_1, d_2\}}\right\}$. Then
\begin{align}
\sup_{\Theta_1 \in B, \Theta_2 \in \PD(d_2)} \Pr\left[ \DF(\htheta_1\Vert  \Theta_1)  \geq \delta\right] \geq \frac{1}{2}.
\end{align}
\item\label{it:op-lower} Let $\delta^2 = c \, \min \left\{1,\frac{d_1}{n \min \{n d_1, d_2\}}\right\}$. Then
 \begin{align}
\sup_{\Theta_1 \in B, \Theta_2 \in \PD(d_2)} \Pr\left[ \Dop(\htheta_1\Vert  \Theta_1) \geq \delta\right] \geq \frac{1}{2}. \label{eq:dop-lower-matrix-normal}
\end{align}
\end{enumerate}
As a consequence, we have
\begin{align*}\sup_{\Theta_1 \in B, \Theta_2 \in \PD(d_2)}\E[\DF(\htheta_1\Vert  \Theta_1)^2] &=\Omega\left( \min \left\{\frac{d_1^2}{n \min \{n d_1, d_2\}},1\right\}\right)\\
\text{ and } \sup_{\Theta_1 \in B, \Theta_2 \in \PD(d_2)}\E[\Dop(\htheta_1\Vert  \Theta_1)^2] &= \Omega\left( \min \left\{\frac{d_1}{n \min \{n d_1, d_2\}},1\right\}\right).\end{align*}
\end{theorem}

Intuitively, the above theorem holds because we can choose $\Sigma_2$ to zero out all but $nd_1$ columns of each $X_i$, which allows access to at most $n \cdot n d_1$ samples from a Gaussian with precision $\Theta_1$. However, this does not quite work because $\Sigma_2$ would not be invertible and hence the precision matrix $\Theta_2$ would not exist. We must instead choose $\Sigma_2$ to be approximately equal to a random projection of rank $n d_1$. The resulting construction allows us to deduce the same lower bounds for estimating $\Theta_1$ as the Gaussian case with at most $n\min \{d_2, n d_1\}$ independent samples.

One might ask why the rank of the random projection cannot be taken to be even less than $n d_1$, yielding an even stronger bound. If the rank is less than $n d_1$, then the support of $\Sigma_2$ can be estimated. This would allow one to approximately diagonalize $\Sigma_2$ so that the $n$ samples can be treated as $nd_2$ independent samples in $\R^{d_1}$, yielding the rate $\sqrt{d_1 / n d_2}$ using, e.g., Tyler's M estimator. We now state the lower bound.


\begin{lemma}\label{lem:reduce-lower}
Let $X$ denote a tuple of $n$ samples from the matrix normal model with precision matrices $\Theta_1, \Theta_2$.
Let $Y$ be a tuple of $n\min\{nd_1, d_2\}$ Gaussians on $\R^{d_1}$ with precision matrix $\Theta_1$.
Let $\widehat{\Theta}_1(X)$ be any estimator for $\Theta_1$.
For every $\delta > 0$, there is a distribution on $\Theta_2$ and an estimator $\tilde{\Theta}(Y)$ such that the distribution of $\widehat{\Theta}_1(X)$ and the distribution of $\tilde{\Theta}(Y)$ differ by at most $\delta$ in total variation distance.
\end{lemma}
\begin{proof}
If $d_2 \leq nd_1$, then setting $\Theta_2 = I_{d_2}$ shows that $\htheta_1$ has access to precisely $n d_2$ samples from a Gaussian $\R^{d_1}$ with precision matrix $\Theta_1$.
Thus we may take $\tilde{\Theta} = \htheta_1$ in that case, completing the proof. The harder case is $d_2 > n d_1$.

For intuition, let $B$ be any $d_2\times d_2$ matrix such that the last $d_2 - nd_1$ columns are zero.
Given access to the tuple $X$ of $n$ samples $\sqrt{\Sigma_1} Z_i B^T$, where $Z_i$ are i.i.d standard Gaussian $d_1\times d_2$ matrices, clearly $\widehat{\Theta}_2$ has access to at most $n^2 d_1$ samples of the Gaussian on $\R^{d_1}$ with precision matrix $\Theta_1$ because $Z_i B^T$ depends only on the first $d_1$ columns of each $Z_i$.

However, we must supply \emph{invertible} $B$ in order for $\Theta_2 = (BB^T)^{-1}$ to exist.
Let $\delta \geq 0$.
Let $B_\delta$ be the random matrix obtained by choosing the first $nd_1$ columns of $B_\delta$ uniformly at random among the collections of $nd_1$ orthonormal vectors in $\R^{d_2}$.
Let the remaining entries be i.i.d uniform in $[-\delta, \delta]$ (the precise distribution of the remaining entries does not matter as long as they are independent, continuous, and small).
Let $Y_\delta:=(\sqrt{\Sigma_1} Z_1 B_\delta^T, \dots, \sqrt{\Sigma_1} Z_n B_\delta^T)$ denote the resulting random variable with $B_\delta$ and $X$ chosen independently.
If $\delta = 0$, then, by the argument above, with access to the random variable $Y_\delta:=(\sqrt{\Sigma_1} Z_1 B_\delta^T, \dots, \sqrt{\Sigma_1} Z_n B_\delta^T)$ the estimator $\widehat{\Theta}_1$ has access to at most $n^2d_1$ samples of a Gaussian on $\R^{d_1}$ with precision matrix $\Theta_1$.
We claim that as $\delta \to 0$, the distribution of $Y_\delta$ tends to that of $Y_0$ in total variation distance.
Thus the distribution of $\widehat{\Theta}_1(Y_\delta)$ converges to that of $\widehat{\Theta}_1(Y_0)$ in total variation.
Since $Y_0$ only depends on $n^2d_1$ samples to the Gaussian on $\R^{d_1}$ with precision matrix $\Theta_1$, which we call $Y$, defining $\tilde{\Theta}(Y) = \widehat{\Theta}_1(Y_0)$ proves the theorem.
\footnote{Actually, as $B$ has a probability zero chance of being singular, the final family of densities $Y'_\delta$ we will use is $Y_\delta$ conditioned on $B$ being invertible.
As $B$ is invertible with probability $1$ for $\delta > 0$, the total variation distance between $Y'_\delta, Y_\delta$ is zero for all $\delta > 0$ and hence $Y'_\delta$ converges to $Y_0$ in total variation distance provided $Y_\delta$ does.}

It remains to prove that $Y_\delta$ converges to $Y_0$ in total variation distance.
First note that $Y_\delta = Y_0 + \delta W$ where $W_i = \sqrt{\Theta_1} Z_i C^T$, where $C$ is a random matrix where the first $nd_1$ columns are zero and the last $d_2 - n d_1$ columns have entries i.i.d uniform on $[-1, 1]$.
Because of the zero patterns of $B_0$ and $C$ and the fact that the entries of $Z$ are i.i.d., the random variables $Y_0$ and $W$ are independent.
If we can show that $Y_0$ has a density with respect to the Lebesgue measure on $\R^{nd_1d_2}$, then $Y_0 + \delta W$ converges to $Y_0$ in total variation distance as $\delta \to 0$.
This follows because $Y_0 + \delta W$ has a density obtained by convolving the density of $Y_0$, an $L_1$ function, by the law of $\delta W$.
The density of $Y_0 + \delta W$ then converges to that of $Y_0$ in $L_1$ by the continuity of the translation operator in $L_1$.\footnote{We thank Oliver Diaz for communicating a proof of this fact.}

By invertibility of $\Sigma_1$, it is enough to show that $Y_0$ has a density when $\Sigma_1 = I_{d_1}$.
Consider $Y_0 = (Z_1 B_0^T, \dots, Z_n B_0^T)$.
We may think of $Y_0$ as the $d_2 \times n d_1$ random matrix obtained by horizontally concatenating the matrices $B_0Z_i^T$.
\footnote{
Almost every matrix of these dimensions has rank $n d_1$, but if we had set even more of the columns of $B_0$ to zero then $Y_0$ would have rank \emph{less} than $n d_1$ with probability $1$ and hence would not have a density.
This is why we cannot push this argument any further.}

Now consider the $nd_1$ random vectors in $ \R^{d_2}$ that are the columns of the matrices $B_0 Z_i^T$, for $i \in \{1, \dots, n\}$.
Because $B_0$ is supported only in its first $nd_1$ columns, the joint distribution of these random vectors may be obtained by sampling $n d_1$ independent standard Gaussian vectors $v_j$ on $\R^{nd_1}$ and then multiplying them by the $d_2 \times nd_1$ matrix $B'$ that is the restriction of $B_0$ to its first $nd_1$ columns.
We have chosen $B'$ such that it is an isometry into a uniformly random subspace of $\R^{d_2}$ of dimension $nd_1$.
Thus $Bv_j/\|v_j\|$ are $nd_1$ many independent, random unit vectors in $\R^{d_2}$.
As the $\|v_j\|$ are also independent, $B v_j$ are thus independent. Each marginal $Bv_i$ has a density; one may sample it by choosing a uniformly random vector and then choosing the length $\|v_i\|$, hence the density is a product density in spherical coordinates. The joint density of the $Bv_j$ is then the product density of the marginal densities.
\end{proof}

The above lemma combined with \cref{prp:relative-lower} immediately implies \cref{thm:matrix-lower}. We remark that the below proof uses no properties about $\DF$; a lower bound on any error metric for estimating a Gaussian with $n \min \{n d_1, d_2\}$ samples will transfer to the matrix normal model. In particular, \cref{thm:matrix-lower} holds true when $\DF$ is replaced by the Frobenius error and $\Dop$ replaced by the operator norm error.
\begin{proof}[Proof of \cref{thm:matrix-lower}]
%We begin with the proofs of \cref{it:frob-lower,it:op-lower}, as they imply the rate lower bounds. If $nd_1 \geq d_1$, then the bound follows immediately from \CF{finish}

To show \cref{it:frob-lower}, let $\delta^2 \leq c \, \min \left\{1,\frac{d_1^2}{n \min \{n d_1, d_2\}}\right\}$. Let $\Theta_2$ be the distributed as in \cref{lem:reduce-lower} so that, as guaranteed by \cref{lem:reduce-lower} there is an estimator $\tilde{\Theta}$ with access to a tuple $Y$ of $n \min \{n d_1, d_2\}$ samples of a Gaussian on $\R^{d_1}$ with precision matrix $\Theta_1$ satisfying $\DTV (\htheta_1(X), \tilde{\Theta}(Y)) \leq \delta_0$. Here $X$ is distributed according to the normal model with precision matrices $\Theta_1, \Theta_2$. \cref{prp:relative-lower} implies \begin{align*}
\sup_{\Theta \in B} \Pr_Y\left[ \DF(\tilde{\Theta}(Y)\Vert  \Theta_1)  \geq \delta\right] \geq \frac{1}{2}.
\end{align*}
Clearly we have
$$\sup_{\Theta_1 \in B, \Theta_2 \in \PD(d_2)} \Pr_{X}
\left[ \DF(\htheta_1(X)\Vert  \Theta_1) \geq \delta \right]
\geq \sup_{\Theta_1 \in B} \Pr_{\Theta_2, X}\left[ \DF(\htheta_1(X)\Vert  \Theta_1)\geq \delta\right].$$
On the other hand, the total variation distance bound implies
\begin{align*}
\sup_{\Theta_1 \in B} \Pr_{\Theta_2, X}\left[ \DF(\htheta_1(X)\Vert  \Theta_1)\geq \delta\right]
&\geq \sup_{\Theta_1 \in B} \Pr_{Y}\left[ \DF(\tilde{\Theta}(Y)\Vert  \Theta_1) - \delta_0
 \geq \delta\right]\\
&\geq \frac{1}{2}  -  \delta_0.
\end{align*}
Allowing $\delta_0 \to 0$ implies the theorem. The proof of \cref{it:op-lower} is the same but with $D_{F}$ replaced by $D_{op}$.
\end{proof}



%=============================================================================
\section{Numerics and regularization}\label{sec:numerics}
%=============================================================================

In the undersampled regime, most effort so far has focused on the sparse case.
Existing estimators, such as the Gemini estimator \cite{zhou2014gemini} and KGlasso estimator \cite{tsiligkaridis2013convergence} enforce sparsity by adding a regularizer proportional to the $\ell_1$ norm of the precision matrices to encourage sparsity.
We refer to these as Glasso-type estimators.
We propose a new, shrinkage-based estimator that is simple to compute and experimentally outperforms Gemini and KGlasso in a natural generative model.

To describe our estimator, we remind the reader that the maximum likelihood estimator as defined in \cref{eq:obj via rho} is the optimum of a function depending on the sample covariance matrix.
Namely, we choose $\Theta \in \P$ optimizing $\tr \Theta \rho - \frac{1}{D}\log \det \Theta$ where $\rho = \frac{1}{n D} \sum_{i = 1}^n x_i x_i^T$ is the sample covariance matrix.\footnote{or, more accurately, the matrix of second moments}
In our estimator, $\rho$ is replaced by a shrinkage estimator for the covariance matrix.
Namely, we replace $\rho$ by $\tilde{\rho}: = (1- \alpha) \rho + \alpha \frac{\tr \rho}{D} I $ for some $\alpha \in [0,1]$. Formally, define $\widehat{\Theta}^\alpha_x = \argmin_{\Theta \in \P} f^\alpha_\samp(\Theta)$ where
\begin{align*}
  f_{\samp}^\alpha(\Theta)
% = \frac n2 \log \det(\Theta/2\pi) - \sum_{i=1}^n \frac12 \braket{\samp_i, \Theta \samp_i} ~
%  := \frac {n D} 2 \sum_{a = 1}^k \frac{1}{d_a} \log \det \Theta_a  - \frac12 \sum_{i=1}^n \braket{\samp_i, \left( \textstyle \bigotimes_{a=1}^k \Theta_a \right) \samp_i} + \alpha \prod_{i=1}^k \tr \Theta_a,
  :=  \tr \Theta \left((1 - \alpha) \rho + \alpha \frac{\tr \rho}{D} I\right) - \frac{1}{D}\log\det\Theta.
\end{align*}

This is known as a shrinkage estimator with ridge regularization \citep{warton2008penalized}.
The estimator, which we call the \emph{shrinkage-based flip-flop estimator}, or \emph{ShrinkFlop} for short, is closely related to the shrinkage estimator considered in \cite{goes2020robust} and the Frobenius penalty considered in \cite{tang2018integrated}.

%We propose a regularizer that is simple to compute and experimentally outperforms Gemini and KGlasso under a natural generative model for \emph{dense} covariance matrices.
We consider a generative model in which the covariance matrices $\Theta_i$ are distributed as a rank one Wishart matrix plus a small multiple of the identity matrix to ensure invertibility; we refer to them as \emph{spiked} covariance matrices.
We also show that, even when $\Theta_1$ is sparse, our shrinkage-based estimator can outputerform Gemini and KGlasso when $\Sigma_2$ is spiked.
Moreover, we observe that our regularized estimator is significantly faster to compute than the Glasso-type estimators. All three estimators require parameter tuning, so when possible we compare throughout a plausible range of parameters for each of them. We leave determination of $\alpha$ by cross-validation for future work.

%-----------------------------------------------------------------------------
\subsection{ShrinkFlop estimator}
%-----------------------------------------------------------------------------
We now discuss some properties of the ShrinkFlop estimator. As above, set $\tilde{\rho} := (1- \alpha) \rho + \alpha \frac{\tr \rho}{D} I.$ If $\alpha > 0$, then $\tilde{\rho}$ is invertible and therefore $\widehat{\Theta}^\alpha_x$ exists uniquely for every $x$.
This is because, whenever $\tilde{\rho}$ is invertible, the function $f_{\samp}^\alpha$ is strictly geodesically convex even on the larger domain $\PD(D)$ (see \cref{exa:usual-likelihood}). Thus it has a unique minimizer over the geodesically convex domain $\P$.

The estimator $\widehat{\Theta}^\alpha_x$ can be approximately computed using the flip-flop algorithm (as in \cref{alg:flip-flop}) with $\rho$ replaced by $\tilde{\rho}$.
Due to the special form of $\tilde{\rho}$, one need not look at the entirety of $\tilde{\rho}$ to perform each step of the flip-flop algorithm.
The resulting algorithm is summarized in \cref{alg:reg-flip-flop}.
The only modification as compared to the original algorithm is that at each flip-flop step the estimate of the covariance is replaced by a convex combination with a scaled identity matrix, as well as the computation of the gradient.

%\MW{This is pretty awkward since $\rho$ is already defined above. One option would be to give a formula for $\Theta=I$ (where $\rho$ is the one that you defined above) and a transformation formula, like in \cref{remark:gradient-everywhere}.}
% \MW{While \cref{eq:reg-like} now refers to $\rho$ as defined above, this is not the same as $\rho$ as define in the algorithm. Still a bit awkward/ambiguous since we cite \cref{eq:reg-like} from the algorithm\dots One simple fix would be to move \cref{eq:reg-like} into the algorithm box. What do you think?}

% \CF{I just went ahead and defined it explicitly...such a translation formula would be a pain in this case.}
% \MW{Agreed... perhaps we then do the same in Section~5?}
\MW{Will have another peek at the algo.}
\begin{Algorithm}
\begin{description}
\item[\hspace{.2cm}\textbf{Input}:] Samples $\samp = (\samp_1, \ldots, \samp_n)$ where $\samp_i \in \R^D$ is sampled from a (unknown) tensor normal model distribution with precision matrix $\Theta \in \PD$,
% centered normal distribution $\cN(0, \bigotimes_{j=1}^k \Sigma_j )$
% \MW{actually a tensor normal model\dots maybe remove this?}
and a real number $\eps > 0$.
% where each entry of $\samp_i$ is encoded in binary, with bit size $\le b$. Approximation parameter $\eps > 0$, given in binary representation.

\item[\hspace{.2cm}\textbf{Output}:] A regularized estimator $\otheta \in \SPD$.\\[.1ex]
% such that $\| \nabla \ell^\alpha_x (\otheta)\|_F < \eps$. \\[.3ex]

\item[\hspace{.2cm}\textbf{Algorithm}:]
\end{description}
\begin{enumerate}
\item\label{it:flip-flop step 1 reg} Set $\otheta_a = I_a$ for each $a \in [k]$, and $\otheta = \otimes_{a=1}^k \otheta_a$.
\item\label{it:flip-flop step 2 reg} Repeat the following:
%we need T = \log(1/\eps) \cdot 1/\alpha \lambda
\begin{itemize}
\item Set
$\rho = \dfrac{1}{nD} \otheta^{1/2} \left( \sum_{i=1}^n x_ix_i^T \right) \otheta^{1/2}.$
\item Compute each component of the gradient $\nabla f^\alpha_{\samp}(\otheta_1, \ldots, \otheta_k)$, denoting $\nabla_a := \nabla_a f^\alpha_{\samp}(\otheta_1, \ldots, \otheta_k)$, and find the index $a \in [k]$ for which $\norm{\nabla_a}_F$ is largest.
The $a^{th}$ component of the gradient is computed as follows:
\begin{align*}
  \nabla_a f^\alpha_\samp(\otheta) =
  % (1 - \alpha) (\otheta^{1/2} \rho \otheta^{1/2})^{(a)} +
  \sqrt{d_a} \left((1 - \alpha) \rho^{(a)} +
  \frac{\alpha \|x\|^2}{nD}  (\prod_{b \neq a} \tr \otheta_b ) \otheta_a - \frac{1}{d_a} I_{d_a}.\right)
  %a = \arg\max_{i \in [k]} d_i   \left\| \otheta_i^{1/2} \Upsilon_i \otheta_i^{1/2}  + \alpha (\prod_j \tr \otheta_j) I_{d_i} - I_{d_i}/d_i\right\|_F^2,
\end{align*}
%\itemIf $\norm{\nabla_a}_F < \delta$, output $\otheta$ and return.
\item If $\|\nabla_a \|_F < \eps$, end loop and \textbf{return} $\overline{\Theta}$.
\item Otherwise, set
$$\otheta_a^{-1} \leftarrow  d_a \left((1 - \alpha) \otheta_a^{-1/2} \rho^{(a)} \, \otheta_a^{-1/2} + \frac{\alpha \|x\|^2}{nD} (\prod_{b \neq a} \tr \Theta_b) I_{d_a}  \right).$$
%Set$$\otheta_a = \frac{n D}{d_a}\left(\Upsilon_i + \alpha \left(\prod_{i \neq a} \tr \otheta_a\right) I_{d_a} \right)^{-1}.$$

\end{itemize}
\end{enumerate}
\caption{Shrinkage-based flip-flop algorithm}\label{alg:reg-flip-flop}
\end{Algorithm}

%-----------------------------------------------------------------------------
\subsection{Scenario 1: Spiked, dense covariances}
%-----------------------------------------------------------------------------
Here we compare the performance of our shrinkage based estimator with Zhou's single step estimator (Gemini) for the matrix normal model assuming $\Sigma_1, \Sigma_2$ are dense, spiked covariance matrices.
%More precisely, $\Sigma_1, \Sigma_2$ are each a random rank one matrix plus a small multiple of the identity matrix.
\cref{fig:spiked} was generated by setting $d_1 = 25, d_2 = 50$ and $n = 1$, and for each choice of regularization parameter independently generating 5 different pairs $\Sigma_1 \sim I_{d_1} + 10 v_1v_1^T$ and $\Sigma_2 \sim I_{d_2} + 10 v_2v_2^T$ where the $v_i$ are standard $d_i$-dimensional Gaussian vectors and then normalizing each to have trace $d_i$, respectively.
As in \cite{zhou2014gemini}, we always normalize the trace of $\htheta_1$ to match that of $\Theta_1$ to focus on the core difficulties rather than estimating the overall scale of the data (which is easy to do). Though our main focus is estimation in geodesic distance, we also considered the Frobenius distance due to its prevalence in the literature.
For each of the 5 pairs, following \cite{tsiligkaridis2013convergence} we computed the normalized squared error (the squared Frobenius distance from $\Theta_1$ to $\htheta_1$ divided by the squared Frobenius norm of $\Theta_1$ for samples drawn from this distribution 5 times.
Finally we averaged all 25 errors.
Note that with $d_1 = 25, d_2 = 50$ and $n = 1$, the MLE never exists and so we cannot compare with flip-flop without shrinkage.

We also computed the error in geodesic distance. We used geodesic distance in place of the relative Frobenius error (\cref{dfn:relative-error}) because these quantities are comparable (see \cref{sec:rel-error}).
To compute the error in geodesic distance, we compute the squared geodesic distance between the estimator and the truth and divide by the geodesic distance between the truth and the identity.

\cref{fig:spiked} demonstrates that in the spiked case, for all choices of regularization parameter, the Gemini and KGlasso estimator were outperformed by the ``trivial'' estimator which always outputs the identity matrix.
For a broad range of regularization parameters, our regularized estimator outperforms both the trivial estimator and Gemini.
The poor performance of Gemini and KGlasso in this case are to be expected because the true precision matrices are dense.

%However, for a fairly broad range of regularization parameter, the Gemini does outperform the inverse of the the partial trace of the sample covariance matrix (the partial trace of the sample covariance matrix is an unbiased estimator of $\Sigma_1$). As expected, as the regularization parameter tends to zero Gemini becomes identical to the inverse of the the partial trace of the sample covariance matrix. \CF{explain that this is what Gemini for $\Theta_1$ is.} For a broad range of regularization parameters, our regularized estimator outperforms the trivial estimator.

\begin{figure}
         \centering
          \begin{subfigure}[b]{.4\textwidth}
         \includegraphics[width=\textwidth]{./code/zhou-comparison/25-50-spiked-frob.pdf}
         \end{subfigure}
          \begin{subfigure}[b]{.4\textwidth}
          \includegraphics[width=\textwidth]{./code/zhou-comparison/25-50-spiked-geo.pdf}
          \end{subfigure}
         %\caption{$y=5/x$}
         %\label{fig:five over x}
        %\caption{Three simple graphs}
        %\label{fig:three graphs}
\caption{Average Frobenius error with $d_1 = 25, d_2 = 50, n = 1$ for spiked, dense covariance matrices. ``Gemini'' refers to the Gemini estimator of \cite{zhou2014gemini}, ``KGlasso'' refers to the Kronecker Glasso algorithm \cite{tsiligkaridis2013convergence}, ``ShrinkFlop'' refers to our shrinkage-based flip-flop estimator, and ``Trivial'' refers to the estimator that always outputs the identity matrix. The choice of regularizer~$\alpha$ for ShrinkFlop is given by $\alpha = \frac{2}{\pi} \arctan{x}$, where $x$ is the value on the $x$-axis in the figures above. }\label{fig:spiked}
\end{figure}

%-----------------------------------------------------------------------------
\subsection{Scenario 2: Sparse and partially sparse precision matrices}
%-----------------------------------------------------------------------------
%We next compare the performance of the estimator with Zhou's single step estimator \cite{zhou2014gemini} for the matrix normal model.
We now compare the performance of our estimator with other leading estimators in the case when one or more of the precision matrices $\Theta_1, \Theta_2$ is sparse. We find that when both $\Theta_1$ and $\Theta_2$ are sparse, the Gemini estimator outperforms the regularized Sinkhorn algorithm in Frobenius error; see \cref{fig:sparse-ii}. However, when $\Theta_2$ is spiked, we find that the shrinkage-based flip-flop estimator outperforms the Gemini estimator; see \cref{fig:sparse-i}. In practice, $\Theta_2$ is often considered a nuisance parameter and $\Theta_1$ is the object that should be interpretable (e.g., sparse). Thus ill-conditioned and dense nuisance parameters $\Theta_2$ can break Glasso-type estimators even when $\Theta_1$ is sparse.

The figures were generated in the same manner as \cref{fig:spiked}, apart from the generative model. The sparse matrices $\Theta_i$ were generated by adding $\frac{1}{2}I_{d_1}$ to the Laplacian of a random multigraph with $0.4 d_i$ edges and normalizing to have trace $d_i$, and the spiked covariance matrix $\Sigma_2$ for \cref{fig:sparse-i} was drawn according to $\Sigma_2 \sim I_{d_2} + 100 v_2v_2^T$ where $v_2$ has i.i.d. Gaussian coordinates, and then normalized to have trace $d_2$.

\begin{figure}
              \centering
              \begin{subfigure}[b]{.4\textwidth}
         \includegraphics[width=\textwidth]{./code/zhou-comparison/25-50-doubly-sparse-frob.pdf}
         \end{subfigure}
         \begin{subfigure}[b]{.4\textwidth}
         \includegraphics[width=\textwidth]{./code/zhou-comparison/25-50-doubly-sparse-geo.pdf}
         \end{subfigure}
         %\caption{$y=5/x$}
         %\label{fig:five over x}
        %\caption{Three simple graphs}
        %\label{fig:three graphs}
\caption{Average error with $d_1 = 25, d_2 = 50, n = 1$ for both precision matrices sparse.
Labels and choice of regularizer as in \cref{fig:spiked}.\label{fig:sparse-ii}}
\end{figure}


\begin{figure}
         \centering
                       \begin{subfigure}[b]{.4\textwidth}
         \includegraphics[width=\textwidth]{./code/zhou-comparison/25-50-sparse-frob.pdf}
         \end{subfigure}
         \begin{subfigure}[b]{.4\textwidth}
         \includegraphics[width=\textwidth]{./code/zhou-comparison/25-50-sparse-geo.pdf}
         \end{subfigure}
         %\caption{$y=5/x$}
         %\label{fig:five over x}
        %\caption{Three simple graphs}
        %\label{fig:three graphs}
\caption{Average error with $d_1 = 25, d_2 = 50, n = 1$ for $\Theta_1$ sparse and $\Theta_2$ spiked. Interestingly, for Frobenius distance the shrinkage-based estimator performs best at the bottom of the regularization path and for the geodesic distance it performs best for larger regularization parameters.
Labels and choice of regularizer as in \cref{fig:spiked}. All algorithms perform poorly in geodesic distance under this model; as expected, ShrinkFlop approaches the trivial algorithm as the regularizer becomes large.}\label{fig:sparse-i}
\end{figure}

%-----------------------------------------------------------------------------
\subsection{Performance as a function of the number of samples}
%-----------------------------------------------------------------------------
In this subsection we examine how the number of samples affects the error. We found best performance when the shrinkage parameter scales inverse exponentially with the number of samples, in this case $2^{-1.1 n}$. The regularization parameter for Gemini was chosen to scale as $\sqrt{(\log d_1) / d_2 n}$ as suggested in \cite{zhou2014gemini}.


In \cref{fig:lc} we see that Gemini outperforms flip-flop with a single sample, and shrinkage-based flip-flop outperforms both. When the number of samples are increased, the error for shrinkage-based flip-flop approaches that of flip-flop from below.


\begin{figure}
         \centering
                       \begin{subfigure}[b]{.4\textwidth}
         \includegraphics[width=\textwidth]{./code/zhou-comparison/25-25-spiked-lc-frob.pdf}
         \end{subfigure}
         \begin{subfigure}[b]{.4\textwidth}
         \includegraphics[width=\textwidth]{./code/zhou-comparison/25-25-spiked-lc-geo.pdf}
         \end{subfigure}
         %\caption{$y=5/x$}
         %\label{fig:five over x}
        %\caption{Three simple graphs}
        %\label{fig:three graphs}
\caption{Average error with $d_1 = 25, d_2 = 25, $ and number of samples $n$ ranging from $1$ to $50$ for both precision matrices spiked. It was necessary to choose $d_1 = d_2$ so that the flip-flop estimator converged for $n = 1$. The regularization for ShinkFlop was given by $\alpha = 2^{-1.1n}$. We did not include KGlasso here because its running time was prohibitive.\label{fig:lc}}
\end{figure}

%-----------------------------------------------------------------------------
\subsection{Computational aspects}
%-----------------------------------------------------------------------------
Here we experimentally demonstrate that our shrinkage-based estimator is much faster to compute than the Glasso-type estimators. We considered $d_1 =100, d_2 = 200$ with $n = 1$ samples. We considered regularized flip-flop and Gemini on spiked data, and chose the best regularization parameters for both using the computation from \cref{fig:spiked}. We did not consider KGLasso because it consistently took far longer than Gemini. Out of 2 draws of data from 2 instances, the average time of completion was 0.078 seconds for regularized flip-flop and 30 seconds for Gemini. Both computations were done in R using a MacBook~Pro with an Apple M1 chip with 16 gigabytes of RAM.

%=============================================================================
\section{Conclusion and open problems}
%=============================================================================
Though there has been a large volume of work on estimating the covariance in the matrix and tensor normal models under further assumptions like sparsity and well-conditionedness, some fundamental questions concerning estimation without further assumptions were still open prior to our work.
Contrary to the state of the art for unstructured covariance estimation (i.e., $k = 1$), all previous existing results depended on the condition number of the true covariance matrices (in the case of the tensor normal model under Frobenius norm) or had suboptimal sample complexity (the matrix normal model under operator norm).
Using strong convexity in the geometry induced by the Fisher information metric, we are largely able to remedy these issues and obtain nearly optimal estimates in the strongest possible metrics, namely the \emph{relative} operator and Frobenius norms.
As a consequence, we can also control other equivariant statistical distances such as relative entropy, Fisher-Rao distance, and total variation distance.

In particular, we showed that the maximum likelihood estimator (MLE) for the covariance matrix in the matrix normal models has optimal sample complexity up to logarithmic factors in the dimensions.
We showed that the MLE for tensor normal models with a constant number of tensor factors has optimal sample complexity in the regime where it is information-theoretically possible to recover the covariance matrix to within a constant Frobenius error.
Whenever the number of samples is large enough for either of the aforementioned statistical results to hold, we show that the flip-flop algorithm converges to the MLE exponentially quickly.
Hence, the output of the flip-flop algorithm with $O\left(\dmax +  \log n \right)$ iterations (see the discussion after \cref{thm:matrix-flipflop}) is an efficiently computable estimator with statistical guarantees comparable to those we show for the MLE.

We also observed empirically that under a certain natural generative model of ill-conditioned, dense covariance matrices, the flip-flop algorithm combined with a very simple shrinkage technique can outperform existing estimators designed for the sparse case (Gemini and KGlasso).
We view our empirical results as evidence that, in some cases, flip-flop combined with shrinkage provides the fastest, simplest and most statistically accurate known method to estimate the covariances.
More work is needed in the future to rigorously understand the statistical guarantees for flip-flop with shrinkage.


Our main theoretical open question is whether the assumption $n = \Omega( \dmax^3/ D)$ for \cref{thm:tensor-frobenius} can be weakened to $n = \Omega( \dmax^2/ D)$ for $k \geq 3$.
Equivalently, do the guarantees of \cref{thm:tensor-frobenius} hold even when one cannot hope to estimate the Kronecker factors to constant Frobenius error, but only to constant \emph{operator norm} error?
In the case $k = 1$ (i.e., unstructured covariance estimation) the weaker assumption is well-known to suffice, and for $k = 2$ the same follows (up to logarithmic factors) by our \cref{thm:matrix-normal}.
Filling in this final gap will place the tensor normal model on the same tight theoretical footing as unstructured covariance estimation.

\begin{appendix}

%=============================================================================
\section{Pisier's proof of quantum expansion}\label{sec:pisier}
%=============================================================================
In this appendix we prove the spectral condition for random Gaussian completely positive maps given in \cref{thm:hess-pisier}.
This follows from the work of \cite{pisier2012grothendieck}, whose original theorem dealt with square matrices and gave slightly weaker probabilistic guarantees than \cref{thm:Pisier-expansion} stated below.
We adapt this result to give exponentially small error probability in the setting of rectangular matrices.
We emphasize that these are minor modifications, which follow readily from \cite{P14,pisier2012grothendieck}.
Therefore, we state the proof below for completeness and claim no originality.

\begin{theorem}[Pisier]\label{thm:Pisier-expansion}
Let $A_1,\dots,A_N,A$ be independent $n \times m$ random matrices with independent standard Gaussian entries.
For any~$t \geq 2$, with probability at least $1 - t^{-\Omega(m+n)}$,
\begin{align*}
  \norm*{\left(\sum_{i=1}^N A_i \otimes A_i \right) \circ \Pi}_{\op}
  \leq O\left( t^2 \sqrt{N} ( m+n ) \right),
\end{align*}
where $\Pi$ denotes the orthogonal projection onto the traceless subspace of $\R^m \ot \R^m$, that is, onto the orthogonal complement of $\vect(I_m)$.
\end{theorem}

We first explain how \cref{thm:Pisier-expansion} implies \cref{thm:hess-pisier}.

\begin{proof}[Proof of \cref{thm:hess-pisier}]
Choose $n=d_a$ and $m=d_b$.
Observe that
\begin{align*}
  \norm{\Phi_A}_0
= \max_{\substack{H \text{ traceless symmetric} \\ \norm H_F=1}} \norm{\Phi(H)}_F
% = \max_{\substack{H \text{ traceless symmetric} \\ \norm H_F \leq 1}} \norm{\Phi(H)}_F
\leq %\max_{\substack{H \in \Mat(n,m) \\ \norm H_F \leq 1}} \norm{\Phi(\Pi(H))}_F =
\max_{\substack{H \in \Mat(m) \\ \norm H_F = 1}} \norm{\Phi(\Pi(H))}_F
= \norm{\Phi \circ \Pi}_{\op}.
\end{align*}
Here we identified $\Mat(m) \cong \R^m \ot \R^m$, so $\Pi$ identifies with the orthogonal projection onto the traceless matrices, and we used that $\norm{\Pi(H)}_F \leq \norm{H}_F$, since $\Pi$ is an orthogonal projection.
Using \cref{eq:vec rep}, the result now follows from \cref{thm:Pisier-expansion}.
% as
% \begin{align*}
%   \norm{\Phi_A}_0 \leq O\left( t^2 \sqrt{N} (m+n) \right),
% \end{align*}
% with the desired probability.
% Using \cref{eq:op norm upper bound}, we can bound the right-hand side operator norm,
% \begin{align*}
%   \bigl( \E \norm{A}_{\op} \bigr)^2
% = O\left( \left( \sqrt{n} + \sqrt{m} \right)^2 \right)
% = O\left(n+m\right),
% \end{align*}
% which concludes the proof.
\end{proof}

In the remainder we discuss the proof of \cref{thm:Pisier-expansion}.
% Our setting required the result on rectangular matrices with strong error bounds \MW{$\leftarrow$ Should mention this at the beginning of the appendix instead of just talking about a `slightly different language'.}, so we follow the proof in \cite{pisier2012grothendieck} with these minor modifications and claim no originality.
The proof proceeds by a symmetrization trick, followed by the trace method.
We first state some relevant bounds on Gaussian random variables and then give the proof of \cref{thm:Pisier-expansion}.

We will often use the following estimate of the operator norm of a standard Gaussian $n \times m$ random matrix~$A$ (see Theorem~5.32 in \cite{vershynin2010introduction}),
\begin{equation}\label{eq:op norm upper bound}
  \E \norm{A}_{\op} \leq \sqrt{n} + \sqrt{m}  .
\end{equation}

\begin{theorem}\label{thm:banach conc}
Let $A$ be a centered Gaussian random variable that takes values in a separable Banach space with norm $\|\cdot\|$.
Then $\|A\|$ satisfies the following concentration and moment inequalities with parameter $\sigma^2 := \sup \{ \E \langle \xi, A \rangle^{2} \mid \|\xi\|_{*} \leq 1 \}$, where $\|\cdot\|_{*}$ denotes the dual norm:
\[ \forall t > 0 \colon \quad \Pr\Bigl( \abs[\big]{\norm A - \E \norm A} \geq t\Bigr) \leq 2 \exp\Bigl( - \frac{\Omega(t^2)}{\sigma^{2}}\Bigr)  , \qquad \text{and}  \]
\begin{equation}\label{eq:conc via moments}
  \forall p \geq 1 \colon \quad \E \|A\|^{p} \leq (2 \E \|A\|)^{p}  + O ( \sigma \sqrt{p} )^{p}.
\end{equation}
\end{theorem}
\begin{proof}
The first statement on concentration is exactly Theorem~1.5 in \cite{P86}.
For the second, we consider the random variable $X := \frac{1}{\sigma} (\|A\| - \E \|A\| )$.
Then the equivalence in Lemma 5.5 of \cite{vershynin2010introduction} gives the moment bound
\[
  \Bigl( \E |X|^{p} \Bigr)^{1/p}
= \frac1\sigma \Big( \E \Big| \|A\| - \E \|A\| \Big|^{p} \Big)^{1/p}
 \leq O(\sqrt{p}).
\]
The moment bound in the theorem now follows by rearranging as
\[
  \E \|A\|^{p}
= \E \Bigl( \E \|A\| + \sigma X  \Bigr)^{p}
\leq 2^{p} \Bigl( (\E \|A\|)^{p} + O( \sigma \sqrt{p} )^{p} \Bigr),
\]
where the last step was by the simple inequality $(a+b)^{p} \leq 2^{p} (|a|^{p} + |b|^{p})$.
\end{proof}

% \begin{remark}
% The concentration inequalities and moment bounds in \cref{thm:banach conc} are related to the notion of sub-Gaussian random variables (see Lemma 5.5 of \cite{vershynin2010introduction}) which are often used in random matrix theory.
% \end{remark}

Below, we calculate the $\sigma^{2}$ parameter in \cref{thm:banach conc} with regards to our random matrix setting.

\begin{corollary}\label{lem:opNormSubG}
Let $A$ be an $n \times m$ matrix with independent standard Gaussian entries. Then the random variable $\norm{A}_{\op}$ satisfies the conclusions of \cref{thm:banach conc} with $\sigma^{2} = 1$.
% \MW{See sub-Gaussian comment above.}
\end{corollary}
\begin{proof}
Note that the dual norm is the trace norm~$\norm{\cdot}_{\operatorname{tr}}$, hence the concentration parameter can be estimated as
\begin{align*}
  \sigma^2
= \sup \left\{ \E \langle \xi, A \rangle^2 \;\mid\; \norm{\xi}_{\operatorname{tr}} \leq 1 \right\}
= \sup \left\{ \norm\xi_F^2 \;\mid\; \norm{\xi}_{\operatorname{tr}} \leq 1 \right\}
= 1,
\end{align*}
where we first used that $\braket{\xi,A}$ is distributed the same as $\norm\xi_F A_{11}$ by orthogonal invariance, and then that the trace norm dominates the Frobenius norm, with equality attained for example by $\xi = E_{11}$.
\end{proof}

% \AR{I think we don't need this lower bound anymore if we just bound by $m+n$ instead of $\E\|A\|_{\op}^{2}$}.
% We can now prove a lower bound that complements \cref{eq:op norm upper bound}.

% \begin{lemma}\label{lem:op norm lower bound}
% Let $A$ be an $n \times m$ matrix with independent standard Gaussian entries. Then there is some universal constant $c > 0$ such that
% \begin{equation*}
%   \E \norm{A}_{\op} \geq c (\sqrt{n} + \sqrt{m}).
% \end{equation*}
% \end{lemma}
% \begin{proof}
% Without loss of generality we may assume that $n \geq m$.
% Then,
% \[ \E \norm{A}_{\op} \geq \E \norm{A e_{1}}_{2} \geq \frac{\sqrt{n}}{2} \geq \frac{\sqrt{m} + \sqrt{n}}{4}. \]
% The second inequality follows by observing that $g := A e_1$ is a vector of standard Gaussian entries in~$\R^n$.
% Its expected length can be bounded by elementary integration, using that $\norm{g}_2$ is distributed as $\sqrt X$ for $X = \norm{g}_2^2$~a $\chi^2$ random variable with~$n$ degrees of freedom (see Section 3.1 of \cite{ENormLB}).
% \end{proof}

We will also use the the \emph{Schatten $p$-norms} $\norm{A}_p = (\tr\left[(A^TA)^{\frac p2}\right])^{\frac1p}$, which generalize the trace, Frobenius, and operator norms.
They satisfy the following H\"older inequality for $p\geq1$:
\begin{align}\label{eq:holder}
  \abs*{\tr \prod_{i=1}^{p} A_i} \leq \prod_{i=1}^{p} \norm{A_i}_p,
\end{align}

\begin{proof} [Proof of \cref{thm:Pisier-expansion}]
The operator we want to control has entries which are dependent in complicated ways.
We first begin with a standard symmetrization trick to linearize (compare the proof of Lemma~4.1 in~\cite{P14}).
A single entry of $A_i \otimes A_i$ is either a product $g g'$ of two independent standard Gaussians, or the square $g^2$ of a single standard Gaussian.
In expectation, we have $\E g g' = 0, \E g^{2} = 1$, and so the expected matrix is
\[ \E \left( \sum_{i=1}^N A_i \otimes A_i \right) = N \vect(I_n) \vect(I_m)^T. \]
Accordingly, after projection we have
\[ \E \left( \sum_{i=1}^N A_i \otimes A_i \right) \circ \Pi = 0. \]
Therefore we may add an independent copy:
Let $B_1,\dots,B_N$ be independent $n\times m$ random matrices with standard Gaussian entries, that are also independent from~$A_1,\dots,A_N$.
Then,
\begin{align*}
  \left( \sum_{i=1}^N A_i \otimes A_i \right) \circ \Pi
= \E_B \left( \sum_{i=1}^N A_i \otimes A_i - \sum_{i=1}^N B_i \otimes B_i \right) \circ \Pi
\end{align*}
and hence, for any $p\geq1$,
\begin{align*}
  \E \norm*{\left( \sum_{i=1}^N A_i \otimes A_i \right) \circ \Pi}_{\op}^p
\leq \E \norm*{\left( \sum_{i=1}^N A_i \otimes A_i - \sum_{i=1}^N B_i \otimes B_i \right) \circ \Pi}_{\op}^p
\end{align*}
by Jensen's inequality, as $\norm{\cdot}_{\op}^p$ is convex as the composition of the norm $\norm{\cdot}_{\op}$ with the convex and nondecreasing function $x \to x^{p}$.
Now note $(A_i,B_i)$ has the same distribution as $(\frac{A_i+B_i}{\sqrt2},\frac{A_i-B_i}{\sqrt2})$, so the right-hand side is equal to
\begin{align*}
&\quad \E \norm*{ \frac{1}{2} \left( \sum_{i=1}^N (A_i + B_i) \otimes (A_i + B_i) - \sum_{i=1}^N (A_i - B_i) \otimes(A_i - B_i) \right) \circ \Pi}_{\op}^p \\
&= \E \norm*{\left( \sum_{i=1}^N A_i \otimes B_i + \sum_{i=1}^N B_i \otimes A_i \right) \circ \Pi}_{\op}^p
% \leq 2 \E \norm*{\left( \sum_{i=1}^N A_i \otimes B_i \right) \circ \Pi}_{\op}^p
\leq 2^{p} \, \E \norm*{\sum_{i=1}^N A_i \otimes B_i}_{\op}^p
\end{align*}
Thus, we have proved that
\begin{align}\label{eq:symmetrization}
  \E \norm*{\left( \sum_{i=1}^N A_i \otimes A_i \right) \circ \Pi}_{\op}^p
\leq 2^{p} \, \E \norm*{\sum_{i=1}^N A_i \otimes B_i}_{\op}^p.
\end{align}
Note that we have lost the projection and removed the dependencies.
Next we use the trace method to bound the right-hand side of \cref{eq:symmetrization}.
That is, we approximate the operator norm by the Schatten $p$-norm for a large enough $p$ and control these Schatten norms using concentration of moments of Gaussians (compare the proof of Theorem~16.6 in~\cite{pisier2012grothendieck}).
For any $q\geq1$,
\begin{align*}
\E \norm*{\sum_{i=1}^N A_i \ot B_i}_{2q}^{2q}
% = \E \tr \left( \sum_{i=1}^N A_i \ot B_i \right)^{2q}
&= \E \tr \left[ \left( \sum_{i,j\in[N]} A_i^T A_j \ot B_i^T B_j \right)^{\!\!q} \, \right] \\
&= \sum_{i, j \in [N]^q} \E \tr \left( A^T_{i_1} A_{j_1} \cdots A^T_{i_q} A_{j_q} \ot B^T_{i_1} B_{j_1} \cdots B^T_{i_q} B_{j_q} \right) \\
&= \sum_{i, j \in [N]^q} \E \tr \left( A^T_{i_1} A_{j_1} \cdots A^T_{i_q} A_{j_q} \right) \E \tr \left( B^T_{i_1} B_{j_1} \cdots B^T_{i_q} B_{j_q} \right)
\end{align*}
where we used the independence of $\{A_i\}$ and $\{B_i\}$ in the last step.
Now, the expectation of a monomial of independent standard Gaussian random variables is always nonnegative.
% Now, the expectation of a (non-empty) monomial of independent standard Gaussian random variables vanishes unless each variable appears an even number of times, in which case it is positive.
Thus the same is true for $\E \tr ( A^T_{i_1} A_{j_1} \cdots A^T_{i_q} A_{j_q} )$, so we can upper bound the sum term by term as
\begin{align*}
&\quad \sum_{i, j \in [N]^q} \E \tr \left( A^T_{i_1} A_{j_1} \cdots A^T_{i_q} A_{j_q} \right) \E \tr \left( B^T_{i_1} B_{j_1} \cdots B^T_{i_q} B_{j_q} \right) \\
&\leq \sum_{i, j \in [N]^q} \E \tr \left( A^T_{i_1} A_{j_1} \cdots A^T_{i_q} A_{j_q} \right) \E \left( \norm{ B_{i_1} }_{2q} \norm{B_{j_1}}_{2q} \cdots \norm{ B_{i_q} }_{2q} \norm{B_{j_q}}_{2q} \right) \\
&\leq \sum_{i, j \in [N]^q} \E \tr \left( A^T_{i_1} A_{j_1} \cdots A^T_{i_q} A_{j_q} \right) \E \left( \norm{ B_1 }_{2q}^{2q} \right) \\
&= \left( \E\norm{\sum_{i=1}^N A_i}_{2q}^{2q} \right) \left( \E \norm{ A }_{2q}^{2q} \right)
= N^q \left( \E\norm{A}_{2q}^{2q} \right)^2.
\end{align*}
In the first step we used H\"older's inequality~\eqref{eq:holder} for the Schatten norm.
The second step holds since $\E\norm{B_i}_{2q}^k \leq (\E \norm{B_i}_{2q}^{2q})^{\frac k {2q}}$ by Jensen's inequality, so we can collect like terms together.
Next, we used that the~$B_i$ have the same distribution as $A$.
In the last step, we used that $\sum_i A_i$ has the same distribution as $\sqrt N A$.
Accordingly, we obtain for~$q\geq1$,
\begin{align*}
  \E \norm*{\sum_{i=1}^N A_i \ot B_i}_{2q}^{2q}
\leq N^q \left( \E\norm{A}_{2q}^{2q} \right)^2,
\end{align*}
and hence
\begin{align*}
\E \norm*{\left( \sum_{i=1}^N A_i \otimes A_i \right) \circ \Pi}_{\op}^{2q}
&\leq 4^q \, \E \norm*{\sum_{i=1}^N A_i \ot B_i}_{\op}^{2q}
\leq 4^q \, \E \norm*{\sum_{i=1}^N A_i \ot B_i}_{2q}^{2q} \\
&\leq (4N)^q \left( \E\norm{A}_{2q}^{2q} \right)^2
\leq (4N)^q m^2 \Bigl( \E\norm{A}_{\op}^{2q} \Bigr)^2.
\end{align*}
The first inequality is \cref{eq:symmetrization}, and in the last inequality we used that $A \in \Mat(n,m)$ has rank~$\leq m$, and therefore $\norm{A}_{2q}^{2q} \leq m \norm A_{\op}^{2q}$.
To bound the right-hand side, we use \cref{thm:banach conc}, applied to the random variable~$A$ in the Banach space~$\Mat(n,m)$ with the operator norm~$\norm{\cdot}_{\op}$.
Then, $\sigma^2=1$, as computed in \cref{lem:opNormSubG}, and we find that
\begin{align*}
\E \norm*{\left( \sum_{i=1}^N A_i \otimes A_i \right) \circ \Pi}_{\op}^{2q}
\leq (4 N)^q m^2 \Bigl( (2 \E\norm A_{\op})^{2q} + (C \sqrt{q})^{2q} \Bigr)^{2}.
\end{align*}
where~$C>0$ is a universal constant implied by the big-$O$ notation in \cref{eq:conc via moments}.
We can bound the first term $\E \norm{A}_{\op} \leq \sqrt{m}+\sqrt{n}$ by \cref{eq:op norm upper bound}, so for $q = 2(m + n)$, we can upper bound the mean by
\begin{align*}
\E \norm*{\left( \sum_{i=1}^N A_i \otimes A_i \right) \circ \Pi}_{\op}^{2q}
% &\leq (4 N)^q m^2 \Bigl( (2 (\sqrt m + \sqrt n))^{2q} + (C^2 q)^q \Bigr)^{2} \\
% &\leq (4 N)^q m^2 \Bigl( (2^2  q)^q + (C^2 q)^q \Bigr)^{2} \\
&\leq 4 m^2 \Big( (\max\{2, C\})^{2} \cdot q \cdot \sqrt{ 4 N }\Big)^{2q} .
\end{align*}
Finally, we can use Markov's inequality to see that, for $C' = \sqrt{2} \max\{2,C\}$, the event
%\MW{Why dont' we choose $C' = \sqrt{2} \max\{2,C\}$\dots}
\begin{align}\label{eq:desired event}
  \norm*{\left(\sum_{i=1}^N A_i \otimes A_i \right) \circ \Pi}_{\op}
  \leq (C' t)^2 \cdot (m+n) \cdot \sqrt{4 N}
\end{align}
holds up to failure probability at most
%\MW{\dots so that $(2t)^{-2q}$ would become $t^{-2q}$ in the following, no?}
\begin{align*}
  4 m^2 \left( \frac{(\max\{2, C\})^{2} \cdot q \cdot \sqrt{ 4 N } }{(C' t)^2 \cdot (m+n) \cdot \sqrt{4 N}} \right)^{2q}
  \leq 4 m^2 t^{-2q} \leq t^{-\Omega(m+n)} ,
%   \frac{(4 N)^q m^2 \cdot 4 (\max\{2, C\})^{4q} q^{2q}}{\left( (C' t)^2 \sqrt{4 N} (m+n) \right)^{2q}}
% \leq \frac{2 N^q m^2 \Bigl( \E\norm A_{\op} + C \sqrt{q} \Bigr)^{4q}}{(2C')^{4q} t^{4q} N^q \bigl( \E \norm{A}_{\op} \bigr)^{4q}} =
  % \frac{m^2 \Bigl( \E\norm A_{\op} + C \sqrt{q} \Bigr)^{4q}}{(C' t)^{4q} \bigl( \E \norm{A}_{\op} \bigr)^{4q}}.
  % \frac{m^2 \Bigl( \E\norm A_{\op} + C \sqrt{q} \Bigr)^{4q}}{\bigl( C' t \, \E \norm{A}_{\op} \bigr)^{4q}}.
%   m^2 \frac{\Bigl( (2 \E\norm A_{\op})^{2q} + (C \sqrt{q})^{2q} \Bigr)^{2}}{(C' t \, \E \norm{A}_{\op})^{4q}}.
\end{align*}
  where the first step was by our choice of $q = 2(m+n)$ and of $C' = \sqrt{2} \max\{2,C\}$, and the final inequality was by the fact that $t\geq2$, so the prefactor $4 m^{2}$ can be absorbed at the cost of slightly changing the constant in the exponent.

% We choose $q = \max\{1, (C^{-1} \E \norm{A}_{\op})^{2} \}$, which by \cref{lem:op norm lower bound} is $\Omega(m + n)$.
% We now have two cases.
% If $\E \norm{A}_{\op} \leq C$, and therefore $q = 1$, then we can bound the failure probability as
% \begin{align*}
%   m^2 \left( \frac{\E\norm A_{\op} + C \sqrt{q}}{C' t \, \E \norm{A}_{\op}} \right)^{4q}
% \leq m^2 \left( \frac{2C}{C' t \, \E \norm{A}_{\op}} \right)^{4q}
% \leq m^2 t^{-4q},
% \end{align*}
% where the second inequality follows by choosing $C'$ large enough, as $\E\norm A_{\op}$ is bounded below by a universal constant (according to \cref{lem:op norm lower bound}).
% If instead $C \sqrt{q} \leq \E \norm{A}_{\op}$, then we bound the failure probability as
% \begin{align*}
%   m^2 \left( \frac{\E\norm A_{\op} + C \sqrt{q}}{C' t \, \E \norm{A}_{\op}} \right)^{4q}
% \leq m^2 \left( \frac{2 \E\norm A_{\op}}{C' t \, \E \norm{A}_{\op}} \right)^{4q}
% = m^2 \left( \frac{2}{C' t} \right)^{4q}
% \leq m^2 t^{-4q}
% \end{align*}
% where the seond inequality follows by choosing~$C'\geq2$.
% Thus, in either case the event~\eqref{eq:desired event} holds up to a failure probability of at most
% \begin{align*}
%   m^2 t^{-4q} = t^{-\Omega(m+n)},
% \end{align*}
% where we used that $q = \Omega(m + n)$ as well as the fact that $t\geq2$, so the prefactor $m^{2}$ can be absorbed at the cost of slightly changing the constant in the exponent.
\end{proof}


%=============================================================================

\section{Proof of the robustness lemma}\label{app:robust}
%=============================================================================
In this appendix we give a proof of \cref{convexRobustness}, which shows that strong convexity at a particular point implies strong convexity nearby.
First note that by \cref{remark:hessian-everywhere}, we have $\nabla^2 f_\samp(\Theta) = \nabla^2 f_{\samp'}$ where $\samp' = \Theta^{1/2} \samp$.
Thus we need only bound the difference between $f_\samp$ and $f_{\samp'}$ for $\|\log \Theta\|_{\op}$ small, $\Theta \in \P$.
For a matrix $\delta_{a}$ in $\Mat(d_a)$, we use
$e^{(\delta_{a})_{(a)}}$ to denote
$$ e^{(\delta_{a})_{(a)}} = I_{d_1} \otimes \cdots \otimes I_{d_{a-1}} \otimes e^{\delta} \otimes I_{d_{a+1}} \otimes \cdots \otimes I_{d_k},$$
as in \cref{definition:partial-trace}.
We will write $\Theta^{1/2}$ as $e^{\delta}$, where $\delta = \sum_{a=1}^k (\delta_{a})_{(a)}$.
%\MW{$\leftarrow$ $\delta$ is already a matrix in $\Mat(d_a)$, now it is a matrix tuple}
We now have
$\Theta^{1/2} = e^{\delta} = \otimes_{a = 1}^k e^{\delta_a}$, and $\frac{1}{2}\|\log \Theta\|_{\op} = \|\delta\|_{\op} = \sum_{a =1}^k \|\delta_{a}\|_{\op}$.
To bound the difference between $\nabla^2 f_{x'}$ and $\nabla^2 f_x$, we will show each component of the Hessian $\nabla f_{x'}$ (as presented in \cref{lem:hessian}) only changes (from $\nabla f_x$) by a small amount under the perturbation $x \to x' := e^\delta x$.
In particular we will give bounds on each block under each component-wise perturbation $x \to e^{(\delta_{a})_{(a)}} x$, and write the overall perturbation as a sequence of such component-wise perturbations. For convenience, we adopt the short-hand
$$ \rho_x:= \frac{1}{nD} x x^T.$$



We begin with an easy fact relating the exponential map and matrix norms.

\begin{fact} \label{f:expTaylor} For all symmetric $d\times d$ matrices $A$ such that $ \|A\|_{\op} \leq 1$, we have
$$ \|e^{A} - I\|_{\op} \leq 2 \|A\|_{\op}$$
and
$$ \|e^{A} - I\|_{F} \leq 2 \|A\|_{F}. $$
\end{fact}


%Recall the definition of a quadratic form of the Hessian:
%\[ \langle Z, (\nabla^{2} f) Z \rangle = \langle Z, (\nabla^{2} F) Z \rangle + \langle Z, (\nabla^{2} f - \nabla^{2} F) Z \rangle     \]
%The second term is rank one, so the quadratic form is:
%\[ \langle Z, (\nabla^{2} f - \nabla^{2} F) Z \rangle = \left( \sum_{a} \sqrt{d_{a}} \langle \rho^{(a)}, Z_{a} \rangle   \right)^{2}       \]
\noindent The $00$ component of the Hessian is a scalar $\nabla^{2}_{00} f = \tr[\rho]$, and for $a \geq 1$ we think of each $0a$ component as a vector:
\[ \sum_{a} \langle z_{0}, (\nabla^{2}_{0a} f) Z_{a} \rangle = z_{0} \langle \rho, \sum_{a} \sqrt{d_{a}} Z_{(a)} \rangle       \]
The diagonal components involve only one-body marginals of $\rho$:
\[ \langle Z_{a}, (\nabla^{2}_{aa} f) Z_{a} \rangle = \langle d_{a} \rho^{(a)}, Z_{a}^{2} \rangle       \]
And the off-diagonal components involve two-body marginals:
\[ \langle Z_{a}, (\nabla^{2}_{ba} f) Z_{b} \rangle =  \langle \sqrt{d_{a} d_{b}} \rho^{(ab)}, Z_{a} \otimes Z_{b} \rangle.   \]
Therefore in \cref{atoaaRobustness} and \cref{btoaaRobustness}, we will prove perturbation bounds on one-body marginals, and in \cref{btoabRobustness} we will prove perturbation bounds on two-body marginals. This will allow us to bound the change in the $0a$ components, diagonal components, and the off-diagonal components, respectively.
Then, following the structure of the proof of \cref{thm:tensor-convexity}, we will collect all the term-wise bounds to prove an overall bound at the end of the section.


%By the above discussion then we will bound the difference of each under each component-wise perturbation. Note the terms involve $\{\rho^{(a)}\}, \{\rho^{(ab)}\}$, so we prove perturbations on marginals in the following lemmas.

\begin{lemma} \label{atoaaRobustness}
For $\samp \in \R^{D \times n}$ and a symmetric matrix $\delta \in \Mat(d_{a})$ such that $\|\delta\|_{\op} \leq 1$, if we denote $\samp' := e^{\delta_{(a)}} \samp$ then
\[ \|\rho_{\samp'}^{(a)} - \rho_{\samp}^{(a)}\|_{\op} \leq 8 \|\delta\|_{\op} \|\rho_{\samp}^{(a)}\|_{\op}   . \]
\end{lemma}
\begin{proof}By definition, $\|\rho_{\samp'}^{(a)} - \rho_{\samp}^{(a)}\|_{\op} = \sup_{\|Z\|_{1} \leq 1} \langle Z_{(a)}, \rho_{\samp'} - \rho_{\samp} \rangle $.
Let $\delta' := e^{\delta} - I_{a}$. Note that $\|\delta'\|_{\op} \leq 2 \|\delta\|_{\op}$ by \cref{f:expTaylor} and our assumption $\|\delta\|_{\op} \leq 1$. Now
\begin{align*} \langle Z_{(a)}, \rho_{\samp'} - \rho_{\samp} \rangle &= \langle Z_{(a)}, (I+\delta')_a \rho_{\samp} (I+\delta')_a - \rho_{\samp} \rangle \\
& = \langle Z, \delta' \rho_\samp^{(a)} \rangle + \langle Z, \rho_\samp^{(a)} \delta' \rangle + \langle Z, \delta' \rho_\samp^{(a)} \delta' \rangle \\
& \leq (2\|\delta'\|_{\op} + \|\delta'\|_{\op}^{2}) \|\rho^{(a)}\|_{\op}\|Z\|_1  \\
&\leq 8 \|\delta\|_{\op} \|\rho^{(a)}\|_{\op}.\end{align*}
\end{proof}

%\CF{ I think we should combine these lemmas into a single one with two items.}\AR{The proofs are different, and I like the similar structure for diagonal/off-diagonal blocks. It may clutter the statements more to combine. }
\begin{lemma} \label{btoaaRobustness}
For $\samp \in \R^{D \times n}$ and symmetric matrix $\delta \in \Mat(d_{b})$ such that $\|\delta\|_{\op} \leq 1$, if we denote $\samp' := e^{\delta_{(b)}} \samp$ then for $b \neq a$:
\[ \|\rho_{\samp'}^{(a)} - \rho_{\samp}^{(a)}\|_{\op} \leq 2 \|\delta\|_{\op} \|\rho_{\samp}^{a}\|_{\op} .    \]
\end{lemma}
\begin{proof}
By definition, $\|\rho_{\samp'}^{(a)} - \rho_{\samp}^{(a)}\|_{\op} = \sup_{\|Z\|_{1} \leq 1, Z \succeq 0} |\langle Z_{(a)}, \rho_{\samp'} - \rho_{\samp} \rangle|$. \\
Let $\delta' := e^{\delta} - I_b$.
Note that $\|\delta'\|_{\op} \leq 2 \|\delta\|_{\op}$ by \cref{f:expTaylor} and our assumption $\|\delta\|_{\op} \leq 1$. \\
Now
\begin{align*}
|\langle Z_{(a)}, \rho_{\samp'} - \rho_{\samp} \rangle|
& = | \langle Z_{(a)}, e^{\delta_{(b)}}\rho_{\samp} e^{\delta_{(b)}} - \rho_{\samp} \rangle|\\
& = | \langle Z_{(a)} \delta'_{(b)}, \rho_{\samp} \rangle   |
= | \langle Z \otimes \delta', \rho_{\samp}^{(ab)} \rangle   | \\
% &\leq \langle Z \otimes |\delta_b'|, \rho_{\samp}^{(ab)} \rangle\\
&\leq \langle \|\delta'\|_{\op} Z \otimes  I_b , \rho_{\samp}^{(ab)} \rangle\\
&= \|\delta'\|_{\op} \langle Z, \rho_{\samp}^{(a)} \rangle \leq 2\|\delta\|_{\op} \|Z\|_1 \|\rho_{\samp}^{(a)}\|_{\op}.
\end{align*}
\end{proof}



This is already enough to prove a bound on $0a$ and $aa$ terms:
%and rank one term $(\nabla^{2} f - \nabla^{2} F)$.

\begin{corollary} \label{diagRobustness}
Let $\samp \in \R^{D \times n}$ be such that $\|d_{a} \rho_{\samp}^{(a)}\|_{\op} \leq 1 + \frac{1}{20}$, and for $b \in [k]$ let $\delta_b \in \Mat(d_b)$ be a  symmetric matrix such that $\sum_{b} \|\delta_{b}\|_{\op} \leq \frac{1}{8}$. Denoting $\delta_{(b)} := (\delta_b)_{(b)}$,  $\delta = \sum_b \delta_{(b)}$ and $x' = e^{\delta} \samp$, for $a \geq 1$ we have
\[ \|\nabla^{2}_{aa} f(e^{2\delta}) - \nabla^{2}_{aa} f(I)\|_{\op} \leq 25 \|\delta\|_{\op} .  \]

\end{corollary}
%\CF{technically $f(e^{\delta})$ corresponds to $e^{\delta/2} \samp$}
\begin{proof}
Recall from \cref{lem:hessian} that $\langle Y, (\nabla^{2}_{aa} f_{\samp}) Y \rangle = \langle d_{a} \rho_{\samp}^{(a)}, Y^{2} \rangle$; thus it is enough to show that $\|\rho_{\samp'}^{(a)} - \rho_{\samp}^{(a)}\|_{\op} \leq 25 \|\delta\|_{\op} /d_a$. We treat the perturbation $e^\delta$ as the composition of $k$ perturbations:
\[ \samp_{(0)}:=\samp \to \samp_{(1)}:= e^{\delta_{(1)}} \samp_{(0)} \to ... \to \samp_{(k)}:=e^{\delta_{(k)}} \samp_{(k-1)} = \samp'. \]
We can use \cref{atoaaRobustness} to handle $e^{\delta_{(a)}}$ and \cref{btoaaRobustness} for the rest. Let $Z$ be a symmetric matrix.
\begin{align*}
 |\langle \rho_{\samp'}^{(a)} - \rho_{\samp}^{(a)}, Z \rangle|
 &\leq \sum_{j=1}^{k} |\langle \rho_{\samp_{(j)}}^{(a)} - \rho_{\samp_{(j-1)}}^{(a)}, Z \rangle| \\
 &\leq \sum_{j=1}^{k}  8 \|\delta_{j}\|_{\op} \|\rho_{\samp_{(j-1)}}^{(a)}\|_{\op} \|Z\|_{1}.
\end{align*}
Where the last inequality is due to \cref{atoaaRobustness,btoaaRobustness}.
To bound each term in the right-hand side, note that by \cref{atoaaRobustness,btoaaRobustness} we have
$$\|\rho_{\samp_{(j)}}^{(a)}\|_{\op} \leq \|\rho_{\samp_{(j)}}^{(a)} - \rho_{\samp_{(j-1)}}^{(a)}\|_{\op} + \|\rho_{\samp_{(j-1)}}^{(a)}\|_{\op} \leq   (1+8 \|\delta_{j}\|_{\op})\|\rho_{\samp_{(j-1)}}^{(a)}\|_{\op}$$
and hence by induction the $j^{th}$ term in the sum is at most $$8 \|\delta_j\|_{\op} \left( \prod_{l=1}^k (1+8 \|\delta_{l}\|_{\op}) \right) \|\rho_{\samp}^{(a)}\|_{\op} \|Z\|_{1}.$$ By our assumption that $\sum_l \|\delta_l\|_{\op} \leq 1/8$, this is at most $8 \|\delta_j\|_{\op} e^{8 \sum \|\delta_l\|_{\op}} \|\rho_{\samp}^{(a)}\|_{\op} \|Z\|_{1} \leq 8e \|\delta_j\|_{\op} \|\rho_{\samp}^{(a)}\|_{\op} \|Z\|_{1}. $ Adding up the terms and using that $\|\delta\|_{\op} = \sum \|\delta_{(c)}\|_{\op}$, the overall sum is then at most $8 e \|\delta\|_{\op} \|\rho_{\samp}^{(a)}\|_{\op} \|Z\|_{1}$. Using our assumption on $\|d_{a} \rho_{\samp}^{(a)}\|_{\op}$ completes the proof.
\end{proof}

\begin{corollary} \label{constantRobustness}
Let $\samp \in \R^{D \times n}$ be such that $\|d_{a} \rho_{\samp}^{(a)}\|_{\op} \leq 1 + \frac{1}{20}$, and for $b \in [k]$ let $\delta_b$ be symmetric matrices such that $\|\sum_{b} \delta_{(b)}\|_{\op} = \sum_{b} \|\delta_{b}\|_{\op} \leq \frac{1}{8}$, where once again we denote $\delta_{(b)} := (\delta_b)_{(b)}$ and $\delta := \sum_b \delta_{(b)}$.
Denoting $\samp' := e^{\delta} \samp$, for $a \geq 1$ we have
\begin{align*} |\nabla^{2}_{00} f_{\samp'} - \nabla^{2}_{00} f_{\samp}| &\leq 5 \|\delta\|_{\op}  \\
\text{ and } \|\nabla^{2}_{0a} f_{\samp'} - \nabla^{2}_{0a} f_{\samp}\|_{\op} &\leq 25 \|\delta\|_{\op} .  \end{align*}
\end{corollary}
\begin{proof}
Recall from \cref{lem:hessian} that the $00$ component of the Hessian is just the scalar $\tr \rho$. The assumption that $\|d_{a} \rho_{\samp}^{(a)}\|_{\op} \leq 1 + \frac{1}{20}$ implies $\tr[\rho_{\samp}] = \tr \rho_{\samp}^{(a)} \leq 1 + 1/20$. Now we can use the approximation for $e^{\delta}$ in \cref{f:expTaylor}:
\[ |\tr[\rho_{\samp'} - \rho_{\samp}]| = |\langle \rho_{\samp}, e^{2\delta} - I \rangle| \leq \tr[\rho_{\samp}] \|e^{2 \delta} - I\|_{\op} \leq 5 \|\delta\|_{\op}     \]
In the last step we used our bound on $\tr[\rho_{\samp}].$
The $0a$ component is a vector, so it is enough to bound the inner product with any traceless matrix $Z$ of unit Frobenius norm:
\[ |\langle \rho_{\samp'}^{(a)} - \rho_{\samp}^{(a)}, \sqrt{d_{a}} Z \rangle| \leq \|\rho_{\samp'}^{(a)} - \rho_{\samp}^{(a)}\|_{\op} \sqrt{d_{a}} \|Z\|_{1}. \]
In the proof of \cref{diagRobustness} we showed under the same assumptions we have $\|\rho_{\samp'}^{(a)} - \rho_{\samp}^{(a)}\|_{\op} \leq 25 \|\delta\|_{\op}/d_a$, from which it follows that the above is at most $25 \|\delta\|_{\op} \|Z\|_{F}.$ \end{proof}

%\begin{corollary} \label{rankoneRobustness}
%For input $x \in \R^{nD}$ such that for all $a \in [k]$ such that $\|d_{a} \rho_{\samp}^{(a)} - \tr[\rho_{\samp}] I_{a}\|_{\op} \leq \frac{1}{20}$; and perturbation $\delta := \sum_{a} (\delta_{a} \in \Mat(d_{a}))_{a}$ such that $\|\delta\|_{\op} = \sum_{a} \|\delta_{a}\|_{\op} \leq \frac{1}{20}$; if we denote $\samp' := e^{\delta} \samp$, then we have:
%\[ \|(\nabla^{2} f_{\samp'} - \nabla^{2} F_{\samp'}) - (\nabla^{2} f_{\samp} - \nabla^{2} F_{\samp})\|_{\op} \leq 1.5 k \|\delta\|_{\op}     \]
%\end{corollary}
%\begin{proof}
%Recall again from the discussion after $\cref{convexRobustness}$ that $\langle Z, (\nabla^{2} F - \nabla^{2} f) Z \rangle = \left\langle \rho, \sum_{a} \sqrt{d_{a}} (Z_{a})_{a}  \right\rangle^{2}$. We use the same iterative strategy as $\cref{diagRobustness}$:
%\[    \left\langle \rho_{\samp'}, \sum_{a} \sqrt{d_{a}} (Z_{a})_{a}  \right\rangle^{2} -  \left\langle \rho_{\samp}, \sum_{a} \sqrt{d_{a}} (Z_{a})_{a}  \right\rangle^{2}    \]
%\[ = \left\langle \rho_{\samp'} + \rho_{\samp}, \sum_{a} \sqrt{d_{a}} (Z_{a})_{a}  \right\rangle \left\langle \rho_{\samp'} - \rho_{\samp}, \sum_{a} \sqrt{d_{a}} (Z_{a})_{a}  \right\rangle  \]
%\[ = \left( \sum_{a} \langle (d_{a} \rho_{\samp'}^{(a)} - \tr[\rho_{\samp'}] I_{a}) + (d_{a} \rho_{\samp}^{(a)} - \tr[\rho_{\samp}] I_{a}) , d_{a}^{-1/2} Z_{a} \rangle \right) \left( \sum_{a} \sqrt{d_{a}} \langle \rho_{\samp'}^{(a)} - \rho_{\samp}^{(a)}, Z_{a} \rangle \right)     \]
%%\AR{Here I could use that $Z \perp I$ to get a constant factor improvement; need an assumption on $\nabla$; but it improves the overall constant by factor $\approx 2$}
%\[ \leq \left( \sum_{a} (2 + 10 \|\delta\|_{\op}) \|d_{a} \rho_{\samp}^{(a)} - \tr[\rho_{\samp}] I_{a} \|_{\op} \|d_{a}^{-1/2} Z_{a}\|_{1}   \right)
%\left( 10\|\delta\|_{\op} \sum_{a} \sqrt{d_{a}} \| \rho_{\samp}^{(a)}\|_{\op} \|Z_{a}\|_{1}   \right)    \]
%\[ \leq \left( \sum_{a} \frac{2 + 10 \|\delta\|_{\op}}{20} \|Z_{a}\|_{F}  \right)
%\left( 10\|\delta\|_{\op} \sum_{a} (1 + \frac{1}{20}) \|Z_{a}\|_{F} \right)
%\leq 1.5 k \|\delta\|_{\op} \|Z\|^{2}      \]
%%\AR{What norm are we using on $Z$ as a whole? Should I keep around the $d_{a}$'s?} \CF{the norm is just the standard norm, there shouldn't be $d_a$'s}
%% The final step follows from the initial conditions on $\|d_{a} \rho_{a}\|_{\op}, \|d_{b} \rho_{b}\|_{\op}$.
%In the third line we used that $Z$ is traceless; in the last line we used our initial conditions on $\rho$; the last step was by Cauchy-Schwarz.
%\end{proof}


The off-diagonal components require the following two lemmata on bipartite marginals:

\begin{lemma} \label{btoabRobustness}
For $\samp \in \R^{D \times n}$ and a symmetric matrix $\delta \in \Mat(d_{c})$ such that $\|\delta\|_{\op} \leq \frac{1}{8}$; if we denote $\samp' := e^{\delta_{(c)}} \samp$, then for $c \in \{a,b\}$ we have
\[ \sup_{Y \in \smallSym_{d_{a}}^{0}, Z \in \smallSym_{d_{b}}^{0}} \frac{|\langle \rho_{\samp'}^{(ab)} - \rho_{\samp}^{(ab)}, Y \otimes Z \rangle|}{\|Y\|_{F} \|Z\|_{F}} \leq 3 \|\delta\|_{\op} \sup_{Y \in \smallSym_{d_{a}}, Z \in \smallSym_{d_{b}}} \frac{\langle \rho_{\samp}^{(ab)}, Y \otimes Z \rangle}{\|Y\|_{F} \|Z\|_{F}}.        \]
Note that $\smallSym_{d}^{0}$ are traceless symmetric matrices, whereas $\smallSym_{d}$ are symmetric matrices.
%\[ \|\nabla^{2}_{ab} F_{\samp'} - \nabla^{2}_{ab} F_{\samp}\|_{0} \leq 4.5 \|\delta\|_{\op} \|\nabla^{2}_{ab} F_{\samp}\|_{F \to F}    \]
\end{lemma}
\begin{proof}
%Recall from the discussion after $\cref{convexRobustness}$ that $\langle Y, (\nabla^{2}_{ab} F) Z \rangle = \sqrt{d_{a} d_{b}} \langle \rho^{(ab)}, Y \otimes Z \rangle$.
By taking adjoints, we can assume w.l.o.g. that $c = b$. Let $R : \Mat(d_{b}) \to \Mat(d_{b})$ be defined as $R(Z) :=  e^{\delta} Z e^{\delta}$. Then
% defined by our normalization $\|(e^{\delta})_{b} \samp\|_{2}^{-1} =: (1+\eta) \|\samp\|_{2}^{-1}$.
\[ |\langle \rho_{\samp'}^{(ab)} - \rho_{\samp}^{(ab)}, Y \otimes Z \rangle| = |\langle \rho_{\samp}^{(ab)}, Y \otimes (R(Z) - Z) \rangle|  \]
The subspace $\smallSym_{d_{b}}^{0}$ is not invariant under $R$, but we show $R \approx I$. Let $\delta' :=  e^{\delta} - I$; by \cref{f:expTaylor}, $\|\delta'\|_{\op} \leq \frac{1}{4}$. Now
\[ \|R(Z) - Z\|_{F} \leq 2 \|\delta' Z\|_{F} + \|\delta' Z \delta'\|_{F} \leq (2 \|\delta'\|_{\op} + \|\delta'\|_{\op}^{2}) \|Z\|_{F} \leq 3\|\delta\|_{\op}\|Z\|_{F}.    \]
We combine these inequalities and apply a change of variables $R(Z) - Z \leftarrow Z'$ to finish the proof.
\begin{align*} \sup_{Y \in \smallSym_{d_{a}}^{0}, Z \in \smallSym_{d_{b}}^{0}} \frac{|\langle \rho_{\samp'}^{(ab)} - \rho_{\samp}^{(ab)}, Y \otimes Z \rangle|}{\|Y\|_{F} \|Z\|_{F}}
& = \sup_{Y \in \smallSym_{d_{a}}^{0}, Z \in \smallSym_{d_{b}}^{0}}\frac{|\langle \rho_{\samp}^{(ab)}, Y \otimes (R(Z) - Z) \rangle|}{\|Y\|_F\|Z\|_F} \\
&\leq \sup_{Y \in \smallSym_{d_{a}}^{0}, Z' \in \smallSym_{d_{b}}} \frac{|\langle \rho_{\samp}^{(ab)}, Y \otimes Z' \rangle| \cdot 3 \|\delta\|_{\op}}{\|Y\|_F\|Z'\|_F}.
\end{align*}
\end{proof}

\begin{lemma} \label{ctoabRobustness}
For $\samp \in \R^{D \times n}$ and a symmetric matrix $\delta \in \Mat(d_{c})$ such that $\|\delta\|_{\op} \leq \frac{1}{8}$; if we denote $\samp' := e^{\delta_{(c)}}  \samp$, then for $c \not\in \{a,b\}$ we have
\begin{align*}\sup_{Y \in \smallSym_{d_{a}}^{0}, Z \in \smallSym_{d_{b}}^{0}} \frac{|\langle \rho_{\samp'}^{(ab)} - \rho_{\samp}^{(ab)}, Y \otimes Z \rangle|}{\|Y\|_{F} \|Z\|_{F}} \leq 4 \|\delta\|_{\op} \sup_{Y \in \smallSym_{d_{a}}, Z \in \smallSym_{d_{b}}} \frac{\langle \rho_{\samp}^{(ab)}, Y \otimes Z \rangle}{\|Y\|_{F} \|Z\|_{F}} .     \end{align*}
%\[ \|\nabla^{2}_{ab} F_{\samp'} - \nabla^{2}_{ab} F_{\samp}\|_{0} \leq 19 \|\delta\|_{\op} \|\nabla^{2}_{ab} F_{\samp}\|_{F \to F}    \]
\end{lemma}
\begin{proof}
Let $\delta' := e^{2 \delta} - I_{c}$ so that $|\langle \rho_{\samp'}^{(ab)} - \rho_{\samp}^{(ab)}, Y \otimes Z \rangle| =  |\langle \rho_{\samp}^{(abc)}, Y \otimes Z \otimes \delta' \rangle|$.
We first assume $Y,Z \succeq 0, $ and without loss of generality we assume that $\|Y\|_{F} = \|Z\|_{F} = 1$. Because $\rho_{\samp}^{(abc)}, Y, Z \succeq 0,$ and $\delta' \preceq \norm{\delta'}_{\op} \cdot I_c$, we have
%\[ \frac{1}{\sqrt{d_{a} d_{b}} } \langle Y, (\nabla^{2}_{ab} F_{\samp'} - \nabla^{2}_{ab} F_{\samp}) Z \rangle = \langle \rho_{\samp}^{(abc)}, Y \otimes Z \otimes \delta' \rangle   \]
\begin{align*}
|\langle \rho_{\samp}^{(abc)}, Y \otimes Z \otimes \delta' \rangle|
& \leq \langle \rho_{\samp}^{(abc)}, Y \otimes Z \otimes \norm{\delta'}_{\op} \cdot I_c \rangle\\
& \leq \|\delta'\|_{\op} \langle \rho_{\samp}^{(ab)}, Y \otimes Z \rangle \leq 2 \|\delta\|_{\op} \langle \rho_{\samp}^{(ab)}, Y \otimes Z \rangle, \end{align*}
where the last inequality is by \cref{f:expTaylor}.
 %We cannot bound this by $c_{0}$ as $Y \succeq 0$, but the RHS $c$ is sufficient.
To finish the proof we decompose $Y = Y_{+} - Y_{-}, Z = Z_{+} - Z_{-}$, where $Y_+, Y_-, Z_+, Z_-$ are all positive semidefinite, and bound
\begin{align*} |\langle \rho_{\samp'}^{(ab)} - \rho_{\samp}^{(ab)}, Y \otimes Z \rangle|
& \leq \sum_{s,t \in \{+,-\}} |\langle \rho_{\samp'}^{(ab)} - \rho_{\samp}^{(ab)}, Y \otimes Z \rangle| \\
& \leq \sum_{s,t \in \{+,-\}} 2\|\delta\|_{\op} \langle \rho_{\samp}^{(ab)}, Y_s \otimes Z_t \rangle\\
&\leq 2\left( \sup_{Y \in \smallSym_{d_{a}}, Z \in \smallSym_{d_{b}}} \frac{\langle \rho_{\samp}^{(ab)}, Y \otimes Z \rangle}{\|Y\|_{F} \|Z\|_{F}} \right) \|\delta\|_{\op} \sum_{s,t \in \{+,-\}} \|Y_{s}\|_{F} \|Z_{t}\|_{F}
\end{align*}
The Cauchy Schwarz inequality allows us to bound the summation:
\[\sum_{s,t \in \{+,-\}} \|Y_{s}\|_{F} \|Z_{t}\|_{F}  \leq (2\|Y_{+}\|_{F}^{2} + 2\|Y_{-}\|_{F}^{2})^{1/2} (2\|Z_{+}\|_{F}^{2} + 2\|Z_{-}\|_{F}^{2})^{1/2} = 2 \|Y\|_{F} \|Z\|_{F} .   \]
Plugging this bound in to the supremum on the left-hand side in the statement of the lemma completes the proof.
\end{proof}

The following definition will be helpful for translating the above lemmas into statements about the Hessian.
\begin{definition}
For a linear map $M : \Mat(d) \to \Mat(d')$, we let $\|M\|_{0}$ denote the $F \to F$ norm of its restriction to the traceless subspaces $\smallSym^0_{d} \to \smallSym^0_{d'}$, i.e.
$$ \|M\|_0 = \sup_{Z \in \Sym^0_{d}} \frac{\|M(Z) - \frac{\tr M(Z)}{d'} I_{d'}\|_F}{\|Z\|_F}.$$
\end{definition}
The following lemma will be helpful.

\begin{lemma}[\cite{KLR19}] \label{inftyto2} For $x \in \R^{D \times n}$,
$$\|\nabla^{2}_{ab} f_{\samp}\|_{F \to F}^{2} \leq \|d_{a} \rho_{\samp}^{(a)}\|_{\op} \|d_{b} \rho_{\samp}^{(b)}\|_{\op}.$$
\end{lemma}
%\begin{proof}This was already in KLR and we have two new proofs: one by convexity, and one by Riesz-Thorin. \AR{The proofs are in some other file, we can add it if we like}\end{proof}
Analogously to the proof of \cref{diagRobustness}, we can now combine \cref{btoabRobustness} and \cref{ctoabRobustness} to bound the effect of a perturbation with more than one nontrivial tensor factor.

\begin{corollary} \label{offdiagRobustness}
Let $\samp \in \R^{D \times n}$ be such that $\|d_{a} \rho_{\samp}^{(a)}\|_{\op}, \|d_{b} \rho_{\samp}^{(b)}\|_{\op} \leq 1+\frac{1}{20}$, and for $c \in [k]$ let $\delta_c$ be a symmetric matrix such that $\|\sum_{c} \delta_{(c)}\|_{\op} = \sum_{c} \|\delta_{c}\|_{\op} \leq \frac{1}{8}$. Denoting $\samp' := e^{\delta} \samp$, we have
%\[ \frac{1}{\sqrt{d_{a} d_{b}}} \|\nabla^{2}_{ab} f(e^{2 \delta}) - \nabla^{2}_{ab} f(I)\|_{\op} \leq 100 \|\delta\|_{\op} \sqrt{\|\rho_{\samp}^{(a)}\|_{\op} \|\rho_{\samp}^{(b)}\|_{\op}}     \]
\[ \|\nabla^{2}_{ab} f_{\samp'} - \nabla^{2}_{ab} f_{\samp}\|_{0} \leq 21 \|\delta\|_{\op}  \]
\end{corollary}
\begin{proof}
First, using \cref{lem:hessian}, we write the left-hand and right-hand sides of the inequalities in \cref{btoabRobustness} and \cref{ctoabRobustness} in terms of the Hessian:
\begin{align*}
\sup_{Y \in \smallSym_{d_{a}}^{0}, Z \in \smallSym_{d_{b}}^{0}} \frac{\langle \rho_{\samp'}^{(ab)} - \rho_{\samp}^{(ab)}, Y \otimes Z \rangle}{\|Y\|_{F} \|Z\|_{F}} &= \frac{\|\nabla^{2}_{ab} f_{\samp'} - \nabla^{2}_{ab} f_{\samp}\|_{0}}{\sqrt{d_{a} d_{b}} }, \\
\text{ and }\sup_{Y \in \smallSym_{d_{a}}, Z \in \smallSym_{d_{b}}} \frac{\langle \rho_{\samp}^{(ab)}, Y \otimes Z \rangle}{\|Y\|_{F} \|Z\|_{F}} &= \frac{\|\nabla^{2}_{ab} f_{\samp}\|_{F \to F}}{\sqrt{d_{a} d_{b}} }   .
\end{align*}
Using the same iterative strategy as in the proof of \cref{diagRobustness} for the left-hand sides of the above identities, we have
\[ |\langle Y, (\nabla^{2}_{ab} f_{\samp'} - \nabla^{2}_{ab} f_{\samp}) Z \rangle| \leq 20 \|\delta\|_{\op} \|\nabla^{2}_{ab} f_{\samp}\|_{F \to F} \|Y\|_{F} \|Z\|_{F} ,   \]
using \cref{btoabRobustness} for $a$ and $b$ and \cref{ctoabRobustness} for the rest. Finally, we may rewrite \cref{inftyto2} using \cref{lem:hessian} to find $\|\nabla^{2}_{ab} f_{\samp}\|_{F \to F}^{2} \leq \|d_{a} \rho_{\samp}^{(a)}\|_{\op} \|d_{a} \rho_{\samp}^{(a)}\|_{\op}$. Using our assumption that $\|d_{a} \rho_{a}\|_{\op}, \|d_{b} \rho_{b}\|_{\op} \leq 1 + \frac{1}{20}$ completes the proof.
\end{proof}

Now we can combine the above term-by-term bounds to bound the change in the Hessian.

\begin{proof} [Proof of \cref{convexRobustness}]
The above corollaries (\ref{constantRobustness},\ref{diagRobustness},\ref{offdiagRobustness}) require $\|d_{a} \rho^{(a)}\|_{\op} \leq 1 + \frac{1}{20}$ , which are implied by our assumption on the gradient:
\[ \|d_{a} \rho^{(a)}\|_{\op} \leq 1 + |\tr \rho - 1| + \|d_{a} \rho^{(a)} - (\tr \rho) I_{d_{a}} \|_{\op}    \]
\[ = 1 + |\nabla_{0} f| + \|\sqrt{d_{a}} \nabla_{a} f\|_{\op} \leq 1 + 2 \eps_0 ,    \]
 so choosing $\eps_0 \leq \frac{1}{40}$ suffices.
Recall the expression of the Hessian as a quadratic form evaluated on $Z = (z_0, Z_1, \dots, Z_k):$
\begin{align*} \langle Z, (\nabla^{2} f) Z \rangle&=\\
  z_{0} &(\nabla^{2}_{00} f) z_{0} + 2 \sum_{a} \langle z_{0}, (\nabla^{2}_{0a} f) Z_{a} \rangle + \sum_{a} \langle Z_{a}, (\nabla^{2}_{aa} f) Z_{a} \rangle + \sum_{a \neq b} \langle Z_{a}, (\nabla^{2}_{ab} f) Z_{b} \rangle .  \end{align*}
%\[ \langle Z, (\nabla^{2} f) Z \rangle = \langle Z, (\nabla^{2} F) Z \rangle + \langle Z, (\nabla^{2} f - \nabla^{2} F) Z \rangle     \]
%\[ = \sum_{a} \langle Z_{a}, (\nabla^{2}_{aa} F) Z_{a} \rangle + \sum_{a \neq b} \langle Z_{a}, (\nabla^{2}_{ab} F) Z_{b} \rangle - \left( \sum_{a} \sqrt{d_{a}} \langle \rho^{(a)}, Z_{a} \rangle  \right)^{2}       \]
Let $\samp' := e^{\delta} \samp$. Then by \cref{constantRobustness} we have a bound on the $0a$ terms:
\[ | z_{0}^{2} (\nabla^{2}_{00} f_{\samp'} - \nabla^{2}_{00} f_{\samp} ) + 2 \sum_{a} \langle z_{0}, (\nabla^{2}_{0a} f_{\samp'} - \nabla^{2}_{0a} f_{\samp}) Z_{a} \rangle |      \]
\[ \leq 5 \|\delta\|_{\op} z_{0}^{2} + (2 |z_{0}|) 25 \|\delta\|_{\op} \sum_{a} \|Z_{a}\|_{F}
\leq \|\delta\|_{\op} (17 k z_{0}^{2} + 25 \sum_{a} \|Z_{a}\|_{F}^{2})   \]
In the last step we used Young's inequality ($2pq \leq p^{2} + q^{2}$) for each term with $p = z_0$, $q = \|Z_{a}\|_F$.
%and by $\cref{rankoneRobustness}$ we have a bound on the rank-one term.
%\[ \langle Z, (\nabla^2 f_{\samp'} - \nabla^{2} f_{\samp}) Z \rangle \leq \|\delta\|_{\op} \left( 11 \sum_{a} \|Z_{a}\|_{F}^{2} + 21 \sum_{a \neq b} \|Z_{a}\|_{F} \|Z_{b}\|_{F} + 1.5 k \|Z\|^{2} \right)   \]
%\[ \leq (11 + 21(k-1) + 1.5 k) \|\delta\|_{\op} \|Z\|^{2}    \]

By \cref{diagRobustness} we have a bound on the diagonal terms, and by \cref{offdiagRobustness} we have a bound on the off-diagonal terms:
\[ |\sum_{ab} \langle Z_{a}, (\nabla^{2}_{ab} f_{\samp'} - \nabla^{2}_{ab} f_{\samp} ) Z_{b} \rangle | \leq \|\delta\|_{\op} \left( 25 \sum_{a} \|Z_{a}\|_{F}^{2} + 21 \sum_{a \neq b} \|Z_{a}\|_{F} \|Z_{b}\|_{F} \right)   \]
\[ \leq (25 + 21(k-1)) \|\delta\|_{\op} \left( \sum_{a} \|Z_{a}\|_{F}^{2} \right)   \]
So combining all three terms we see:
\[ |\langle Z, (\nabla^{2} f_{\samp'} - \nabla^{2} f_{\samp} ) Z \rangle | \leq \|\delta\|_{\op} \left( 17 k z_{0}^{2} + (25 + 25 + 21 (k-1)) \sum_{a} \|Z_{a}\|_{F}^{2} \right)    \]
\[ \leq 50 k \|\delta\|_{\op} \left( z_{0}^{2} + \sum_{a} \|Z_{a}\|_{F}^{2} \right)  = 50 k \|\delta\|_{\op} \|Z\|^2.  \]
Note that this also gives an upper bound for $\|\nabla^{2} f_{\samp'}\|_{\op}$.
%\AR{What norm are we using on $Z$ as a whole? Should I keep around the $d_{a}$'s? Also the constant is $50 k$ now, yay for better constants!}\CF{yay!}
\end{proof}

%=============================================================================
\section{The Cheeger constant of a random operator}\label{app:cheeky}
%=============================================================================
In this appendix we prove \cref{thm:operator-cheeger}, which asserts that a random completely positive map with sufficiently many Kraus operators is an almost quantum expander with exponentially small failure probability.
To prove the theorem, we first define the Cheeger constant of completely positive map.
This is similar to a concept defined in \cite{H07}.

\begin{definition}\label{def:cheeger}
Let $\Phi \colon \Mat(d_2) \to \Mat(d_1)$ be a completely positive map.
The \emph{Cheeger constant} $\ch(\Phi)$ is given by
\begin{align*}
  \ch(\Phi) := \min_{\substack{\Pi_1, \Pi_2 \\ 0 < \vol(\Pi_1, \Pi_2) \leq \frac12 \vol(I_{d_1}, I_{d_2})}} \phi(\Pi_1,\Pi_2)
\end{align*}
where $\Pi_1\colon \C^{d_1} \to \C^{d_1}$ and $\Pi_2\colon \C^{d_2} \to \C^{d_2}$ are orthogonal projections, and the \emph{conductance}~$\phi(\Pi_1, \Pi_2)$ of the ``cut'' $\Pi_1, \Pi_2$ is defined to be
\begin{align*}
  \phi(\Pi_1,\Pi_2) := \frac{\cut(\Pi_1, \Pi_2)}{\vol(\Pi_1,\Pi_2)},
\end{align*}
where
\begin{align*}
  \vol(\Pi_1,\Pi_2) &:= \tr \Phi(\Pi_2) + \tr \Phi^*(\Pi_1), \\
  \cut(\Pi_1, \Pi_2) &:= \tr (I_{d_1} - \Pi_1) \Phi(\Pi_2) + \tr \Pi_1 \Phi(I_{d_2} - \Pi_2) \\
  &= \tr (I_{d_1} - \Pi_1) \Phi(\Pi_2) + \tr \Phi^*(\Pi_1) (I_{d_2} - \Pi_2).
\end{align*}
\end{definition}

The key connection that we will leverage to prove \cref{thm:operator-cheeger} is that a large Cheeger constant implies quantum expansion:

\begin{lemma} [\cite{FM20}, Remark 5.5]\label{lem:op-cheeger}
There exist absolute constants~$c, C>0$ such that if $\Phi$ is a completely positive map that is $\eps$-doubly balanced for some~$\eps < c \ch(\Phi)^2$, then $\Phi$ is an $(\eps,\lambda)$-quantum expander, where
\begin{align*}
  \lambda = \max\left\{ \frac12, 1 -  \ch(\Phi)^2 + C \frac{\eps}{\ch(\Phi)^2} \right\}.
\end{align*}
\end{lemma}

For intuition, consider a weighed bipartite graph $G$ on $[d_1] \cup [d_2]$.
The projections~$\Pi_1$ and~$\Pi_2$ are analogous to subsets~$A \subset[d_1]$ and~$B \subset [d_2]$, respectively.
The quantity $\vol(\Pi_1, \Pi_2)$ is analogous to the total mass of the edges adjacent to $A$ plus that of the edges adjacent to $B$, which is the volume of $A \cup B$ considered as a cut of $G$.
% \MW{Why does one count edges between $A$ and $B$ in the bipartite setting?}\CF{I agree its weird. I think it's because the same def is used for bipartite and non-bipartite.}
The quantity $\cut(\Pi_1, \Pi_2)$ corresponds to the total mass of the edges between~$A \cup B$ and its complement, that is, to the weight of the cut defined by~$A \cup B$.
In fact, if the Cheeger constant were defined with~$\Pi_1$ and~$\Pi_2$ restricted to be coordinate projections, it would be exactly the Cheeger constant of the bipartite graph on $[d_1]$ and $[d_2]$ with edge $(i,j)$ weighted by $\tr e_i e_i^T \Phi(e_j e_j^T)$, and the volume and the cut would be the same as the volume and the cut on that bipartite graph.

For the remainder of this section let $\Phi = \Phi_X$ where $X_1, \dots, X_n$ are random $d_1 \times d_2$ matrices with independent standard Gaussian entries.
In this case, each edge-weight $\tr e_i e_i^T \Phi(e_j e_j^T)$ of the bipartite graph is an independent $\chi^2$ random variable with~$n$ degrees of freedom.
Accordingly:

\begin{lemma}\label{fact:chi}
Let $\Pi_1\colon\C^{d_1} \to \C^{d_1}$, $\Pi_2\colon \C^{d_2} \to \C^{d_2}$ be orthogonal projections of rank~$r_1$ and~$r_2$, respectively.
Then $\cut(\Pi_1, \Pi_2)$, $\vol(\Pi_1, \Pi_2)$, $\vol(I_{d_1}, I_{d_2})$ are jointly distributed as
\begin{align*}
  R_1, \, R_1 + 2R_2, \, 2R_1 + 2 R_2 + 2R_3,
\end{align*}
where $R_1$, $R_2$, $R_3$ are independent $\chi^2$ random variables with $F_1:=n r_1(d_2 - r_2) + n r_2(d_1-r_1)$, $F_2:= n r_1r_2$, and $F_3:= n(d_1 - r_1)(d_2 - r_2)$ degrees of freedom, respectively.
\end{lemma}
\begin{proof} As the distribution of $\Phi_X$ is invariant under the action $(U_1, U_2) \cdot \Phi_X(Y) =  U_1\Phi_X( U_2^T Y U_2) U_1^T$ of unitary matrices $U_1, U_2$, the joint distribution of $\cut(\Pi_1, \Pi_2)$, $\vol(\Pi_1, \Pi_2)$ depends only on the rank of~$\Pi_1$, $\Pi_2$. Thus we may compute in the case that $\Pi_1$ and $\Pi_2$ are coordinate projections, in which case one may verify the fact straightforwardly; see the discussion above.
\end{proof}

We next show a sufficient condition for the Cheeger constant being bounded away from $1$ that is amenable to the previous lemma.

\begin{lemma}\label{lem:suff}
Let $\Phi$ be a completely positive map and $\delta<0.005$ such that the following hold for all orthogonal projections $\Pi_1\colon \C^{d_1} \to \C^{d_1}$, $\Pi_2 \colon \C^{d_2}\to\C^{d_2}$, not both zero, where we denote by~$r_1$,~$r_2$ the ranks of $\Pi_1$ and $\Pi_2$, respectively, and abbreviate $F_1 := n r_1(d_2 - r_2) + n r_2(d_1-r_1)$, and $F_2 := n r_1 r_2$ as in \cref{fact:chi}:
\begin{enumerate}
\item If $F_2 \geq \frac49 n d_1 d_2$, then
\begin{align}\label{eq:vol}
  \vol(\Pi_1, \Pi_2)
\geq \left(\frac{101}{200} - \delta\right) \vol(I_{d_1}, I_{d_2})
= \left(1.01 - 2\delta\right) \tr \Phi(I_{d_2}).
\end{align}
\item If $F_2 < \frac49 n d_1 d_2$ and $\vol(\Pi_1, \Pi_2)>0$, then
\begin{align}\label{eq:cut}
  \vol(\Pi_1, \Pi_2) \leq \left(\frac43 + \delta\right)\left(F_1 + 2 F_2\right) \quad\text{and}\quad
  \cut(\Pi_1, \Pi_2) \geq \left(\frac23 - \delta\right) F_1.
\end{align}
\end{enumerate}
Then $\ch(\Phi) \geq \frac16 - O(\delta)$.
\end{lemma}
\begin{proof}
The first assumption implies we only need to reason about the case that $F_2 < \frac49 n d_1 d_2$.
This is because the minimization in the definition of the Cheeger constant is over $\Pi_1$, $\Pi_2$ such that $\vol(\Pi_1, \Pi_2) \leq \tr \Phi(I_{d_2})$.
Therefore, the second assumption implies that
\begin{align*}
  \ch(\Phi)
\geq \frac{\frac43 + \delta}{\frac23 - \delta} \min_{F_2 < \frac49 n d_1 d_2} \frac{F_1}{F_1 + 2 F_2}
= \left( \frac12 - O(\delta) \right) \min_{r_1 r_2 < \frac49 d_1 d_2} \frac{F_1}{F_1 + 2 F_2}.
\end{align*}
It suffices to show that $F_1/(F_1 + 2 F_2) \geq 1/3$ provided $r_1 r_2 < \frac49 d_1 d_2$.
Indeed, if either $r_1 = 0$ or $r_2 = 0$, then $F_2 = 0$ and $F_1>0$ and the claim holds.
Otherwise, if $r_1>0$ and $r_2>0$, then
\begin{align*}
  \frac{F_1}{F_1 + 2 F_2}
&= \frac{r_1 d_2 + r_2 d_1 - 2 r_1 r_2}{r_1 d_2 + r_2 d_1}
= 1 - \frac{2 r_1 r_2}{r_1 d_2 + r_2 d_1} \\
&= 1 - \sqrt{\frac{r_1r_2}{d_1d_2}} \frac2{\sqrt{\frac{r_1 d_2}{r_2 d_1}} + \sqrt{\frac{r_2 d_1}{r_1 d_2}}}
% &\geq 1 - \sqrt{\frac{r_1r_2}{d_1d_2}}
\geq 1 - \sqrt{\frac49}
% = 1 - \frac23
= \frac13.
\end{align*}
In the last inequality we used that $a + a^{-1} \geq 2$ for all $a>0$ and that $r_1 r_2 < \frac49 d_1 d_2$.
\end{proof}

Next we use \cref{fact:chi} to show that for a random completely positive map, the events in \cref{lem:suff} hold with high probability for any \emph{fixed} $\Pi_1$ and $\Pi_2$. We also need a third bound which we will use to transfer properties of a $\delta$-net to the whole space of projections.

\begin{lemma}\label{lem:probabilities}
Suppose $d_1 \leq d_2$. Let $\Pi_1\colon \C^{d_1} \to \C^{d_1}$, $\Pi_2\colon \C^{d_2} \to \C^{d_2}$ be orthogonal projections of rank~$r_1$ and~$r_2$, respectively such that $r_1 + r_2 > 0$.
Let $F_1:= n r_1(d_2 - r_2) + n r_2(d_1-r_1)$ and $F_2 = n r_1 r_2$.
Then, the following holds for the random completely positive map $\Phi=\Phi_X$:
\begin{enumerate}
\item If $F_2 \geq \frac49 n d_1 d_2$, then \cref{eq:vol} holds with $\delta = 0$ with probability at least $1 - e^{-\Omega( n d_1 d_2)}$.
\item If $F_2 < \frac49 n d_1 d_2$, then \cref{eq:cut} holds with $\delta = 0$ with probability at least $1 - e^{-\Omega( F_1)}$.
\item Finally, $\vol(\Pi_1, \Pi_2) \geq \frac{1}{4d_2}  \vol(I_{d_1}, I_{d_2})$ with probability at least $1 - e^{- \Omega(F_1 + 2F_2)}$.
% $1 - e^{- \Omega(n r_1 d_2 + n r_2 d_1)}$.
\end{enumerate}
\end{lemma}
\begin{proof}
Recall from \cref{fact:chi} that $\cut(\Pi_1, \Pi_2)$, $\vol(\Pi_1, \Pi_2)$, $\vol(I_{d_1}, I_{d_2})$ are jointly distributed as $R_1$, $R_1 + 2R_2$, $2R_1 + 2R_2 + 2R_3$ for $R_1, R_2, R_3$ independent $\chi^2$ random variables with $F_1$, $F_2$, and $F_3 = n(d_1-r_1)(d_2-r_2)$ degrees of freedom, respectively.
In view of \cref{eq:vol,eq:cut} with $\delta=0$, it is thus enough to show that
\begin{enumerate}
\item If $F_2 \geq \frac49 n d_1 d_2$, then $R_2 \geq \frac1{99} R_1 + \frac{101}{99} R_3$ with probability $1 - e^{-\Omega( n d_1 d_2)}$.
\item If $F_2 < \frac49 n d_1 d_2$, then $R_1 + 2R_2 \leq \frac43 (F_1 + 2F_2)$ and $R_1 \geq \frac23 F_1$ hold with probability $1 - e^{-\Omega(F_1)}$.
\item $R_1 + 2 R_2 \geq \frac23 (F_1 + 2F_2)$ and $R_1 + R_2 + R_3 \leq \frac43 (F_1 + F_2 + F_3)$ with probability $1 - e^{- \Omega(F_1 + 2F_2)}$.
\end{enumerate}
Indeed, the first (resp.\ second) item above implies the first (resp.\ second) item in the lemma by substituting the expressions for $\vol$ and $\cut $ of $(\pi_1, \pi_2)$ and $(I_{d_1}, I_{d_2})$ in terms of $R_1, R_2, R_3$. The last item follows from the same reasoning combined with the inequality $F_1 + 2 F_2 \geq \frac{1}{d_2} (F_1 + F_2 + F_3)$ for $r_1, r_2$ not both zero and the fact that $d_1 \leq d_2$.

All three follow from standard results for concentration of $\chi^2$ random variables; see e.g.~\cite{W19}.
We only prove the first item; the second and third items are straightforward.
To prove the first item, we first reason about the case when one of $r_1  = 0$.

note that $F_1 + 2 F_2 \geq \frac43(F_1 + F_2 + F_3)$, because
\begin{align*}
  \frac{F_1 + 2 F_2}{F_1 + F_2 + F_3}
% &= \frac{r_1 d_2 + r_2 d_1}{d_1 d_2}
= \frac{r_1}{d_1} + \frac{r_2}{d_2}
= \sqrt{\frac{r_1r_2}{d_1d_2}} \left( \sqrt{\frac{r_1 d_2}{d_1 r_2}} + \sqrt{\frac{d_1 r_2}{r_1 d_2}} \right)
\geq \sqrt{\frac49} \cdot 2 = \frac43.
\end{align*}
In particular, $F_2 \geq \frac23(F_2 + F_3)$ and $F_2 \geq \frac16(F_1 + F_2)$. % \MW{Even $1/3$.}.

We first reason about the ratio between $R_2$ and $R_3$ using the first inequality.
With probability $1 - e^{- c F_2} \geq 1 - e^{- \Omega(n d_1 d_2)}$, it holds that $R_2 \geq \frac89 F_2$ and $R_2 + R_3 \leq \frac{10}9 (F _2 + F_3)$. The latter holds because $R_2 + R_3$ is a $\chi^2$ random variable with $F_2 + F_3 \geq F_2$ degrees of freedom. so $R_2 \geq \frac89 \frac23 \frac9{10} (R_2 + R_3) = \frac8{15} (R_2 + R_3)$, or $R_2 \geq \frac87 R_3$.
We next apply the same reasoning with the inequality $F_2 \geq (F_1 + F_2)/6$ to estimate the ratio between $R_1$ and $R_2$.
With probability $1 - e^{- c n d_1 d_2}$, we have $R_2 \geq \frac89 F_2$ and $R_1 + R_2 \leq \frac{10}9 (F_1 + F_2)$.
Thus $R_1 \geq \frac89 \frac16 \frac9{10} (R_1 + R_2) = \frac4{30} (R_1 + R_2)$, or $R_2 \geq \frac2{13} R_1$.
Together, we obtain that $R_2 \geq \frac1{99} R_1 + \frac{101}99 R_3$ with probability~$1 - e^{-\Omega( n d_1 d_2)}$.
\end{proof}

Finally, we show using a net argument that the Cheeger constant is large for \emph{all} projections.

\begin{lemma}[\cite{FM20}, Lemma~5.18]\label{lem:net}
For any $\eps>0$, there is an operator norm $\eps$-net of the rank-$r$ orthogonal projections $\Pi\colon \C^d \to \C^d$ with cardinality $e^{O(d r \abs{\log\eps})}$.
\end{lemma}

As a corollary, the set of pairs of projections $\Pi_1$, $\Pi_2$ of rank $r_1$ and $r_2$, respectively, has an (elementwise) operator norm $\eps$-net of cardinality $e^{O((r_1 d_1 + r_2 d_2) \abs{\log \eps})}$.

\begin{lemma}[Continuity of cut and volume]\label{lem:net-suffices}
Let $\Pi_1,\Pi'_1 \colon \C^{d_1}\to\C^{d_1}$ and $\Pi_2,\Pi'_2\colon\C^{d_2}\to\C^{d_2}$ be orthogonal projections such that $\norm{\Pi_1 - \Pi'_1}_{\op}\leq\eps$ and $\norm{\Pi_2 - \Pi'_2}_{\op}\leq\eps$.
Then:
\begin{align*}
  \abs{\cut(\Pi'_1,\Pi'_2) - \cut(\Pi_1,\Pi_2)}
% \leq 4 \eps \tr \Phi(I_{d_2}) \\
\leq 2 \eps \vol(I_{d_1}, I_{d_2}) \\
  \quad\text{and}\quad
  \abs{\vol(\Pi'_1,\Pi'_2) - \vol(\Pi_1,\Pi_2)}
% \leq 4 \eps \tr \Phi(I_{d_2}).
\leq 2 \eps \vol(I_{d_1}, I_{d_2}).
\end{align*}
\end{lemma}
\begin{proof}
We begin with the first inequality:
\begin{align*}
  \abs{\cut(\Pi'_1,\Pi'_2) - \cut(\Pi_1,\Pi_2)}
&\leq \abs{\tr \Pi'_1 \Phi(I_{d_2} - \Pi'_2) - \tr \Pi_1 \Phi(I_{d_2} - \Pi_2)} \\
&+ \abs{\tr (I_{d_1} - \Pi'_1) \Phi(\Pi'_2) - \tr (I_{d_1} - \Pi_1) \Phi(\Pi_2)}.
\end{align*}
Consider the first term:
\begin{align*}
&\quad \abs{\tr \Pi'_1 \Phi(I_{d_2} - \Pi'_2) - \tr \Pi_1 \Phi(I_{d_2} - \Pi_2)} \\
&= \abs{\tr (\Pi'_1 - \Pi_1) \Phi(I_{d_2} - \Pi'_2) + \tr \Pi_1 \Phi(\Pi_2 - \Pi'_2)} \\
&= \abs{\tr (\Pi'_1 - \Pi_1) \Phi(I_{d_2} - \Pi'_2) + \tr \Phi^*(\Pi_1) (\Pi_2 - \Pi'_2)} \\
&\leq \norm{\Pi'_1 - \Pi_1}_{\op} \norm{\Phi(I_{d_2} - \Pi'_2)}_{\operatorname{tr}} + \norm{\Pi_2 - \Pi'_2}_{\op} \norm{\Phi^*(\Pi_1)}_{\operatorname{tr}} \\
&\leq \eps \tr \Phi(I_{d_2}) + \eps \tr \Phi^*(I_{d_1})
% = 2 \eps \tr \Phi(I_{d_2}).
= \eps \vol(I_{d_1}, I_{d_2}).
\end{align*}
The same inequality for the second term follows by symmetry.
The proof of the second inequality is similar.
\end{proof}

\MW{Cole, I gave the following a facelift (hopefully fixing constants and stuff). Please have a quick peek.}

\begin{prop}[Cheeger constant lower bound]\label{lem:union}
There is a universal constant $C>0$ such that the following holds:
If $d_1 \leq d_2$, $d_2>1$, and $n \geq C \frac{d_2}{d_1} \log (d_2)$, then $\ch(\Phi) = \Omega(1)$ with probability $1 - e^{- \Omega(n d_1)}$.
\end{prop}
\begin{proof}
Let $\eps = \frac c {d_2}$ for some sufficiently small constant~$c>0$.
For $r_1 \leq d_1$ and $r_2 \leq d_2$ not both zero, let~$\cN(r_1,r_2)$ denote an (elementwise) operator norm $\eps$-net for the set of pairs of projections of rank~$r_1$ and~$r_2$, respectively.
As discussed below \cref{lem:net}, we may assume that $\abs{\cN(r_1,r_2)} \leq e^{O((d_1r_1 + d_2r_2) \abs{\log\eps})}$.
Let $\mathcal N = \smash{\bigcup_{r_1,r_2}} \mathcal N(r_1,r_2)$.
We claim that to establish the lemma it suffices to show that with probability $1 - e^{- \Omega(n d_1)}$, the following is simultaneously true for all $r_1,r_2$ and for all $(\Pi_1,\Pi_2) \in \mathcal N(r_1,r_2)$:
\begin{enumerate}
\item If $F_2 := n r_1 r_2 \geq \frac49 n d_1 d_2$, then \cref{eq:vol} holds with $\delta=0$.
\item If $F_2 < \frac49 n d_1 d_2$, then \cref{eq:cut} holds with $\delta=0$.
\item $\vol(\Pi_1,\Pi_2) \geq \frac1{4d_2} \vol(I_{d_1}, I_{d_2})$.
\end{enumerate}
To see that this suffices, we only need to show that it implies the hypotheses of \cref{lem:suff} for~$\delta = 32 c$.
Let $(\Pi_1',\Pi_2')$ be an arbitrary pair of projections, not both zero.
Let $r_1$ and $r_2$ denote their ranks.
Then there exists a pair $(\Pi_1,\Pi_2) \in \cN(r_1,r_2)$ such that $\norm{\Pi'_1 - \Pi_1}_{\op} \leq \eps$ and~$\norm{\Pi'_2 - \Pi_2}_{\op} \leq \eps$.
If $F_2 \geq \frac49 n d_1 d_2$,
\begin{align*}
  \vol(\Pi'_1,\Pi'_2)
&\geq \vol(\Pi_1,\Pi_2) - 2 \eps \vol(I_{d_1}, I_{d_2}) \\
&\geq \left( \frac{101}{200} - 2 \eps \right) \vol(I_{d_1}, I_{d_2})
\geq \left( \frac{101}{200} - 2 c \right) \vol(I_{d_1}, I_{d_2}),
\end{align*}
where the first inequality is \cref{lem:net-suffices}, the second uses the first item above, and finally we estimate $\eps \leq c$.
Thus we have verified that \cref{eq:vol} holds for $(\Pi'_1,\Pi'_2)$, that is, the first hypothesis of \cref{lem:suff}.
If $F_2 < \frac49 n d_1 d_2$, then
\begin{align*}
  \vol(\Pi'_1,\Pi'_2)
&\leq \vol(\Pi_1,\Pi_2) + 2 \eps \vol(I_{d_1}, I_{d_2})
\leq (1 + 8 \eps d_2) \vol(\Pi_1,\Pi_2) \\
&\leq (1 + 8 \eps d_2) \frac43 \left( F_1 + 2 F_2 \right)
= \left( \frac43 + \frac{32}3 c \right) \left( F_1 + 2 F_2 \right),
\end{align*}
where $F_1 := n r_1(d_2 - r_2) + n r_2(d_1-r_1)$, the first inequality is \cref{lem:net-suffices}, the second inequality uses the third item above, and the third inequality uses the second item above.
On the other hand,
\begin{align*}
  \cut(\Pi'_1,\Pi'_2)
&\geq \cut(\Pi'_1,\Pi'_2) - 2 \eps \vol(I_{d_1}, I_{d_2})
\geq \cut(\Pi'_1,\Pi'_2) - 8 \eps d_2 \vol(\Pi_1,\Pi_2) \\
&\geq \frac23 F_1 - 8 \eps d_2 \frac43 (F_1 + 2 F_2)
= \frac23 F_1 - 8 c \frac43 (F_1 + 2 F_2) \\
&\geq \frac23 F_1 - 32 c F_1
= \left( \frac23 - 32 c \right) F_1,
\end{align*}
again using \cref{lem:net-suffices}, the second and third item above.
In the last step we used the fact that $F_1 \geq \frac{1}{3} (F_1 + 2 F_2)$ provided $F_2 < \frac49 n d_1 d_2$, which we established in the proof of \cref{lem:suff}.
Thus we have verified that \cref{eq:cut} holds for $(\Pi'_1,\Pi'_2)$, which is the remaining hypotheses of \cref{lem:suff}.

To prove the lemma we still need to show that the three conditions above hold with the desired probability.
We show that for fixed $r_1$ and $r_2$, each condition holds with probability at least $1 - e^{\Omega(n (r_1 d_2 + d_1 r_2))}$.
% \MW{where the $\Omega$ hides a universal constant}
By the union bound, this implies that the conditions hold simultaneously for all $r_1 \leq d_1$ and $r_2 \leq d_2$, not both zero, with the desired probability, because the sum of $e^{-\Omega(n (r_1 d_2 + d_1 r_2))}$ over all such~$r_1$ and~$r_2$ is $e^{- \Omega( n d_1)}$, using that $d_1 \leq d_2$.
Thus fix~$r_1$ and~$r_2$ as above.
We first bound the probability for the first item.
By \cref{lem:probabilities} and the union bound, if $F_2 \geq \frac49 n d_1 d_2$ then \cref{eq:vol} holds for every $(\Pi_1,\Pi_2) \in \cN(r_1, r_2)$ with probability
\begin{align*}
1 - \abs{\cN(r_1, r_2)} e^{- \Omega( n d_1 d_2) } &\leq 1 - e^{(d_1r_1 + d_2r_2) \abs{\log\eps}} e^{ - \Omega(n (r_1 d_2 + d_1 r_2))}
\leq 1 - e^{ - \Omega(n (r_1 d_2 + d_1 r_2))}.
\end{align*}
The last step follows by our assumption on~$n$ (for a large enough universal constant $C>0$), since
\begin{align*}
  (d_1r_1 + d_2r_2) \abs{\log\eps}
\leq \frac{d_2}{d_1}(r_1 d_2 + d_1 r_2) \left( \log(d_2) + \abs{\log c} \right)
= O\left(\frac{d_2}{d_1} \log(d_2) \right) \cdot (r_1 d_2 + d_1 r_2).
\end{align*}
Next we bound the probability for the second item.
By \cref{lem:probabilities} and the union bound, if $F_2 < \frac49 n d_1 d_2$, \cref{eq:cut} holds for every $(\Pi_1,\Pi_2) \in \cN(r_1, r_2)$ with probability
\begin{align*}
  1 - \abs{\cN(r_1, r_2)} e^{-\Omega(F_1)}
\leq 1 - \abs{\cN(r_1, r_2)} e^{-\Omega(n (r_1 d_2 + r_2 d_1))}
\leq 1 - e^{ - \Omega(n (r_1 d_2 + d_1 r_2))},
\end{align*}
where the first step holds since $F_1 \geq \frac{1}{3} (F_1 + 2 F_2) = \frac13 n (r_1 d_2 + r_2 d_1)$ whenever $F_2 < \frac49 n d_1 d_2$, as already used earlier in the proof, and the second step follows as above by our assumption on~$n$ (for large enough $C>0$).
The probability for the third item can be bounded completely analogously.
% By using \cref{lem:probabilities}, the union bound, and our choice of~$n$, it follows that it holds
\end{proof}

\begin{proof}[Proof of \cref{thm:operator-cheeger}]
Let $\Phi:=\Phi_X$.
Since~$n \geq C \frac{d_2}{d_1} \log d_2$, \cref{lem:union} shows that $\ch(\Phi) = \Omega(1)$ with failure probability $e^{- \Omega(n d_1)}$.
The latter is $e^{- \Omega(d_2 t^2)}$ using our assumption that $n \geq C \frac{d_2}{d_1} t^2$.

Now let $\eps := \smash{t \sqrt{\frac{d_2}{n d_1}}}$, which by the same assumption satisfies $\eps \leq \smash{\frac1{\sqrt C}}$.
Moreover, $n \geq \frac {\dmax^2} {D \eps^2}$, since this is equivalent to our assumption that $t\geq1$.
% \begin{align*}
% n \geq \frac {\dmax^2} {D \eps^2}
% = \frac {d_2^2} {d_1 d_2 t^2 \frac{d_2}{n d_1}}
% = \frac n {t^2}
% \end{align*}
% We need $\eps^2 \leq \frac1C \leq 1$ if we choose $C \geq 1$.
% We also need $n >= \frac {\dmax^2} {D \eps^2} = \frac n {t^2}$, i.e., $t \geq 1$.
Therefore, if we choose~$C$ sufficiently large then, similarly to the proof of \cref{thm:tensor-convexity}, we find using \cref{prop:gradient-bound,prp:xnorm} that $\Phi$ is $\eps$-doubly balanced with failure probability $e^{-\Omega(nD)} + O(e^{-\Omega(n d_1 \eps^2)}) \leq O(e^{-\Omega(d_2 t^2)})$.

By making~$C$ larger, we can ensure that~$\eps$ is less than any absolute constant.
Then \cref{lem:op-cheeger} applies (with balancedness~$\eps$) and shows that $\Phi$ is an $(\eps,\lambda)$-quantum expander for some absolute constant~$\lambda<1$.
\end{proof}

%=============================================================================
\section{Proof of concentration for matrix normal model}\label{app:flipflop-concentration}
%=============================================================================
In this section we prove \cref{lem:flipflop-concentration}, which shows concentration after one step of the flip-flop algorithm.
For convenience, we consider the differently normalized random variable $Z = Y/\sqrt{nd_1}$.
Note that these satisfy
$Z_i = X_i \Phi_X^*(I_{d_1})^{-1/2} =  X_i (\sum_{i = 1}^n X_i^T X_i)^{-1/2}.$
Thus we wish to prove that
\begin{align}\label{eq:1marg goal}
  % \norm*{\Phi_Z(I_{d_2}) - \frac{d_2}{d_1} I_{d_1}}_{\op} =
  \norm*{\sum_{i=1}^n Z_i Z_i^T - \frac{d_2}{d_1} I_{d_1}}_{\op}
\leq t \sqrt{\frac{d_2}{nd_1}}.
\end{align}
Since we are interested in the spectral norm, we will consider the random variable $\langle \xi, \sum_{i = 1}^n Z_i Z_i^{T} \xi \rangle$ for a fixed unit vector $\xi \in \R^{d_1}$.
We will show that this variable is highly concentrated, and apply a union bound over a net of the unit vectors.
To show the concentration, we first cast $\langle \xi, \sum_{i = 1}^n Z_i Z_i^{T} \xi \rangle$ as the inner product between a random orthogonal projection and a fixed one.
Since each $Z_i$ is a $d_1 \times d_2$ matrix, we can consider $Z$ as an $n d_1 \times d_2$ matrix by vertically concatenating the $Z_i$.
By definition of the flip-flop step, $Z^T Z = \sum_{i=1}^n Z_i^T Z_i = I_{d_2}$, so $Z Z^T$ is an orthogonal projection onto a $d_2$-dimensional subspaces of~$\R^{n d_1}$.
In fact, $Z Z^T$ is a uniformly random such projection.
This is because $X$, considered as a $n d_1 \times d_2$ random matrix with i.i.d.\ Gaussian entries, is invariant under left multiplication $X \mapsto OX$ by orthogonal transformations~$O \in O(n d_1)$, hence the same is true for $Z = X (X^T X)^{-1/2}$.
We can now write
\begin{align*}
  \langle \xi,  \sum_{i = 1}^n Z_i Z_i^{T} \xi \rangle = \langle Z Z^T,  \xi \xi^T \ot I_{n} \rangle.
\end{align*}
The matrix $\xi \xi^T \otimes I_{n}$ is a fixed rank $n$ projection on $\R^{n d_1}$.
We now use the following result on the inner product of random projections.

\begin{theorem} [Lemma III.5 in \cite{hayden2006aspects}]
Let $P$ be a uniformly (Haar) random orthogonal projection of rank $a$ on $\R^{m}$, let $Q$ be a fixed orthogonal projection of rank $b$ on $\R^{m}$, and let $\eps>0$.
Then,
\begin{align*}
\Pr \left[ \langle P, Q \rangle \not\in (1 \pm \eps) \frac{ab}{m} \right] \leq 2 e^{ - \Omega( ab \eps^2 ) }.
\end{align*}
\end{theorem}

We apply this result with $P = ZZ^T$, $Q = \xi \xi^T \otimes I_n$, $a = d_2$, $b = n$, and~$m = n d_1$ to obtain
\begin{align}\label{eq:fixed-concentration}
  \Pr\left[ \abs*{\langle \xi, \left( \sum_{i = 1}^n Z_i Z_i^{T} - \frac{d_2}{d_1} I_{d_1} \right) \xi \rangle} > \frac{d_2}{d_1} \eps \right]
\leq 2 e^{ - \Omega( n d_2 \eps^{2} ) }
\end{align}
for any fixed unit vector $\xi\in\R^{d_1}$.

Next we apply a standard net argument for the unit vectors over $\R^{nd_1}$.
We use the following lemma.

\begin{lemma}[Lemma~5.4 \cite{vershynin2010introduction}]\label{lem:versh-net}
Let $A$ be a symmetric $d\times d$ matrix, and let $\mathcal{N}$ be an $\delta$-net of the unit sphere of $\R^d$ for some $\delta \in [0,1)$.
Then,
$$\|A\|_{\op} \leq (1 - 2 \delta)^{-1} \sup_{\xi \in \mathcal{N}} | \langle \xi, A \xi \rangle|.$$
\end{lemma}
We apply the above lemma with $A = \sum_{i = 1}^n Z_i Z_i^{T} - \smash{\frac{d_2}{d_1}} I_{d_1}$, $d = d_1$, and a net~$\mathcal N$ for~$\delta = 1/4$.
By standard packing bounds (e.g., Lemma~4.2 in \cite{vershynin2010introduction}) we may take $\abs{\mathcal{N}} \leq 9^{d_1}$.
By \cref{eq:fixed-concentration} and the union bound, with failure probability $2 \cdot 9^{d_1} e^{- \Omega (n d_2 \eps^2)}$ we have that $|\langle \xi , A \xi \rangle| \leq \frac{d_2}{d_1} \eps$ for all $\xi \in \mathcal{N}$, and by \cref{lem:versh-net} this event implies $\norm{A}_{\op} \leq 2  \frac{d_2}{d_1} \eps$.
Setting
\[ \eps = t \sqrt{\frac{d_1}{4n d_2}}, \]
we obtain \cref{eq:1marg goal}, i.e.,
\begin{align*}
  \norm*{\sum_{i = 1}^n Z_i Z_i^{T} - \frac{d_2}{d_1} I_{d_1}}_{\op}
\leq 2 \frac{d_2}{d_1} t \sqrt{\frac{d_1}{4n d_2}}
= t \sqrt{\frac{d_2}{n d_1}},
\end{align*}
with failure probability at most $2 \cdot 9^{d_1} e^{- \Omega(d_1 t^2)}$, which is at most $e^{ - \Omega(d_1 t^2)}$, provided $t$ is bounded from below by a large enough constant~$c>0$.
This concludes the proof of \cref{lem:flipflop-concentration}. \qed

%=============================================================================
\section{Relative error metrics}\label{sec:rel-error}
%=============================================================================
In this section we discuss the properties of our relative error metrics $\DF$, $\Dop$ (\cref{eq:def D_F,eq:def D_op}).
First note that they can be related to the usual norms by following inequalities:
\begin{align}
\label{eq:D_F rel vs abs}
\|B^{-1}\|_{\op}^{-1} \, \DF(A\Vert B)&\leq \norm{A - B}_F \leq \norm{B}_{\op} \, \DF(A\Vert B)\\
\label{eq:D_op rel vs abs}
\|B^{-1}\|_{\op}^{-1} \, \Dop(A\Vert B) &\leq \norm{A  - B}_{\op} \leq \norm{B}_{\op} \, \Dop(A \Vert B).
\end{align}

Next we state the approximate triangle inequality, also called a \emph{local} triangle inequality in \cite{yang1999information}, and approximate symmetry for our relative error metrics.

\begin{lemma}\label{lem:triangle-ineq}
Let $A, B, C \in \PD(d)$.
Let $D \in \{\Dop, \DF\}$. Provided $D(A\Vert B), D(B\Vert C)$ are at most an absolute constant $c>0$, we have
\begin{align}
  D(A\Vert C) &= O\bigl( D(A\Vert B) + D(B\Vert C) \bigr),\label{eq:tri}\\
  D(B\Vert A) &= O\bigl( D(A\Vert B) \bigr),\label{eq:sym} \text{ and }\\
  D(A^{-1}\Vert B^{-1}) &= O\bigl( D(A\Vert B) \bigr).
\end{align}
\end{lemma}

For $\Dop$, the result is \cite[Lemma~C.1]{FM20}.
For $\DF$, the result holds because because $\DF(A\Vert  B) \asymp d(A, B)$ if either is at most some absolute constant (as shown below), where $d(A, B)$ denotes the Fisher-Rao distance from \cref{eq:fisher rao}.
Because $d(A, B)$ is a metric, it automatically satisfies \cref{eq:tri,eq:sym}. Furthermore, $d(A,B) = d(A^{-1}, B^{-1})$ by direct calculation.
We next consider the relationship between $\DF$ and other dissimilarity measures in the statistics literature.

\begin{prop}[Relationships between dissimilarity measures]\label{prop:dissimilarities}
There is a constant~$c > 0$ such that the following holds.
If any of
$\DF(\Theta_1\Vert  \Theta_2)$,
$\DTV(\mathcal{N}(0, \Theta_1^{-1}), \mathcal{N}(0, \Theta_2^{-1}))$,
$\DKL(\mathcal{N}(0, \Theta_1^{-1}) \Vert  \mathcal{N}(0, \Theta_2^{-1}))$,
or the Fisher-Rao distance $\dFR(\Theta_1, \Theta_2)$
is at most~$c$, then
\begin{align*}
  \DF(\Theta_1\Vert  \Theta_2)^2
&\asymp \dFR(\Theta_1, \Theta_2)^2
\asymp \DTV\bigl(\mathcal{N}(0, \Theta_1^{-1}), \mathcal{N}(0, \Theta_2^{-1})\bigr)^2 \\
&\asymp \DKL\bigl(\mathcal{N}(0, \Theta_1^{-1}) \Vert  \mathcal{N}(0, \Theta_2^{-1})\bigr).
\end{align*}
\end{prop}
\begin{proof}
For the relationship between $\DF(\Theta_1\Vert  \Theta_2)^2$ and $\dFR(\Theta_1, \Theta_2)^2$, observe that the former is $\sum_{i = 1}^d (\lambda_i - 1)^2$ and the latter is $\sum_{i = 1}^d (\log \lambda_i)^2$ for the eigenvalues $\lambda_i$ of $\Theta_2^{-1} \Theta_1$. Because $\lambda-1 \asymp \log \lambda$ in any fixed interval not containing~$0$, there is some absolute constant $c$ such that if $\DF(\Theta_1 \Vert  \Theta_2) \leq c$ or $d(\Theta_1, \Theta_2) \leq c$ then $\DF(\Theta_1 \Vert  \Theta_2) \asymp d(\Theta_1, \Theta_2)$.

To relate $\DF$ to the total variation distance, we use the following bound from \cite{devroye2018total}:
\begin{align*}
  .01  \leq \frac{\DTV\bigl(\mathcal{N}(0, \Theta_1^{-1}), \mathcal{N}(0, \Theta_2^{-1})\bigr)}{\DF(\Theta_1 \Vert  \Theta_2)} \leq 1.5.
\end{align*}
This implies that if either the numerator or denominator is a small enough constant, then they are on the same order.

Finally we relate $\DF$ to the relative entropy.
We claim that if $\DKL(\mathcal{N}(0, \Theta_1^{-1}), \mathcal{N}(0, \Theta_2^{-1})) \leq c$ for some small enough constant~$c$ or if $\DF(\Theta_2 \Vert  \Theta_1) \leq 1/2$, then we have
\begin{align*}
  \DKL\bigl(\mathcal{N}(0, \Theta_1^{-1}), \mathcal{N}(0, \Theta_2^{-1})\bigr) \asymp \DF(\Theta_2 \Vert  \Theta_1)^2.
\end{align*}
This bound can be seen explicitly from the formula
\begin{align*}
  \DKL\bigl(\mathcal{N}(0, \Theta_1^{-1}), \mathcal{N}(0, \Theta_2^{-1})\bigr)
&= \frac{1}{2} \tr \Theta_1^{-1} \Theta_2 - \frac{1}{2}\log\det(\Theta_1^{-1} \Theta_2) - \frac{d}{2}\\
& = \frac{1}{2} \sum_{i =1}^d (\lambda_i -1 -  \log \lambda_i),
\end{align*}
where $\lambda_i \in 1 \pm \Dop(\Theta_1\Vert \Theta_2)$ are the eigenvalues of $\Theta_1^{-1} \Theta_2$,
and the fact that $\lambda - 1 -\log\lambda \asymp \frac12 (\lambda - 1)^2$ on $[1/2, 3/2]$.
To complete the argument, choose $c$ small enough that $\frac{1}{2}(\lambda - 1 -\log\lambda) \leq c$ implies $\lambda \in [1/2, 3/2]$.
\end{proof}
\end{appendix}


%=============================================================================
\section*{Acknowledgements}
%=============================================================================
This work was supported in part by NWO Veni grant no.~680-47-459 and NWO grant OCENW.KLEIN.267.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Supplementary Material, if any, should   %%
%% be provided in {supplement} environment  %%
%% with title and short description.        %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\begin{supplement}
%\stitle{???}
%\sdescription{???.}
%\end{supplement}

\bibliographystyle{imsart-nameyear}
\bibliography{refs}

\end{document}
