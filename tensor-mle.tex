\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{amsthm,mathtools,braket}
\usepackage{amssymb,bm, hyperref}
\usepackage{xcolor, float}
\usepackage[capitalize]{cleveref}

\newtheorem{theorem}{Theorem}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{prop}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corol}[theorem]{Corollary}
\newtheorem{fact}[theorem]{Fact}
\newtheorem{claim}[theorem]{Claim}
\newtheorem{remark}{Remark}
\newtheorem{definition}{Definition}
\newtheorem{example}{Example}
\floatstyle{boxed}\newfloat{Algorithm}{ht}{alg}

\DeclareMathOperator{\Lie}{Lie}
\DeclareMathOperator{\Lin}{L}
\DeclareMathOperator{\ope}{op}
\DeclareMathOperator{\poly}{poly}

\DeclarePairedDelimiter{\abs}{\lvert}{\rvert}
\DeclarePairedDelimiter{\norm}{\lVert}{\rVert}
\DeclarePairedDelimiter{\ip}{\langle}{\rangle}


\newcommand{\junk}[1]{}


\newcommand{\R}{{\mathbb{R}}}
\newcommand{\C}{{\mathbb{C}}}
\newcommand{\ot}{\otimes}
\newcommand{\mat}{\operatorname{Mat}}
\newcommand{\op}{\operatorname{op}}
\newcommand{\ch}{\operatorname{ch}}
\newcommand{\cut}{\operatorname{cut}}
\newcommand{\vol}{\operatorname{vol}}
\newcommand{\Q}{{\mathbb{Q}}}
\newcommand{\N}{{\mathbb{N}}}
\renewcommand{\vec}{\bm}
\newcommand{\Z}{{\mathbb{Z}}}
\renewcommand{\S}{\mathbb{S}}

\newcommand{\E}{\mathbb{E}}
\renewcommand{\Pr}{\mathbb{P}}

\newcommand\eps{\varepsilon}

\newcommand\cN{\mathcal{N}}
\newcommand\FF{\mathcal{F}}
\newcommand\HH{\mathcal{H}}
\newcommand\GG{\mathcal{G}}
\newcommand\SL{\operatorname{SL}}
\newcommand\PD{\operatorname{PD}}
\newcommand\Herm{\operatorname{Herm}}
\newcommand\Sym{\operatorname{Sym}}
\newcommand\GL{\operatorname{GL}}
\newcommand\TT{\mathcal{T}}
\newcommand\PP{\mathcal{P}}
\newcommand\tr{\operatorname{Tr}}
\newcommand\rk{\operatorname{rk}}
\newcommand\RR{\mathcal{R}}
\newcommand\CC{\mathcal{C}}
\newcommand\BB{\mathcal{B}}
\newcommand\II{\mathcal{I}}
\renewcommand\AA{\mathcal{A}}
\newcommand{\MM}{\mathcal{M}}
\newcommand\AP{\mathcal{AP}}

\newcommand{\eqdef}{:=}
\newcommand{\maps}{\colon}
\newcommand{\ale}{\lesssim}
\newcommand{\age}{\gtrsim}
\newcommand{\CF}[1]{{\color{purple}[CF: #1]}}
\newcommand{\AR}[1]{{\color{orange}[AR: #1]}}
\newcommand{\RMO}[1]{{\color{red}[RMO: #1]}}
\newcommand{\MW}[1]{{\color{red}[MW: #1]}}
\newcommand{\TODO}[1]{{\color{blue}[TODO: #1]}}

\crefname{Algorithm}{Algorithm}{Algorithms}


\title{Logarithmic sample complexity for dense matrix normal models and bla bla}
\author{Cole Franks, Rafael Oliveira, Akshay Ramachandran, Michael Walter}
\date{February 2020}

\begin{document}

\maketitle
\tableofcontents
\TODO{
\begin{enumerate}
\item Use $n$ instead of $T$ for number of samples.
\item Replace the failure probability $1/poly(D)$ by an actual expression.
\item Fix the dimension issue in \cref{thm:tensor-convexity}.
\item Small annoying thing - we assume determinant 1. This can be handled easily, but needs to be done.
\item When not random, use lower case $x$.
\item Comment that the geodesic distance is some variation on the Fischer-Rao distance.
\end{enumerate}

}


\section{Introduction}
\CF{background, history; remark that we automatically know flip-flop works but our strong convexity will give efficient algorithms in this setting, as opposed to just converging algorithms. }
The covariance matrix $\Sigma$ of a Gaussian random variable $X$ on $\R^D$ is a fundamental object in statistics, and its estimation is a central task in data analysis. If the number of samples is comparable to the dimension, the sample covariance matrix approximates $\Sigma$ well in spectral norm. However, one can still often obtain useful estimates of $\Sigma$ with far fewer samples under the assumption that $\Sigma$ is structured. Performing this task under various structural assumptions such as sparsity, rank constraints, bandedness, and many more is a well-studied problem\CF{cite}. Here we consider the family of distributions, known as the \emph{tensor normal model}, with covariance matrices that are Kronecker products of some matrices of known dimensions $d_1, \dots, d_k$ with $d_1d_2\dots d_k = D$.

Perhaps the best-known case is the case $k = 2$, called the \emph{matrix normal model} or  \emph{matrix normal variate model} \CF{cite drton, etc}. This model applies when each sample can be viewed as a matrix and the covariance between entries can be modeled as a product of a inter-row factor and a inter-column factor, such as data indexed by both space and time in magnetoencephalography \CF{cite}. \CF{figure out some uses for the tensor normal models}.

Much research has been devoted to estimating the covariance in matrix and tensor normal models. Authors have proven that the MLE exists uniquely under mild assumptions \CF{cite}, and there is an iterative algorithm (known as the \emph{flip-flop} algorithm) that converges quickly to the MLE in practice \CF{dutilleul}. Through recent connections noted in \CF{cite philipp et al} it follows that the flip-flop algorithm converges whenever the MLE exists. Estimators with good performance have been shown to exist under the assumption that the true covariance matrix is highly sparse.

Nonetheless, to our knowledge there exist no nonasymptotic bounds on convergence of the MLE, or any other tractable estimator, in the dense case \CF{actually, tsiligkaridis do this in frobenius norm for matrix models but they are not tight for every value of $d_1, d_2$.}. Moreover, it has not been proven that the flip-flop algorithm converges \emph{quickly} to the MLE, or even that there is a polynomial time to compute the MLE or any other \CF{asymptotically consistent?} estimator. We make significant progress on both of these open questions. As an informal summary, we show the following:
\begin{enumerate}
\item The MLE for the matrix normal model is close to the covariance in spectral norm for a number of samples that is \CF{check}
$$O(\max \{d_1/d_2, d_2/d_1\} \log \max \{d_1/d_2, d_2/d_1\}),$$ which is tight up to logarithmic factors.
\item The MLE for the tensor normal model converges is close to the covariance in Frobenius norm for a number of samples that is $O(\max_{i \in [k]} D/d_i^3)$. This result is tight.
\item Under the same sample requirements as above, the flip-flop algorithm converges exponentially quickly to the MLE with high probability. As a corollary, there is an algorithm to compute the  MLE with polynomial expected runtime.
\end{enumerate}
It remains an open question to show tight bounds for convergence of the MLE in spectral norm for the tensor normal model.
\subsection{Notation}
\CF{temporary}
\begin{itemize}
\item Number of samples $n$, dimensions $d_1, \dots, d_k$. $D$ for product of these.
\item $X$ for the tensor random variable, $X_1, \dots, X_n$ for the samples, $\vec X = (X_1, \dots, X_n)$ for the random tuple of samples.
$\rho = \vec X \vec X^T/\|\vec X\|_F^2$. Lower case $x$ for samples.
\item $\braket{\cdot,\cdot}_{\vec d}$ denote modified Hilbert-Schmidt inner products
\MW{sadly the corresponding norms look like $\ell_p$ norms},
$\langle \cdot, \cdot \rangle$ is the $\ell_2$ inner product of vectors
\item $f_{\vec X}$ for the function in \cref{dfn:function}, mostly drop $\vec X$.
\item $\Sym_d$ for $d \times d$ real symmetric (meh), $\PD_d$ for $d \times d$ real positive definite, $\Sym_d^0$ for traceless symmetric, $\PD_d^1$ for $\det=1$ positive definite? $\mathcal{S}?$ $\mathfrak{S}?$. 
\item $\Theta$ for big tensor product pd concentration matrix, $\Theta_a$ for individual pd's.
\item I'm going to call $\SL = \oplus \SL_{d_i}, \PD = \oplus \PD_{d_i}^1, \Sym = \oplus \Sym_{d_i}^0$. Explain somewhere how $\Sym$ is the tangent space of $\PD$. \MW{Suppressing the $^1$ and $^0$ is a bit confusing I think. Maybe $\operatorname{SPD}$ for $\PD$ with $\det=1$? I still feel that $\Sym$ looks somewhat horrible (with or without subscript, but I am not sure what would be better).}
%\item $G = \prod_{a=1}^k \SL_{d_a}$, $P = \prod_{a=1}^k \PD_{d_a}^1$, and $S = \bigoplus_{a=1}^k \Sym_{d_a}^0$.

\item $\nabla, \nabla^2$ for Riemannian Hessians and gradients, $\nabla_a f$, $\nabla^2_{ab} f$ for components. $\nabla f$ means at the identity. $\|\nabla^2_{ab}f\|_{op}:=\|\nabla^2_{ab}f\|_{F\to F}$.
\item $C, c$ large (resp. small) constants that change line to line.
\item $\rho^{(a)}$, $\rho^{(ab)}$ for marginals.
\item $I$ for an identity matrix, $I_a$ for the $d_a \times d_a$ identity matrix
\item $\norm{\cdot}_p$ for the $\ell_p$ norm of vectors and the Schatten-$p$ norm of operators
\end{itemize}

\subsection{Results}

\CF{Enough background material to state \cref{thm:matrix-normal,thm:tensor-frobenius,thm:matrix-flipflop,thm:tensor-flipflop}; maybe make some of these into 2-parters, explanation of tightness of results.}

We consider the tensor normal model, in which samples are drawn according to a normal distribution $\cN(0, \Sigma)$, where the covariance matrix $\Sigma = \bigotimes_{a = 1}^{k} \Sigma_a$ is the Kronecker product of positive definite $d_a\times d_a$ matrices $\Sigma_a$ for $a\in [k]$.
Let $\Theta$ denote the precision matrix $\Sigma^{-1}$, i.e., $\Theta = \bigotimes_{a=1}^k \Theta_a$ where $\Theta_a = \Sigma_a^{-1}$.

We use the following notions of distance. The first arises naturally because it is within a constant factor of the total variation distance (assuming both are small enough constants), and the second because bounds on it typically strengthen bounds on the first.

\begin{definition}[Distance]
Define
% \begin{align}d_{F}(\Sigma_1; \Sigma_2) = \| I - \Sigma_2^{1/2} \Sigma_1^{-1} \Sigma_2^{1/2}\|_F\\
% d_{op}(\Sigma_1; \Sigma_2) = \| I - \Sigma_2^{1/2} \Sigma_1^{-1} \Sigma_2^{1/2}\|_{op}
% \end{align}
% and
\begin{align}d_{F}(\Theta_1; \Theta_2) = \| I - \Theta_2^{-1/2} \Theta_1 \Theta_2^{-1/2}\|_F\\
d_{op}(\Theta_1; \Theta_2) = \| I - \Theta_2^{-1/2} \Theta_1 \Theta_2^{-1/2}\|_{op}
\end{align}
\end{definition}

We may now state precisely our result for the tensor normal models.

\begin{theorem}[Tensor normal Frobenius error]\label{thm:tensor-frobenius} Suppose $\eps \leq c$ and the number of samples satisfies $n \geq C k \max d_{a}^3/D\eps^2$. The MLE $(\widehat{\Theta}_1, \dots, \widehat{\Theta}_k) $ for $(\Theta_1, \dots \Theta_k)$ from $n$ independent samples of the tensor normal model satisfies
$$ d_{F}(\widehat{\Theta}_a, \Theta_a) \leq \eps $$
for $a \in [k]$ with probability $1 - \CF{1/poly(D)}$.
\end{theorem}

\CF{explain tightness; the distance bounds are tight but when they kick in is not necessarily tight. Truly tight bounds would follow from operator norm analysis.} In the case of matrix normal models $(k=2)$, we obtain the following stronger result.

\begin{theorem}[Matrix normal model spectral error]\label{thm:matrix-normal} Suppose $d_1 \leq d_2$,
$$n \geq C (d_2/d_1) \max\{\log (d_2/d_1),  \log^2(d_1) \eps^{-2}\}$$ for $C$ a large enough constant and $\eps < c$ for a small enough constant $c$. The MLE $(\widehat{\Theta}_1,\widehat{\Theta}_2) $ for $(\Theta_1, \Theta_2)$ from $n$ independent samples of the matrix normal model satisfies
$$ d_{op}(\widehat{\Theta}_a, \Theta_a) = O\left(\eps \sqrt{\frac{d_2}{n d_1}} \log d_1\right) $$
for $a \in \{1,2\}$ with probability $1 - O(e^{ - \Omega( d_2 \eps^2/\log^2 d_1)})$.
\end{theorem}

Our next results show that in either case the flip-flop algorithm can find the MLEs with high probability. \TODO{change flip flops to thetas}

\begin{theorem}[Tensor flip-flop]\label{thm:tensor-flipflop} If $\hat{\Sigma}$ denotes the MLE estimator for $\Sigma$, then provided $n \geq \TODO{}$ the flip-flop algorithm computes $\underline{\Sigma}$ with
$$ d_F(\underline{\Sigma}, \hat{\Sigma}) \leq \eps $$
in $O(\TODO{\log(1/\eps)})$ iterations with probability at least $\TODO{}$.
\end{theorem}

\begin{theorem}[Matrix flip-flop]\label{thm:matrix-flipflop} If $(\hat{\Theta}_1, \hat{\Theta}_2) $ denotes the MLE estimator for $(\Theta_1, \Theta_2)$, then provided $n \geq \TODO{}$ the flip-flop algorithm computes $(\underline{\Theta}_1, \underline{\Theta}_2)$ with
$$ d_F(\underline{\Theta}_i, \hat{\Theta}_i) \leq \eps $$
for $i \in \{1,2\}$ in $O(\TODO{\log(1/\eps)})$ iterations with probability at least $\TODO{}$.
\end{theorem}

One may wonder why in the above theorems we consider the distances for the individual tensor factors and not the covariance matrix itself, but tight bounds for the covariance matrix itself follow from the above bounds (apart from the logarithmic factor in the matrix normal case and a constant factor in general).

%=============================================================================
\section{The tensor normal model}
%=============================================================================
\TODO{In this section, \dots}

%-----------------------------------------------------------------------------
\subsection{Maximum-likelihood estimation}
%-----------------------------------------------------------------------------
We start by giving a precise definition of maximum-likelihood estimation in the tensor normal model.
Recall that for a centered normal distribution~$\cN(0,\Sigma)$ with $D\times D$ covariance matrix~$\Sigma$, the log-likelihood function for given samples $X_1,\dots,X_n\in\R^D$ is (up to an additive constant)
\begin{align*}
  \ell_{\vec X}(\Theta)
% = \frac n2 \log \det(\Theta/2\pi) - \sum_{i=1}^n \frac12 \braket{X_i, \Theta X_i} ~
  = \frac n2 \log \det \Theta - \frac12 \sum_{i=1}^n \braket{X_i, \Theta X_i},
\end{align*}
where $\Theta$ denotes the precision matrix $\Sigma^{-1}$.
Instead of maximizing the function~$\ell_{\vec X}$, we may instead maximize the scale-invariant function
\begin{align}\label{eq:tilde ell}
  \tilde\ell_{\vec X}(\Theta) = \frac1D\log\det \Theta - \log \sum_{i=1}^n \braket{X_i, \Theta X_i}.
\end{align}
Indeed, the two functions have the same maximizers up to rescaling.%
\footnote{To see this, write $\Theta = \lambda \Theta'$ for $\lambda>0$ and $\Theta'\in\PD_D^1$ and maximize over~$\lambda$.
The result is $\lambda = n D / \sum_{i=1}^n \braket{X_i, \Theta X_i}$, hence $\sup_{\Theta\in\PD_D} \ell_{\vec X}(\Theta) = \sup_{\Theta'\in\PD_D^1} \frac {nD}2 \left( \log(nD) - 1 \right) + \frac{nD}2 \tilde\ell_{\vec X}(\Theta')$.}
One may think of \cref{eq:tilde ell} as proportional, up to an additive constant, to the log-likelihood of $\Sigma$ after receiving $[X_1, \dots, X_n]$, the equivalence class of the tuple of samples in projective space.

Both log-likelihoods are geodesically concave, as will be defined precisely in the next section, but only the latter function has geodesically Lipschitz gradients, i.e., is geodesically smooth~\cite{burgisser2019towards}.
This suggests it is more amenable to geodesically convex optimization, our main tool in this work.

The tensor normal model for dimensions~$\vec d=(d_1,\dots,d_k)$ consists of the normal distributions~$\cN(0,\Sigma)$, where $D = \prod_{a=1}^k d_a$ and the covariance matrix is a Kronecker product $\Sigma = \bigotimes_{a=1}^k \Sigma_a$ of positive definite $d_a\times d_a$-matrices $\Sigma_a$.
Let $\Theta_a = \Sigma_a^{-1}$, so that the precision matrix is given by $\Theta = \bigotimes_{a=1}^k \Theta_a$.
Then the same argument as above, restricted to precision matrices of this form, shows that to compute the MLE we may maximize the function
\begin{align*}
  \tilde\ell_{\vec X}(\Theta_1,\dots,\Theta_k) = \sum_{i=1}^k \frac1{d_a} \log \det \Theta_a - \log \sum_{i=1}^n \braket{X_i, \left( \textstyle \bigotimes_{a=1}^k \Theta_a \right) X_i},
\end{align*}
or equivalently minimize its negation.
Since this function is also invariant under rescaling the~$\Theta_a$, we may restrict to $\PD_{d_a}^1$, the $d_a\times d_a$ positive definite matrices with unit determinant.
Thus we are led to minimizing the following function:

\begin{definition}[Objective]\label{dfn:function}
Given $n$ samples $\vec{X} = (X_1, \dots, X_n)$ in $\R^D$, define the function $f_{\vec X}\colon\prod_{a=1}^k \PD_{d_a}^1 \to \R$ by
\begin{align}\label{eq:projective-likelihood}
  f_{\vec X}(\Theta_1,\dots,\Theta_k) = \log \sum_{i=1}^n \braket{X_i, \left( \textstyle \bigotimes_{a=1}^k \Theta_a \right) X_i}.
\end{align}
Let $(\widehat{\Theta}_{\vec X,1}, \dots, \widehat{\Theta}_{\vec X,k})$ denote the minimizer of $f_{\vec X}$ if it exists and is unique.
\end{definition}

\noindent
As explained above, $\widehat{\Theta}_{\vec X} = \widehat{\Theta}_{\vec X,1} \ot \cdots \ot \widehat{\Theta}_{\vec X,k}$ is directly related to the maximum likelihood estimator for the tensor normal model (namely, by an known overall multiplicative factor).
We will often identify $\Theta$ with the tuple $(\Theta_1,\dots,\Theta_k)$, and similarly for $\widehat{\Theta}_{\vec X}$.
When $\vec X$ is clear from context we will drop it.


%-----------------------------------------------------------------------------
\subsection{Sample complexity from geodesic convexity}\label{subsec:outline}
%-----------------------------------------------------------------------------
\TODO{
\begin{itemize}
\item change variables instead of using $d$-norm
\end{itemize}}
Following \cite{FM20}, and in spirit also \CF{cite weisel}, we use geodesic convexity to prove \cref{thm:tensor-frobenius}.
\Cref{thm:matrix-normal}, proven later in \cref{sec:matrix-normal}, requires the stronger notion of \emph{quantum expansion}.

Before defining our notion of convexity, we introduce some notation.
Let~$\SL_d$ denote the group of matrices with unit determinant.
Recall that $\PD_d^1$ denotes the positive definite matrices with unit determinant.
Let $\Sym_d^0$ denote the vector space of $d\times d$ traceless symmetric matrices.
Then, $A^T e^Z A \in \PD_d^1$ for any $A \in \SL_d$ and $Z\in\Sym_d^0$.
% Any matrix in $\PD_d^1$ can be written as the matrix exponential of a matrix in $\Sym_d^0$.
Finally, let $G = \prod_{a=1}^k \SL_{d_a}$, $P = \prod_{a=1}^k \PD_{d_a}^1$, and $S = \bigoplus_{a=1}^k \Sym_{d_a}^0$.
We denote by $AB=(A_1B_1,\dots,A_kB_k)$ and $e^Z=(e^{Z_1},\dots,e^{Z_k})$ the componentwise product and matrix exponential, respectively, of matrix tuples $A, B \in G$ and $Z\in S$.

%\CF{technically $\Theta(t)$ is ill-defined} Then, the curve $\Theta(t) = A^T e^{tZ} A = (A_1^T e^{t Z_1} A_1, \dots, A_k^T e^{t Z_k} A_k)$ is a geodesic or `straight line' through $\Theta = A^T A$ with direction~$Z$ for a natural Riemannian metric on~$P$~\cite{bhatia2009positive}.
This motivates the following definition.

\begin{definition}[Geodesic convexity]\label{dfn:g-convexity}
 Let $\Theta \in P$. The \emph{exponential map} $\exp_\Theta:S \to P$ at $\Theta$ is defined by 
$$ \exp_\Theta(Z) = \Theta^{1/2} e^{\sqrt{\vec d}Z} \Theta^{1/2},$$
where $\sqrt{\vec d} Z$ denotes the componentwise product $(\sqrt{d_1} Z_1, \dots, \sqrt{d_k}Z_k)$.
The \emph{geodesics} through $\Theta$ are the curves $t\mapsto \exp_\Theta(tZ)$ for $t \in \R$. These curves are in fact geodesics, or ``straight lines,'' through $\Theta$ for a natural Riemannian metric on~$P$~\cite{bhatia2009positive}.
% The length of the curve given by restricting $\gamma$ to $[0,1]$, or $\gamma|_{[0,1]}$, is defined to be $\|Z\|_F$. 


A function $f\colon P \to \R$ is said to be \emph{geodesically convex} if the function 
$$\R \to \R, \quad t \mapsto f (\exp_\Theta( tZ))$$ is convex for every $\Theta \in P, Z \in S$, i.e. the function $f \circ \gamma$ is convex for every geodesic $\gamma$. Equivalently, assuming $f$ is twice differentiable, $\partial^2_t f(\exp_\Theta(tZ)) \geq 0.$ 

Further, say $f$ is \emph{$\lambda$-strongly geodesically convex} at a point $\Theta \in P$ if, for some (equivalently, for any) $A \in G$ with $\Theta = A^T A$ and for any $Z \in S$, we have
\begin{align*}
  \partial^2_{t=0} f(\exp_\Theta(tZ)) \geq \lambda \norm{Z}^2.
\end{align*}
% and \emph{$L$-geodesically smooth} with respect to the norm $\norm\cdot$ if
% \begin{align*}
%   \partial^2_t f(A^T e^{t Z} A) \leq L \norm{Z}^2 \qquad (\forall Z\in S).
% \end{align*}
%Here we use the norm $\norm{\cdot}_{\vec d}$ induced by the rescaled Hilbert-Schmidt inner product $\braket{X, Y}_{\vec d} = \sum_{a=1}^k \frac1{d_a} \tr(X_a Y_a)$ for $X, Y \in S$.
\end{definition}

The function $f_{\vec X}$ defined in \cref{dfn:function} is geodesically convex~\cite{burgisser2019towards}. \CF{see the commented out motivation. Where to put it?}
\MW{Write $\norm Z_F^2 = \sum_i \norm{Z_i}_F^2$???}
%One might wonder why we define $\exp_\Theta$ using the componentwise multiplication by $\sqrt{\vec d}$. In fact, this is equal to the exponential map for the Riemannian metric $\braket{X, Y}_{\vec d} = \sum_{a=1}^k \frac1{d_a} \tr(X_a Y_a)$. Oddly enough, defining the gradient and Hessian according to this metric, rather than the usual Hilbert-Schmidt inner product, improves the ratio between the norm of the gradient of $f_{\vec X}$ and its strong geodesic convexity parameter - a ratio which controls the quality of our statistical estimate. 
%RMO: Is this oddity simply because of the structure of the g-convex function that we are studying, the fact that it is a function over $\otimes \Theta_i$ and therefore we will get these $d_i$'s naturally? (todo for myself: do these calculations and see)
We also note that $f$ has a certain useful transformation property or ``equivariance." For all $\Theta \in P$, $A \in G$, and $\vec X=(X_1,\dots,X_n)$, we have that
\begin{align}\label{eq:equivariance}
  f_{A^{-1} \vec X}(A^T \Theta A) = f_{\vec X}(\Theta),
\end{align}
where we write $A^{-1} \vec X = (A^{-1} X_1,\cdots,A^{-1} X_n)$.
With these definitions in place, we are able to state the rather straightforward plan.

\begin{enumerate}
\item\label{it:reduce} \textbf{Reduce to identity:}
Firstly, \cref{eq:equivariance} implies that $\widehat\Theta_{\Theta^{-1/2} \vec X} = \Theta^{1/2} \widehat\Theta_{\vec X} \Theta^{1/2}$ assuming either minimizer exists and is unique.
Therefore,
\begin{align*}
  d_F(\widehat\Theta_{\Theta^{-1/2} \vec X}; \Theta)
= \norm{ I - \Theta^{-1/2} \widehat\Theta_{\Theta^{-1/2} \vec X} \Theta^{-1/2} }_F
% = \norm{ I - \widehat\Theta_{\Theta^{1/2} \vec X} }_F
= d_F(\widehat\Theta_{\vec X}; I)
\end{align*}
and likewise for the distance $d_{\op}$.
Now if $\vec X$ is distributed as $n$ independent samples from a standard Gaussian, $\Theta^{-1/2} \vec X$ is distributed as $n$ independent samples from $\cN(0, \Theta^{-1})$.
This shows that to prove \cref{thm:tensor-frobenius} it is enough to consider the case that $\Theta = I$ and $\vec X$ is standard Gaussian.
\item\label{it:grad} \textbf{Bound the gradient:}
Show that the gradient $\nabla f_{\vec X}(I)$ (defined below) is small with high probability.
\item\label{it:convexity} \textbf{Show strong convexity:}
Show that, with high probability, $f_{\vec X}$ is $\Omega(1)$-strongly geodesically convex near $I$.
\end{enumerate}

These together imply the desired sample complexity bounds -- as in the Euclidean case, strong convexity in a suitably large ball about a point implies the optimizer cannot be far.
Furthermore, like in the Euclidean case, if a function is strongly geodesically convex in a region and one can prove a \emph{descent lemma} for a sequence in the region, then it must converge exponentially quickly to an optimizer.
It is well-known that various optimization algorithms applied to $f_{\vec X}$ satisfy such a descent lemma~\cite{burgisser2019towards}.

To make this discussion more concrete, we now define the gradient formally and state the lemma that we will use to relate the gradient and strong convexity to the distance to the optimizer as in the plan above.

\begin{definition}[Riemannian gradient]
For $\Theta \in P$, the \emph{Riemannian gradient}~$\nabla f(\Theta)$ is the unique element in $S$ such that
\begin{align*}
  \braket{\nabla f(\Theta), Z} = \partial_{t=0} f(\exp_{\Theta}(Zt))
\end{align*}
for all $Z\in S$.
We often abbreviate $\nabla f = \nabla f(I)$ and write $\nabla f = (\nabla_1 f, \dots, \nabla_k f)$.
\end{definition}

\noindent
Now say that $B_\kappa(\Theta) = \{ \exp_{\Theta}(Z) : \norm Z \leq \kappa \}$ is the ball of radius $\kappa>0$ about~$\Theta$.

\begin{lemma}\label{lem:convex-ball}
Let $f\colon P\to \R$ be geodesically convex everywhere, and $\lambda$-strongly geodesically convex in a ball of radius~$\kappa>0$ about~$I$.
Further assume the Riemannian gradient satisfies $\norm{\nabla f(I)} \leq \eps < \lambda \kappa / 2$.
Then $f$ has a unique minimizer, which is contained in the ball of radius~$\eps/\lambda$ about~$I$.
\end{lemma}
\begin{proof}
We first show that $f$ has a minimum.
Consider $g(t) := f(\exp_I(tZ))$, where $Z\in S$ is an arbitrary vector of unit norm~$\norm Z = 1$.
Then, using the assumption on the gradient,
\begin{align}\label{eq:grad bound}
  g'(0)
= \partial_{t=0} f(\exp_{I}(tZ))
= \braket{\nabla f(I), Z}
\geq -\norm{\nabla f(I)} \, \norm Z
\geq -\eps.
\end{align}
Since $f$ is $\lambda$-strongly geodesically convex on $B_\kappa(I)$, we have $g''(t) \geq \lambda$ for all $\abs t\leq\kappa$.
For $t=\kappa$, it follows in particular that
\begin{align*}
  g(\kappa)
\geq g(0) + g'(0) \kappa + \lambda \frac{\kappa^2}2
\geq g(0) - \eps \kappa + \lambda \frac{\kappa^2}2
= g(0) + \left( \frac{\lambda\kappa}2 - \eps \right) \kappa
> g(0)
\end{align*}
Since $g$ is convex due to the geodesic convexity of $f$, it follows that, for any~$t \geq \kappa$,
\begin{align*}
  g(0) < g(\kappa) \leq \left( 1-\frac{\kappa}t \right) g(0) + \frac{\kappa}t g(t),
\end{align*}
hence
\begin{align*}
  f(I) = g(0) < g(t) = f(\exp_{I}(tZ)).
\end{align*}
It follows that the sublevel set of~$f(I)$ is contained in the ball of radius~$\kappa$, which is a compact set.
In particular, $f$ has a minimum in this ball.

Next, we prove that any minimizer of $f$ is necessarily contained in the (smaller) ball of radius~$\eps/\lambda$.
To see this, take an arbitrary minimizer and write it in the form $\exp_I(TZ)$, where $Z\in S$ is a unit vector and $T>0$.
As before, we consider the function $g(t) = f(\exp_I(tZ))$.
Then, using \cref{eq:grad bound}, the convexity of~$g(t)$ for all $t\in\R$ and the strong convexity of $g(t)$ for $\abs t \leq \kappa$, we have
\begin{align*}
  0 = g'(T) = \int_0^T g''(t) \, dt + g'(0) \geq \lambda \min(T, \kappa) - \eps.
\end{align*}
If $T>\kappa$ then we have a contradiction as $\lambda\kappa - \eps > \lambda\kappa/2 - \eps > 0$.
Therefore, we must have $T\leq\kappa$ and hence $\lambda T - \eps \leq 0$, so $T \leq \eps/\lambda$.
Thus we have proved that any minimizer of $f$ is contained in the ball of radius $\eps/\lambda$.

%\CF{just track down some ref, or proceed as in commented text.} 
%To see that the minimimizer $\Theta$ is unique, we observe that geodesic strong convexity implies strong star convexity of the function $f \circ \exp_{\Theta}$, which has a unique global minimizer at $0$ if and only if $\Theta$ is the unique minimizer of $f$. It is easy to see that function that is strongly star convex in a neighborhood of the origin is uniquely minimized at the origin.

We still need to show that the minimizer is unique; that this follows from strong convexity is convex optimization ``folklore,'' but we include a proof nonetheless. Indeed, suppose that $\Theta$ is a minimizer and let $Z\in S$ be arbitrary.
Consider $h(t) := f(\exp_{\Theta}(tZ))$.
Then the function $h(t)$ is convex, has a minimum at $t=0$, and satisfies $h''(0) > 0$, since $f$ is $\lambda$-strongly geodesically convex at $\Theta$ by what we showed above.
It follows that $h(t) > h(0)$ for any $t\neq0$.
Since $Z$ was arbitrary, this shows that $f(\Theta') > f(\Theta)$ for any $\Theta'\neq\Theta$.
\end{proof}

%-----------------------------------------------------------------------------
\subsection{Bounding the gradient}
%-----------------------------------------------------------------------------
As suggested in the plan from the previous section, we now compute the gradient of $f_{\vec X}$ and bound it using basic matrix concentration results.

To calculate the gradient, we need a definition from linear algebra.

\begin{definition}[Partial trace]\label{def:single marginal}
For $\rho \in \PD_D$ and $a\in[k]$, let $\rho^{(a)} \in \PD_{d_a}$ denote the \emph{partial trace} defined by the condition that
\begin{align*}
  \tr \rho^{(a)} Z = \tr \rho Z_a,
\end{align*}
for all $d_a \times d_a$ matrices $Z$, where $Z_a = (I_1 \ot \cdots \ot I_{a-1} \ot Z \ot I_{a+1} \ot \cdots \ot I_k)$.
Here and in the following we write $I_a$ for the $d_a\times d_a$ identity matrix.
\end{definition}
\CF{the above definition conflicts a bit with just referring to $Z_a$ as the $a^{th}$ component, which I have done elsewhere. How to resolve?}
\MW{Explain how to compute in terms of $\vec X$. Ref above \cref{cor:vershynin}.}

\begin{lemma}[Riemannian gradient]\label{lem:gradient}
Let $\rho = \sum_{i=1}^n X_i X_i^T / \norm{\vec X}_2^2$.
Then the components of the Riemannian gradient $\nabla f_{\vec X}$ at the identity are given by
\begin{align*}
  \nabla_a f_{\vec X} = \sqrt{d_a} \rho^{(a)} - I_a/\sqrt{d_a} \qquad \text{ for } a \in [k].
\end{align*}
\end{lemma}
\begin{proof}
  Let $F_{\vec X}(\Theta) = \tr \rho \, \Theta$.
  Then, for all $a\in[k]$, we have
  \begin{align*}
   \langle \nabla_a F_{\vec X} , Z \rangle
  % = \partial_{t=0} F_{\vec X}(I_1,\dots,I_{a-1}, e^{tZ}, I_{a+1},\dots,I_n)
  = \partial_{t=0} \tr \rho \, e^{t \sqrt{d_a} Z_a}
  = \sqrt{d_a} \tr \rho Z_a
  = \sqrt{d_a} \tr \rho^{(a)} Z
  \end{align*}
  for all traceless symmetric $d_a \times d_a$ matrices $Z$.
  Since $\nabla_a F_{\vec X}$ is traceless and symmetric by definition, this implies
  \begin{align*}
    \nabla_a F_{\vec X}
  = \sqrt{d_a} \rho^{(a)} - \tr(\rho^{(a)}) I_a /\sqrt{d_a}
  = \sqrt{d_a} \rho^{(a)} - I_a/\sqrt{d_a}.
  \end{align*}
  Finally, note that $f_{\vec X}(\Theta) = \log F_{\vec X}(\Theta) + \log \norm{\vec X}_2^2$, so
  \begin{align*}
    \nabla_a f_{\vec X}
  = \frac{\nabla_a F_{\vec X}}{F_{\vec X}(I)}
  = \nabla_a F_{\vec X}
  = \sqrt{d_a} \rho^{(a)} - I_a/\sqrt{d_a},
  \end{align*}
  using that $F_{\vec X}(I) = \tr \rho = 1$.
\end{proof}

Having calculated the gradient, we are ready to state our bound:
%\MW{Please check updated proof. Can we assume $\eps<1$ and simply take~$C=16$?}
\MW{Make pretty by manipulating $C$}
\begin{prop}\label{prop:gradient-bound}
Let $\vec X = (X_1,\dots,X_n)$ be independent standard Gaussian random variables in $\R^D$, where $D=d_1\cdots{}d_k$.
Let $\eps>0$ and $C = 16^2\max\{\eps^2,1\}$.
Suppose that, for all $a \in [k]$,
\begin{align*}
  N_a := \frac{n D}{d_a} \geq C \frac{d_a}{\eps^2}.
\end{align*}
Then, with probability at least $1 - 2 \sum_{a=1}^k e^{-N_a \eps^2/2C}$ we have that
\begin{align*}
  \norm{\nabla_a f_{\vec X}}_{\op} \leq \eps/\sqrt{d_a}
\end{align*}
for all $a\in[k]$, and hence $\norm{\nabla f_{\vec X}}^2 \leq k \eps^2$.
\end{prop}

To prove this we will need a standard result in matrix concentration.
Recall that $\nabla_a f_{\vec X} = \sqrt{d_a} \rho^{(a)} - I_a/\sqrt{d_a}$ by \cref{lem:gradient}.
It is not hard to see that $\rho^{(a)}$ is distributed as $Y Y^T / \norm{Y}_F^2$, where $Y$ is a $d_a \times N_a$ matrix with independent Gaussian entries.
The following result bounds the singular values of such random matrices.

\begin{theorem}[Corollary 5.35 of \cite{vershynin2010introduction}]\label{cor:vershynin}
Let $Y \in \R^{d \times N}$ have independent standard Gaussian entries.
Then, for $t \geq 0$, the following occurs with probability at least $1 - 2 \exp(-t^{2}/2)$:
\begin{align*}
  \sqrt{N} - \sqrt{d} - t \leq s_{\min}(Y) \leq s_{\max}(Y) \leq \sqrt{N} + \sqrt{d} + t,
\end{align*}
where $s_{\min}$ and $s_{\max}$ denote the smallest and largest singular value, respectively (we allow for singular values that are zero).
\end{theorem}

\begin{proof}[Proof of \cref{prop:gradient-bound}]
We first consider a fixed $a\in[k]$ and later take a union bound.
Let $d = d_a$ and $N = N_a = n D/d_a$.
Again, $\nabla_a f_{\vec X} = \sqrt{d_a} \rho^{(a)} - I_a/\sqrt{d_a}$ where $\rho^{(a)}$ is distributed as $Y Y^T/\|Y\|_F^2$, where $Y$ is a $d \times N$ matrix with independent Gaussian entries.
By \cref{cor:vershynin}, we have the following with $\leq 2 \exp(-t^2/2)$ failure probability:
\begin{align*}
  \sqrt{N} \left( 1 -  \frac{\sqrt{d} + t }{\sqrt{N}}  \right)\leq s_{\min}(Y) \leq s_{\max}(Y) \leq  \sqrt{N} \left( 1 + \frac{\sqrt{d} + t }{\sqrt{N}} \right).
\end{align*}
Let $t = \sqrt{N} \eps / \sqrt{C}$.
Because $d \leq N \eps^2 / C$, this shows that all singular values are in $\sqrt{N} \left( 1 \pm 2\eps/\sqrt{C} \right)$.
It follows that the eigenvalues of $YY^T$ are in $N \left( 1 \pm 8\eps/\sqrt{C} \right)$.
Since there are at most $d \leq N$ nonzero eigenvalues, this implies in turn that the eigenvalues of $Y Y^T/\|Y\|_F^2$ are in
\begin{align*}
\frac {\left( 1 \pm 8\eps/\sqrt{C} \right)} {d \left( 1 \pm 8\eps/\sqrt{C} \right)} \subseteq \frac1d \left( 1 \pm 16\eps/\sqrt{C} \right) \subseteq \frac1d \left( 1 \pm \eps \right),
\end{align*}
Thus,
$\norm{\nabla_a f_{\vec X}}_{\op} \leq \eps/\sqrt{d_a}$
with failure probability at most $2 \exp(-N \eps^2/2C)$.
Applying the union bound over $[k]$ completes the proof of the proposition.
% Next, observe that $\|Y\|_F^2$ is a $\chi^2$-distribution with $Nd$ degrees of freedom, and thus satisfies
% \begin{align*}
%   \norm Y_F^2 - Nd \leq tNd
% \end{align*}
% with failure probability at most $e^{-Ndt^2/18}$ for $t \in [0,3]$~\cite[(2.44)]{W19}.
% WAS: $\| Y\|_F^2 - Nd \leq t N d$ with probability $1 - 2e^{- Nd t^2/8}$ for $t \leq 1$ \cite{W19}.
% Thus
  % $$\lambda_1(\rho^{\{a\}}), \lambda_2(\rho^{\{a\}}) \in  \cN(1 \pm c \cdot \eps)/ Nd(1 \pm c \cdot \eps) \in \frac{1}{d}(1 \pm \eps)$$
% with probability $1 - 2 e^{- c N \eps^2}- 2 e^{- c Nd \eps^2} = 1 - 4 e^{- N\eps^2},$ provided $c$ is small enough.
\end{proof}

% Both $\|Y Y^\dagger - N I\|_{op} \leq  c N/\log d_a$ and $\| \vec X\|_F^2 - Nd_a \leq .5 N d_a$ hold with probability \CF{blah}$. Now  $$ \|\rho^{\{a\}} - I_a/d_a\|_{op} \leq \|\rho^{\{a\}} - N I_{d_a}/ \|\vec X\|_F\|_{op} + $$


%Provided $c$ is small enough we then have
 %$$\|Y Y^\dagger - N I\|_{op} \leq  c N/\log d_a.$$


% Next, observe that $\|\vec X\|_F^2$ is a $\chi$-squared distribution with $Nd_a$ degrees of freedom, and thus satisfies $\| \vec X\|_F^2 - Nd_a \leq .5 N d_a$ with probability $1 - 2e^{- c N d_a}$. Both $\|Y Y^\dagger - N I\|_{op} \leq  c N/\log d_a$ and $\| \vec X\|_F^2 - Nd_a \leq .5 N d_a$ hold with probability \CF{blah}$. Now  $$ \|\rho^{\{a\}} - I_a/d_a\|_{op} \leq \|\rho^{\{a\}} - N I_{d_a}/ \|\vec X\|_F\|_{op} + $$



%-----------------------------------------------------------------------------
\subsection{Strong convexity}
%-----------------------------------------------------------------------------
\CF{will move this to the appropriate place; rafael you can cite this for section 4 and just eventually put in the right probability bound.}
\begin{theorem}\label{thm:ball-convexity}
There is $c> 0$ such that the following holds: with probability at least \CF{good; akshay thinks at least $1 - n^{- \Omega(d_{min})}$} the function $f_{\vec X}$ is $\Omega(1)$-strongly convex at all points $Z$ with $\sum_{i = 1}^n \|\log Z_a\|_{op} < c/k.$ 




\end{theorem}



We now prove our main strong convexity results in order to carry out step~\ref{it:convexity} of the plan from \cref{subsec:outline}.
Just like in the Euclidean case, the Hessian is convenient to characterize strong convexity, and we begin by defining it formally in the Riemannian context.
We then show strong convexity \emph{at} the origin, and later show strong convexity \emph{near} the origin.

\begin{definition}[Riemannian Hessian]
For $\Theta \in P$, the \emph{Riemannian Hessian}~$\nabla^2 f(\Theta)$ is the unique linear operator on $S$ such that
\begin{align*}
  \braket{Y, \nabla^2 f(\Theta) Z} = \partial_{s=0} \partial_{t=0} f(\exp_{\Theta}(sY + tZ))
\end{align*}
for all $Y, Z\in S$.
We often abbreviate $\nabla^2 f = \nabla^2 f(I)$.
As a block matrix,
\begin{align*}
  \nabla^2 f = \begin{bmatrix}
  \nabla_{11}^2 f & \dots & \nabla_{1k}^2 f \\
  \vdots & \ddots & \vdots \\
  \nabla_{k1}^2 f & \dots & \nabla_{kk}^2 f \\
  \end{bmatrix},
\end{align*}
where $\nabla_{ab}^2f$ is an operator from $\Sym_{d_b}^0$ to $\Sym_{d_a}^0$.
\end{definition}

We note that the Riemannian Hessian is symmetric with respect to the inner product~$\braket{\cdot,\cdot}$ on $S$.
Furthermore, $\braket{Z, \nabla^2 f(\Theta) Z} = \partial^2_{t=0} f(\exp_{\Theta}(tZ))$ for all $Z\in S$.
Thus, $f$~is geodesically convex if and only if the Riemannian Hessian is positive semidefinite, that is, $\nabla^2 f(\Theta) \succeq 0$.
Similarly, $f$ is $\lambda$-strongly geodesically convex at $\Theta$ if and only if $\nabla^2 f(\Theta) \succeq \lambda I$, i.e., the Hessian is positive definite with eigenvalues larger than or equal to~$\lambda$.
%\MW{Here, $\succeq$ denotes the Loewner order with respect to $\braket{\cdot,\cdot}_{\vec d}$.}

Similarly as for the gradient, we can compute the components of the Hessian in terms of a partial trace, as in the following definition and lemma.

\begin{definition}[Partial trace]
For $\rho \in \PD_D$ and $a \neq b\in[k]$, let $\rho^{(ab)} \in \PD_{d_a \times d_b}$ denote the \emph{partial trace} defined by the condition that
\begin{align*}
  \tr \rho^{(ab)} (Y \ot Z) = \tr \rho Y_a Z_b,
\end{align*}
for all $d_a \times d_a$ matrices $Y$ and $d_b \times d_b$ matrices $Z$, where, as before, we write~$Y_a = (I_1 \ot \cdots \ot I_{a-1} \ot Y \ot I_{a+1} \ot \cdots \ot I_k)$ and similarly for $Z_b$. \CF{does partial trace really need to be defined twice?}
\end{definition}

\noindent
This definition is consistent with \cref{def:single marginal} in the sense that $\tr \rho^{(ab)} (Y \ot I) = \tr \rho^{(a)} Y$ and $\tr \rho^{(ab)} (I \ot Z) = \tr \rho^{(b)} Z$ for all $\rho$, $Y$, $Z$.

\begin{lemma}[Riemannian Hessian]\label{lem:hessian}
Let $\rho = \sum_{i=1}^n X_i X_i^T / \norm{\vec X}_2^2$.
Then the Riemannian Hessian $\nabla^2 f_{\vec X}$ at the identity is given by \CF{fix $Y_a$ here}
\begin{align*}
 \frac{1}{d_a} \langle Y_a,  \left( \nabla^2_{aa} f_{\vec X} \right) Y_a \rangle
&= \left( \tr \rho^{(a)} Y^2 - \bigl(\tr Y \rho^{(a)}\bigr)^2 \right) \\
  \frac1{\sqrt{d_a d_b}} \langle Y_a,  \left( \nabla^2_{ab} f_{\vec X} \right) Z_b \rangle
&= \tr \rho^{(ab)} \left( Y \ot Z \right) - \bigl(\tr Y \rho^{(a)}\bigr)\bigl(\tr Z \rho^{(b)}\bigr) 
\end{align*}
for all $a\neq b\in[k]$ and traceless symmetric $d_a\times d_a$ matrices $Y$, $d_b\times d_b$ matrices~$Z$.
\end{lemma}
\begin{proof}
  Let $F_{\vec X}(\Theta) = \tr \rho \, \Theta$ as in the proof of \cref{lem:gradient}.
  Then,
  \begin{align*}
   \langle Y_a,  \left( \nabla^2_{aa} F_{\vec X} \right) Y_a \rangle
  &= \partial_{s=0} \partial_{t=0} \tr \rho \, e^{(s+t)\sqrt{d_a}  Y_a} \\
  &= \partial_{s=0} \partial_{t=0} \tr \rho^{(a)} e^{(s+t)  \sqrt{d_a}Y}
  = d_a \tr \rho^{(a)} Y^2,
  \end{align*}
  while for $a\neq b$ we have
  \begin{align*}
    \langle Y_a,  \left( \nabla^2_{ab} F_{\vec X} \right) Z_b \rangle
  &= \partial_{s=0} \partial_{t=0} \tr \rho \, e^{s \sqrt{d_a}Y_a + t \sqrt{d_b} Z_b} \\
  &= \partial_{s=0} \partial_{t=0} \tr \rho^{(ab)} \left( e^{s \sqrt{d_a}  Y} \ot e^{t \sqrt{d_b} Z} \right)\\
  &= \sqrt{d_a d_b} \tr \rho^{(ab)} \left( Y \ot Z \right).
  \end{align*}
  %Since $f_{\vec X}(\Theta) = \log F_{\vec X}(\Theta) + \log \norm{\vec X}_2^2$, we have
  Since $f_{\vec X}(\Theta) = \log F_{\vec X}(\Theta)$ \CF{deleted a term; please see commented text to check}, we have
  \begin{align*}
    \braket{Y, (\nabla^2 f_{\vec X}) Z}
  &= \frac{\braket{Y, (\nabla^2 F_{\vec X}) Z}}{F_{\vec X}(I)} - \frac{\braket{Y, \nabla F_{\vec X}} \braket{\nabla F_{\vec X}, Z}}{F_{\vec X}(I)^2} \\
  &= \frac{\braket{Y, (\nabla^2 F_{\vec X}) Z}}{F_{\vec X}(I)} - \braket{Y, \nabla f_{\vec X}} \braket{\nabla f_{\vec X}, Z}.
  \end{align*}
  % \begin{align*}
  %   \nabla^2 f_{\vec X}
  % = \frac {\nabla^2 F_{\vec X}} {F_{\vec X}(I)} - \frac{\nabla F_{\vec X} \braket{\nabla F_{\vec X}, \cdot}_{\vec d}}{F_{\vec X}(I)^2}
  % = \frac {\nabla^2 F_{\vec X}} {F_{\vec X}(I)} - \nabla f_{\vec X} \braket{\nabla f_{\vec X}, \cdot}_{\vec d},
  % \end{align*}
  % where $\dagger$ denotes the adjoint with respect to the inner product $\braket{\cdot,\cdot}_{\vec d}$.
  Since $F_{\vec X}(I)=1$, plugging in the above formulas and using \cref{lem:gradient} for the gradient completes the proof.
  % Indeed,
  % \begin{align*}
  %   \frac1{d_a} \tr Y (\nabla^2_{aa} f_{\vec X}) Y
  % = \frac1{d_a} \tr Y (\nabla^2_{aa} F_{\vec X}) Y - \frac1{d_a} \tr Y (\nabla_a f_{\vec X}) \frac1{d_a} \tr Y (\nabla_a f_{\vec X})
  % = \tr \rho^{(a)} Y^2 - (\tr Y \rho^{(a)})^2
  % \end{align*}
  % and
  % \begin{align*}
  %   \frac1{d_a} \tr Y (\nabla^2_{ab} f_{\vec X}) Z
  % = \frac1{d_a} \tr Y (\nabla^2_{ab} F_{\vec X}) Z - \frac1{d_a} \tr Y (\nabla_a f_{\vec X}) \frac1{d_b} \tr Z (\nabla_b f_{\vec X})
  % = \tr \rho^{(ab)} \left( Y \ot Z \right) - (\tr Y \rho^{(a)})(\tr Z \rho^{(b)})
  % \end{align*}
\end{proof}

\MW{Explain that this looks like $\frac1{\norm X_2^2} \sum_i X_i \ot X_i - \dots$ if we think of \dots}
\CF{most important part of hessian is this $\rho^{ab}$ part ( a channel), discuss that there are three ways to look at it: partial trace, channel (cpm), and natural representation. Cite Pisier in channel form. In appendix prove by translating to channel}
We now state our strong convexity result at the identity:

\begin{theorem}\label{thm:tensor-convexity}
Let $\vec X = (X_1,\dots,X_n)$ be independent standard Gaussian random variables in $\R^D$, where the number of samples is $N \geq k^2 \max_{a=1}^k d_a^2 / D$. Then, with probability at least $1 - 1/{\color{red}\poly(D)}$, 
$$ (1 - (k - 1) \lambda - k \eps^2 - \eps) I \preceq 
 \nabla^2 f_{\vec X} \preceq (1 + (k - 1) \lambda + k \eps^2 + \eps) I.$$
$$ \|\nabla^{2} f_{\vec X} - I\|_{op} \leq (k - 1) \lambda + k \eps^2 + \eps $$
 In particular, $f_{\vec X}$ is $\Omega(1)$ strongly convex with probability $\CF{good}$. 
\end{theorem}

To prove this, we need the following result on quantum expansion (cf.\ \cref{sec:matrix-normal}).
The result is essentially Theorem~16.6 in~\cite{pisier2012grothendieck}, together with a standard symmetrization trick (e.g., Proof of Lemma~4.1 in~\cite{P14}).

\MW{Michael continue here.} 

\begin{theorem}[Pisier]
There is an absolute constant~$C>0$ such that, for all $N,n,m\in\N$ and any scalar sequence $a_i \in \R^N$ the following holds with probability at least $1-1/{\color{red}\poly(???)}$:
\begin{align*}
  \norm{ \sum_{i=1}^N \alpha_i (Y_i \ot Y_i) (I - P_m) }_{\op}
\leq C \norm{\alpha}_2 \left( \E \norm{Y}_{\op} \right)^2,
\end{align*}
where $Y_1,\dots,Y_N,Y$ are independent $n\times m$ standard Gaussian random matrices and $P_m$ denotes the orthogonal projection onto multiples of $\sum_{i=1}^m e_i \ot e_i$.
\MW{Is this a precise version of the Pisier result that we need? I think he proves something like this for complex tensors and square $Y_i$ (i.e., $n=m$). Should add a proof to an Appendix.}
\end{theorem}

\TODO{Fix for new inner product and dimension issue}
\CF{Separate lemma for probability of the off diagonal terms; precise version of the below incluing assumptions/probabilities.}
\begin{corollary}
For $Y \in \R^{d_{a} \times d_{b}}$ where $Y_{ij} \sim \mathcal{N}(0,1)$, our Kraus operator is of the form $\frac{1}{\sqrt{nD}} Y$, and our off-diagonal operator is
\[ \sum_{k=1}^{nD/d_{a}d_{b}} \frac{1}{nD} Y_{k} \otimes Y_{k}   \]
By the above theorem, the operator norm on the subspace $\perp$ to $(I_{a},I_{b})$ is of the order $\frac{d_{a}+d_{b}}{\sqrt{d_{a} d_{b} nD}}$, so for constant expansion it suffices that $nD \gg \max_{a} d_{a}^{2}$
\end{corollary}

\CF{The old proof had the following sentence: 
Now we can again use the high-probability bounds derived above: \TODO{actually cref them}
\begin{align}\nabla^{2}_{aa} f \in \frac{1 \pm \eps}{d_{a}}; \hspace{5mm} \forall a \neq b: \|\nabla^{2}_{ab}\|_{op} \leq \frac{\lambda}{\sqrt{d_{a} d_{b}}}. \label{eq:expansion-thing}  \end{align}
This needs to be elaborated.
}


\begin{proof}[\CF{new } Proof of \cref{thm:tensor-convexity}] 
We use the inequality for block matrices 
$$\begin{bmatrix} 0 & K \\ K^{*} & 0 \end{bmatrix} \succeq - \begin{bmatrix} A & 0 \\ 0 & B \end{bmatrix}$$ for any $A,B \succeq 0$ such that $\|A^{-1/2} K B^{-1/2}\|_{op} \leq 1,$ which may be proved by computing Schur complements. We will sequentially apply this inequality to the block matrix $\nabla^2 f$ until we are left with a diagonal matrix. 

To implement this, we proceed to bound the operator norm of $\nabla^2_{ab} f: \Sym^0_{d_b} \to \Sym^0_{d_a}$ for $a \neq b$. The gradient is the sum of two terms; the first of which is dominated in the Loewner ordering by the map $Z \mapsto \sqrt{d_a d_b} \tr_2 \rho^{(a,b)} (I \ot Z)$. By \cref{eq:expansion-thing} (\CF{which needs to actually be proved}), with probability $\CF{good}$, the map $Z \to \tr_2 \rho^{(a,b)} (I \ot Z)$ has operator norm at most $\lambda/\sqrt{d_a d_b}$ restricted to traceless $Z$. The second term defines the quadratic form 
$$Y,Z \mapsto \sqrt{d_a d_b} (\tr Y \rho^{(a)})\bigl(\tr Z \rho^{(b)}\bigr).$$ As $\|\rho^{(a)} - I_{d_a}/{d_a}\|_{op} \leq \eps/d_a, \|\rho^{(b)} - I_{d_b}/d_b\|_{op} \leq \eps/d_b$, we have
$$ |(\tr Y \rho^{(a)})\bigl(\tr Z \rho^{(b)}\bigr)| \leq \left( \frac{\eps \|Y\|_{1}}{d_{a}} \right) \left( \frac{\eps \|Z\|_{1}}{d_{b}} \right) \leq \eps^2 \frac{\|Z\|_F\|Y\|_F}{\sqrt{d_a d_b}} $$
Taken together, $\|\nabla^2_{ab} f\|_{op} \leq (\lambda + \eps^2)$ as a map $\Sym^0_{d_b} \to \Sym^0_{d_a}$. We first apply the inequality to the upper-left two by two block, or $a = 1$ and $b = 2$. By our bound on $\|\nabla^2_{ab} f\|_{op}$, we may take 
   $A = (\lambda + \eps^2) I_{d_1}$, $B = (\lambda + \eps^2)I_{d_2}$ to find that 
   \begin{align*}
\nabla^2 f \succeq \begin{bmatrix}
 \nabla^2_{11} f - (\lambda + \eps^2) I_{d_1} & 0 & \hdots &  \nabla^2_{1k} f \\
0 & \nabla^2_{22} f - (\lambda + \eps^2) I_{d_2} & \ddots & \vdots \\
\vdots & \ddots & \ddots & \vdots\\
\nabla^2_{k1}f & \hdots &  \hdots & \nabla^2_{kk} f \\
  \end{bmatrix}.\end{align*}
 Applying this for all $a < b$ we find that 
 $$\nabla^2 f  \succeq \bigoplus_{a \in [k]} \nabla^2_{aa} f - (k - 1) (\lambda + \eps^2) I_{d_a}.$$ 
The last thing we need is a lower bound on $\nabla^2_{aa} f.$ This operator is again a sum of two terms, the latter of which we have already analyzed. The former defines the quadratic form $Y \mapsto d_a \tr \rho^{(a)} Y^2$ on $\Sym^0_{d_a}$, and using the bound $\|\rho^{(a)} - I_{d_a}/{d_a}\|_{op} \leq \eps/d_a$ one checks that $\tr \rho^{(a)} Y^2 \in \frac{1 \pm \eps }{d_a} \tr Y^2 = \frac{1 \pm \eps }{d_a} \|Y\|_{F}^2. $ Combining this bound with the bound on the second term, we have $\nabla^2_{aa} f \succeq (1 -  \eps  - \eps^2 ) I_{d_a}$. Plugging this into the direct sum, we have $\nabla^2 f \succeq (1  - \eps - \eps^2  - (k - 1) (\lambda + \eps^2)) I$. Reversing the inequalities in all the steps also yields $\nabla^2 f \preceq (1  + \eps + \eps^2  + (k - 1) (\lambda + \eps^2))$.
\end{proof}


%$$\bigl(\tr Y \rho^{(a)}\bigr)\bigl(\tr Z \rho^{(b)}\bigr)$$



We now show our second strong convexity result, namely that if our function is strongly convex at the origin then it is also strongly convex in an operator norm ball about the origin. We'll use an easy fact relating the exponential map and the operator norm.

\begin{fact} For all symmetric $d\times d$ matrices $\delta $ such that $ \|\delta\|_{op} \leq \frac{1}{2}$, we have $$ \|e^{\delta} - I\|_{op} \leq 2 \|\delta\|_{op}.$$   \end{fact}

We will follow the structure presented in the last proof by letting our input at the identity be represented $\{X_{i}\}$ and showing each block of the Hessian only changes a small amount under perturbation $\{X'_{i} := e^{\delta} X_{i}\}$ for $\delta \in S$. \CF{ I am a little confused, is $\delta$ a single hermitian or a tuple of Hermitians? Right now our notation in the definition of $(Z)_{b}$ assumes $Z$ is a single matrix} In particular we will give bounds on each block under each component-wise perturbation $\{X'_{i} := (e^{\delta})_{b} X_{i}\}$. Recall (c.f. \cref{lem:hessian}) that the $a^{th}$ diagonal block of the Hessian depends only on $\rho^a_{\vec X}$. This motivates the next two lemmas quantifying the change of $\rho^{a}
_{\vec X}$ under perturbations. 

\CF{for the readers' sake some justification is needed for why perturbing $X$ this is the same as considering the Hessian of our function at another point in $P$. We'll probably have to discuss this earlier in the paper when we mention all the different perspectives for scaling, but for now a little reminder would help.}
\begin{lemma} \label{atoaaRobustness}
There is a constant $c>0$ such that the following holds. For input $\{X_{i}\}$ and perturbation $\|\delta\|_{op} \leq c$ giving $\{X'_{i} := (e^{\delta})_{a} X_{i} \}$, there is a universal constant $C$:
\[ \|\rho_{X'}^{a} - \rho_{X}^{a}\|_{op} = C \|\delta\|_{op} \|\rho_{X}^{a}\|_{op}     \]
\end{lemma}
\begin{proof} By definition, $\|\rho_{X'}^{(a)} - \rho_{X}^{(a)}\|_{op} = \sup_{\|Z\|_{1} \leq 1} \langle Z_{a}, \rho_{X'} - \rho_{X} \rangle $. 

Choose $\eta$ such that  $\|(e^{\delta})_{a} X\|_{2}^{-1} = \|X\|_{2}^{-1} (1 + \eta)$; note that $|\eta| = O(\|\delta\|_{op})$ provided $c$ is small enough. Letting $\delta' := (1+\eta)e^{\delta} - I_{a}$. Assuming without loss of generality that $\|Z\|_{1} = 1$, we have \CF{added $a$ subscript on $(I + \delta');$ please check}
\[ | \langle Z_{a}, (I+\delta')_a \rho_{X} (I+\delta')_a - \rho_{X} \rangle | \]
\[ \leq (2\|\delta'\|_{op} + \|\delta'\|_{op}^{2}) \|\rho^{(a)}\|_{op} \|Z\|_{1}    \]
from which the lemma follows since $\|\delta\|_{op} \leq c$.
\end{proof}

\CF{ I think we should combine these lemmas into a single one with two items.}
\AR{The proofs are different, and I like the similar structure for diagonal/off-diagonal blocks. It may clutter the statements more to combine. }
\begin{lemma} \label{btoaaRobustness}
There is a constant $c>0$ such that the following holds. For input $\{X_{i}\}$ and perturbation $\|\delta\|_{op} \leq c$ giving $\{X'_{i} := (e^{\delta})_{b} X_{i} \}$ for $b \neq a$, there is a universal constant $C$:
\[ \|\rho_{X'}^{a} - \rho_{X}^{a}\|_{op} \leq C \|\delta\|_{op} \|\rho_{X}^{a}\|_{op}      \]
\end{lemma}
\begin{proof} 
Choose $\eta$ such that $\|(e^{\delta})_{b} X\|_{2}^{-2} = (1+\eta) \|X\|_{2}^{-2}$ and let $\delta' := (1+\eta)e^{2\delta} - I$. We assume for now $Z \succeq 0$. 
\begin{align*} | \langle Z_{a}, (1+\eta) (e^{\delta})_{b} \rho_{X} (e^{\delta})_{b}^{*} - \rho_{X} \rangle| 
& = | \langle Z_{a} \otimes \delta'_{b}, \rho_{X} \rangle   |  \\ 
&\leq \langle Z \otimes |\delta'|, \rho_{X}^{(ab)} \rangle   
\leq \|\delta'\|_{op} \langle Z, \rho_{X}^{(a)} \rangle    
\end{align*}
Here in the first inequality we used that $\rho_{X} \succeq 0, Z \succeq 0$; and the last inequality was by definition of marginals. 
In general we decompose $Z = Z_{+} - Z_{-}$ and use the above to show
\[ |\langle Z, \rho_{X'}^{(a)} - \rho_{X}^{(a)} \rangle| \leq O(\|\delta\|_{op}) (\|Z_{+}\|_{1} + \|Z_{-}\|_{1}) \|\rho_{X}^{(a)}\|_{op}     \]
The lemma follows by noting $\|Z\|_{1} = \|Z_{+}\|_{1} + \|Z_{-}\|_{1}$ and $\|\delta\|_{op} \leq c$:
\[ \|\delta'\|_{op} = \|(1+\eta) e^{2 \delta} - I\|_{op} \leq |\eta|(1 + 2 \|\delta\|_{op}) + 2\|\delta\|_{op} \leq O(\|\delta\|_{op})   \qedhere  \]
\end{proof}

This gives a bound on the diagonal blocks of the Hessian as well as the rank-one term $\bigl(\tr Y \rho^{(a)}\bigr)\bigl(\tr Z \rho^{(b)}\bigr)$ in all other blocks.

\begin{corollary} \label{diagRobustness}
There is a constant $c>0$ such that the following holds. For input $\{X_{i}\}$ such that $d_{a}\|\rho_{X}^{(a)}\|_{op} \leq (1+c)$; and perturbation $\delta := \sum_{a} (\delta_{a})_{a}$ with $\|\delta\|_{op} = \sum_{a} \|\delta_{a}\|_{op} \leq c$ giving $\{X'_{i} := e^{\delta} X_{i} \}$, there is a universal constant $C$:
\[ \frac{1}{d_{a}} \|\nabla^{2}_{aa} f(e^{2\delta}) - \nabla^{2}_{aa} f(I)\|_{op} \leq C \|\delta\|_{op} \|\rho_{X}^{(a)}\|_{op}      \]
\end{corollary}
%\CF{technically $f(e^{\delta})$ corresponds to $e^{\delta/2} \vec X$}
\begin{proof}
Recall from \cref{lem:hessian} that $\frac{1}{d_{a}}\langle Y, \nabla^{2}_{aa} f_{X} Y \rangle = \langle \rho_{X}^{(a)}, Y^{2} \rangle - \langle \rho_{X}^{(a)}, Y \rangle^{2}$. We treat the perturbation as the composition of $k$ perturbations; 
\[ X_{0}:=X \to X_{1}:= (e^{\delta_{1}})_1 X_{0} \to ... \to X_{k}:=(e^{\delta_{k}})_{k} X_{k-1} = X'  \]
We bound the first term $\langle \rho^{(a)}, Y^{2} \rangle$. We use $\ref{atoaaRobustness}$ to handle $e^{\delta_{a}}$ and $\ref{btoaaRobustness}$ for the rest:
\begin{align*}
 |\langle \rho_{X'}^{(a)} - \rho_{X}^{(a)}, Y^{2} \rangle| 
 &\leq \sum_{j=1}^{k} |\langle \rho_{X_{j}}^{(a)} - \rho_{X_{j-1}}^{(a)}, Y^{2} \rangle| 
\underset{\ref{atoaaRobustness},\;\ref{btoaaRobustness}}{\leq} \sum_{j=1}^{k}  C\|\delta_{j}\|_{op} \|\rho_{X_{j-1}}^{(a)}\|_{op} \|Y^{2}\|_{1} \\
& \leq \left( \prod_{j=1}^k (1+C\|\delta_{j}\|_{op}) - 1 \right) \|\rho_{X}^{(a)}\|_{op} \|Y\|_{F}^{2} \leq C' \|\delta\|_{op} \|\rho_{X}^{(a)}\|_{op} \|Y\|_{F}^{2}.   \end{align*}
The term in parenthesis is shown by induction, and in the last step we used $\|\delta\|_{op} \leq c$. We next bound the rank-one term $\langle \rho_{X}^{(a)}, Y \rangle^{2}$:
\[ |\langle \rho_{X'}^{(a)}, Y \rangle^{2} - \langle \rho_{X}^{(a)}, Y \rangle^{2}| = |\langle \rho_{X'}^{(a)}, Y \rangle - \langle \rho_{X}^{(a)}, Y \rangle| |\langle \rho_{X'}^{(a)}, Y \rangle + \langle \rho_{X}^{(a)}, Y \rangle|    \]
\[ \leq C' \|\delta\|_{op} (2+C' \|\delta\|_{op}) (\|\rho_{X}^{(a)}\|_{op} \|Y\|_{1})^{2} \leq C'' \|\delta\|_{op}) d_{a} \|\rho_{X}^{(a)}\|_{op}^{2} \|Y\|_{F}^{2}       \]

In toto this gives the corollary:
\[ \frac{1}{d_{a}}\|\nabla^{2}_{aa} f(e^{2\delta}) - \nabla^{2}_{aa} f(I)\|_{op} \leq \|\delta\|_{op}(C' \|\rho_{X}^{(a)}\|_{op} + C'' \|\rho_{X}^{(a)}\|_{op}^{2})    \]
from which the statement follows by the initial condition $\|d_{a} \rho_{X}^{(a)}\|_{op} \leq (1+c)$.
\CF{one can calculate $k$ to be ...}
\end{proof}


\AR{The below definition is for brevity, but you guys can choose the right notation.}
\begin{definition}
For operator $M : \mat(d_{b}) \to \mat(d_{a})$, we let $\|M\|_{0}$ denote the $F \to F$ norm of its restriction to the traceless subspaces $\Sym^0_{d_b} \to \Sym^0_{d_a}$
\end{definition}

\begin{definition}
For input $\rho_{X}$ and fixed $a,b$, define
\[ \langle Y, H_{X}^{ab} Z \rangle := \langle (Y_{a})(Z_{b}), \rho_{X} \rangle = \langle Y \otimes Z , \rho_{X}^{(ab)} \rangle     \]
\end{definition}

\begin{lemma} \label{inftyto2}
$\|H_{X}^{ab}\|_{F \to F}^{2} \leq \|\rho_{X}^{(a)}\|_{op} \|\rho_{X}^{(b)}\|_{op}$
\end{lemma}
\begin{proof}
This was already in KLR and we have two new proofs: one by convexity, and one by Riesz-Thorin. \AR{The proofs are in some other file, we can add it if we like}
\end{proof}

\begin{lemma} \label{btoabRobustness}
There is a constant $c>0$ such that the following holds. For input $\{X_{i}\}$ and perturbation $\|\delta\|_{op} \leq c$ giving $\{X'_{i} := (e^{\delta})_{c} X_{i} \}$ for $c \in \{a,b\}$, there is a universal constant $C$:
\[ \|H_{X'}^{ab} - H_{X}^{ab}\|_{0} \leq C \|\delta\|_{op} \|H_{X}^{ab}\|_{F \to F}    \]
\end{lemma}
\begin{proof}
We will drop the superscript $(ab)$ since it doesn't change. By taking adjoints, we can assume wlog that $c = b$. Let $R : \mat(d_{b}) \to \mat(d_{b})$ be defined as $R(Z) := (1+\eta)^{2} e^{\delta} Z e^{\delta}$ for $\eta$ defined by our normalization $\|(e^{\delta})_{b} X\|_{2}^{-1} =: (1+\eta) \|X\|_{2}^{-1}$. The subspace $\Sym_{d_{b}}^{0}$ is not invariant under $R$, but we show $R \approx I$. Let $\delta' := (1+\eta) e^{\delta} - I$. 
\[ \|R(Z) - Z\|_{F} \leq 2 \|\delta' Z\|_{F} + \|\delta' Z \delta'\|_{F} \leq 3 \|\delta'\|_{op} \|Z\|_{F}    \]
So using the definition of $\|\cdot\|_{0}$:
\[ \sup_{Y \in \Sym_{d_{a}}^{0},Z \in \Sym_{d_{b}}^{0}} \frac{\langle Y, (H_{X'} - H_{X}) Z \rangle}{\|Y\|_{F} \|Z\|_{F}} \leq O(\|\delta\|_{op}) \|H_{X}\|_{F \to F}  \]
In the last step we used $\|\delta'\|_{op} \leq O(\|\delta\|_{op})$.
\end{proof}

\begin{lemma} \label{ctoabRobustness}
There is a constant $c>0$ such that the following holds. For input $\{X_{i}\}$ and perturbation $\|\delta\|_{op} \leq c$ giving $\{X'_{i} := (e^{\delta})_{c} X_{i} \}$ for $c \not\in \{a,b\}$, there is a universal constant $C$:
\[ \|H_{X'}^{ab} - H_{X}^{ab}\|_{0} \leq C \|\delta\|_{op} \|H_{X}^{ab}\|_{F \to F}    \]
\end{lemma}
\begin{proof}
Let $\delta' := (1+\eta) e^{2 \delta} - I_{c}$ for $\eta$ defined by normalization $\|(e^{\delta})_{c} X\|_{2}^{-1} =: (1+\eta) \|X\|_{2}^{-2}$
\[ \langle Y, (H_{X'} - H_{X}) Z \rangle = \langle \rho_{X}^{(abc)}, Y \otimes Z \otimes \delta' \rangle   \]
We will use a similar decomposition to lemma \ref{btoaaRobustness}, so first assume $Y,Z \succeq 0, \|Y\|_{F} = \|Z\|_{F} = 1$:
\[ |\langle \rho_{X}^{(abc)}, Y \otimes Z \otimes \delta' \rangle| \leq \langle \rho_{X}^{(abc)}, Y \otimes Z \otimes |\delta'| \rangle \leq \|\delta'\|_{op} \langle \rho_{X}^{(ab)}, Y \otimes Z \rangle   \]
Here we again used that $\rho_{X}^{(abc)} \succeq 0$. This last term cannot be bounded by $\|H_{X}\|_{0}$ as $Y \succeq 0$; but $\|H_{X}\|_{F \to F}$ is good enough for our purposes. 

To finish the lemma we decompose $Y = Y_{+} - Y_{-}, Z = Z_{+} - Z_{-}$ and bound
\[ |\langle Y, (H_{X'} - H_{X}) Z \rangle| \leq O(\|\delta\|_{op}) \|H_{X}\|_{F \to F} \sum_{s,t \in \{+,-\}} \|Y_{s}\|_{F} \|Z_{t}\|_{F}   \]
These last four terms we can bound by Cauchy Schwarz:
\[ \leq (2\|Y_{+}\|_{F}^{2} + 2\|Y_{-}\|_{F}^{2})^{1/2} (2\|Z_{+}\|_{F}^{2} + 2\|Z_{-}\|_{F}^{2})^{1/2} = 2 \|Y\|_{F} \|Z\|_{F}     \]
Again using the definition of $\|\cdot\|_{0}$ we get the lemma. 
\end{proof}

\begin{corollary} \label{offdiagRobustness}
There is a constant $c>0$ such that the following holds. For input $\{X_{i}\}$ such that $\|d_{a} \rho_{X}^{(a)}\|_{op}, \|d_{b} \rho_{X}^{(b)}\|_{op} \leq (1+c)$; perturbation $\delta := \sum_{a} (\delta_{a})_{a}$ with $\|\delta\|_{op} = \sum_{a} \|\delta_{a}\|_{op} \leq c$ giving $\{X'_{i} := e^{\delta} X_{i} \}$, there is universal constant $C$:
\[ \frac{1}{\sqrt{d_{a} d_{b}}} \|\nabla^{2}_{ab} f(e^{2 \delta}) - \nabla^{2}_{ab} f(I)\|_{op} \leq C \|\delta\|_{op} \sqrt{\|\rho_{X}^{(a)}\|_{op} \|\rho_{X}^{(b)}\|_{op}}     \]
\end{corollary}
\begin{proof}
Recall $\frac{1}{\sqrt{d_{a} d_{b}}} \langle Y, \nabla^{2}_{aa} f_{X} Z \rangle = \langle \rho_{X}^{(ab)}, Y^{2} \rangle - \langle \rho_{X}^{(a)}, Y \rangle^{2}$. Using the same iterative strategy as $\ref{diagRobustness}$ we can show: 
\[ |\langle Y, (\rho_{X'}^{(ab)} - \rho_{X}^{(ab)}) Z \rangle| \leq C' \|\delta\|_{op} \|H_{X}\|_{F \to F} \|Y\|_{F} \|Z\|_{F}    \]
We used $\ref{btoabRobustness}$ for $\{a,b\}$ and $\ref{ctoabRobustness}$ for the rest. 

We can similarly bound the off-diagonal "rank-one" terms:
\[ |\langle \rho_{X'}^{(a)}, Y \rangle \langle \rho_{X'}^{(b)}, Z \rangle - \langle \rho_{X}^{(a)}, Y \rangle \langle \rho_{X}^{(b)}, Z \rangle|    \]
\[ \leq |\langle \rho_{X'}^{(a)} - \rho_{X}^{(a)}, Y \rangle \langle \rho_{X'}^{(b)}, Z \rangle| + |\langle \rho_{X}^{(a)}, Y \rangle \langle \rho_{X'}^{(b)} - \rho_{X}^{(b)}, Z \rangle|   \]
\[ 2 C' \|\delta\|_{op} (1+C'\|\delta\|_{op}) \|\rho_{X}^{(a)}\|_{op} \|\rho_{X}^{(b)}\|_{op} \|Y\|_{1} \|Z\|_{1}    \]
\[ \leq C'' \|\delta\|_{op} (\sqrt{d_{a} d_{b}} \|\rho_{X}^{(a)}\|_{op} \|\rho_{X}^{(b)}\|_{op}) \|Y\|_{F} \|Z\|_{F}    \]

In toto this gives the lemma:
\[ \frac{1}{\sqrt{d_{a} d_{b}}} \|\nabla^{2}_{ab} f(e^{2 \delta}) - \nabla^{2}_{ab} f(I)\|_{op} \leq \|\delta\|_{op}((C' \|H_{X}\|_{F \to F}) + C'' (\sqrt{d_{a} d_{b}} \|\rho_{X}^{(a)}\|_{op} \|\rho_{X}^{(b)}\|_{op})) \]
\[ \leq \|\delta\|_{op} (C' + C'') \sqrt{\|\rho_{X}^{(a)}\|_{op} \|\rho_{X}^{(b)}\|_{op}}     \]
We used $\ref{inftyto2}$ for the first term and balanced marginals for the second. 
\end{proof}

\CF{Akshay: $\sum_{a} \|\delta_{a}\|_{op} = \|\delta\|_{op}.$}
\begin{lemma} \label{convexRobustness}
There is a constant $c>0$ such that the following holds. If $f_{X}$ is $\alpha$-strongly convex at $I$; $\forall a: \|d_{a} \rho_{X}^{(a)}\| \leq (1+c)$, or the stronger condition $\|\sqrt{d_{a}} \nabla_{a} f_{X}\|_{op} \leq c$; and $\sum_{a} \|\delta_{a}\|_{op} \leq c$: then $f$ at $e^{\delta} := \otimes_{a} e^{\delta_{a}}$ is $\geq (\alpha - k C \|\delta\|_{op})$-strongly convex for some universal constant $C$. 
\end{lemma}
\begin{proof}
We follow the same structure as the proof of convexity, letting $\{X'_{i} := e^{\delta} X\}$. Then by $\ref{diagRobustness}$ we have a bound on the diagonal blocks, and by $\ref{offdiagRobustness}$ we have a bound on the off-diagonal blocks. Finally we use the same Schur-complement inequality from $\ref{thm:tensor-convexity}$ for block matrices:
\[ \|\nabla^2 f_{X'} - \nabla^{2} f_{X}\|_{op} \leq C \|\delta\|_{op} \|\{I_{ab}\}_{a,b \in [k]}\|_{op} \leq k C \|\delta\|_{op}  \]
Note that this also gives a spectral upper bound for $\nabla^{2} f_{X'}$. 
\end{proof}



%\CF{I think right now this doesn't use our convention in \cref{dfn:g-convexity} which leads to the Hessian in \cref{lem:hessian} }

\section{Improvements for the matrix normal model}\label{sec:matrix-normal}
We now prove \cref{thm:matrix-normal}, an improvement to \cref{thm:tensor-frobenius} in the case $k=2$. The results for $k = 2$ are stronger in that the MLE can be shown to be close to the truth in operator norm rather than the looser Frobenius norm, and that the failure probability is inverse exponential rather than inverse polynomial.




The proof plan is similar to that in \cref{subsec:outline}, but rather than strong convexity we use the similar but stronger notion of quantum expansion \CF{cite klr, etc}, which allows us to obtain bounds on the distance to the optimizer from bounds on the \emph{operator norm} on the gradient rather than the Frobenius norm.

\begin{definition}[Quantum expansion]
$ $
\begin{enumerate}
\item Let $\Phi:\mat(d_1) \to \mat(d_2)$ be the operator defined by $\Phi(Y) = \tr_{\{1,3\}} ( Y \ot I_{d_2} \ot I_{n}) \vec X \vec X^T$. Equivalently,
$$\Phi(Y) = \sum_{i = 1}^n X_i^T Y X_i.$$
$\Phi$ is known as the \emph{completely positive map} with Kraus operators $X_1, \dots, X_n$.
\item $\Phi$ is said to be a \emph{$(1 - \lambda)$-quantum expander} if the second singular value $\sigma_2(\Phi)$ satisfies the following bound:
$$\sigma_2(\Phi) \leq \frac{(1 - \lambda)}{\sqrt{d_1d_2}} \tr \Phi(I_{d_2}).$$
\item Say $\Phi$ is \emph{$\eps$-doubly balanced} if
\begin{align*}
\|d_2 \Phi(I_{d_1})/\tr \Phi(I_{d_1})  - I_{d_2} \|_{op}& \leq \eps\\
\textrm{and }\|d_1 \Phi^*(I_{d_2})/\tr \Phi(I_{d_1})  - I_{d_1}  \|_{op} & \leq \eps,
\end{align*}
\end{enumerate}
\end{definition}

The rationale for the above definition is that $\Phi$ is doubly balanced when $(I_{d_1}/\sqrt{d_1}, I_{d_2}/\sqrt{d_2})$ is a singular pair for $\Phi$. This will approximately be the case in our setting (in fact, $\eps$-balancedness is none other than the conclusion of \cref{prop:gradient-bound} for $k = 2$), and in this case $\sigma_1(\Phi) =  \tr \Phi(I_{d_1})/\sqrt{d_1 d_2}.$ Thus $(1-\lambda)$ is like a spectral gap.
%$\sigma_1(\Phi) = \langle I_{d_1}/\sqrt{d_1}, \Phi(I_{d_1}/\sqrt{d_2}) \rangle$
Our main tool is the following:


\begin{theorem}[\CF{cite klr, comment about how to assume in SL?}]\label{thm:klr}
If $\Phi$ is an $\eps$-balanced, $(1 - \lambda)$-quantum expander, and $\eps \leq c \lambda^2/\log d_1$, then the maximum likelihood estimator $(\widehat{\Theta}_1, \widehat{\Theta}_2) \in \SL_{d_1}\times \SL_{d_2}$ is unique and satisfies
$$\| \widehat{\Theta}_1 - I_{d_1}\|_{op}, \| \widehat{\Theta}_2 - I_{d_2}\|_{op} \leq \frac{\eps \log d_1}{\lambda}.$$
\end{theorem}
Our main result for this section is that $\Phi$ is with high probability a good quantum expander, which allows us to apply the previous theorem.

\begin{theorem}\label{thm:operator-cheeger}
There are absolute constants $c, C$ such that the following holds for all $d_1 \leq d_2$. Suppose $n \geq C (d_2/d_1) \max\{\log (d_2/d_1),  \eps^{-2}\} $. Then $\Phi$ is $\eps \sqrt{d_2/n d_1}$-balanced and a $1 - c$ quantum expander with failure probability $O(e^{ - \Omega( d_2 \eps^2)}).$
\end{theorem}

Before proving the above result, we use it to prove \cref{thm:matrix-normal}.

\begin{proof}[Proof of \cref{thm:matrix-normal}] As discussed in \cref{subsec:outline}, it is enough to prove \cref{thm:matrix-normal} under the assumption $\Theta_a = I_{d_a}$ for $a \in \{1,2\}$. By \cref{thm:operator-cheeger}, provided $n \geq C (d_2/d_1) \max\{\log (d_2/d_1),  \eps_0^{-2}\} $ with probability $1 - O(e^{ - \Omega( d_2 \eps_0^2)})$ the operator $\Phi$ is $\delta:=\eps_0  \sqrt{\frac{d_2}{n d_1}}$-balanced and is a $(1 - c)$-quantum expander. Condition on this event. By our choice of $n$, we have $\delta \leq \lambda^2/\log d_1$ if we take $\eps_0 = \eps/\log(d_1)$ for $\eps \leq c$. By \cref{thm:klr},
\begin{gather*} \| \widehat{\Theta}_1 - I_{d_1}\|_{op}, \| \widehat{\Theta}_2 - I_{d_2}\|_{op} \leq C \eps_0 \sqrt{\frac{d_2}{n d_1}} \log d_1 = O\left(\eps \sqrt{\frac{d_2}{n d_1}}\right).\end{gather*}
The failure probability becomes $O(e^{ - \Omega( d_2 \eps^2/\log^2 d_1)})$.
\end{proof}




%The results in this section are better in that the covariances are approximated in operator norm rather than the looser Frobenius norm, and secondly that the failure probability is inverse exponential.







\subsection{The Cheeger constant of a random operator}

To prove \cref{thm:operator-cheeger}, we first define the Cheeger constant of an operator $\Phi:\mat(d_1) \to \mat(d_2)$. This is similar to a concept defined in \cite{H07}.
\begin{definition}
Let $\Phi : \mat(d_1) \to \mat(d_2)$ be a completely positive map. The Cheeger constant $\ch(\Phi)$ of the weighted bipartite graph associated to $B$ is given by
$$\ch(\Phi):=\min_{\Pi_1, \Pi_2: \vol(\Pi_1, \Pi_2) \leq \tr \Phi(I)} \phi(\Pi_1,\Pi_2)$$
where $\Pi_1: \C^{d_1} \to \C^{d_1}$ and $\Pi_1: \C^{d_2} \to \C^{d_2}$ are orthogonal projections that are not both zero and the \emph{conductance} $\phi$ of the cut $\Pi_1, \Pi_2$ is defined to be
$$\phi(\Pi_1,\Pi_2) := \frac{\cut(\Pi_1, \Pi_2)}{\vol(\Pi_1,\Pi_2)}$$
where
%$$ \vol(\Pi_1,\Pi_2):= \sum_{i \in T, j \in [d_2]} b_{ij} + \sum_{i \in [d_1], j \in S} b_{ij}\textrm{ and } \cut(S, T):= \sum_{i \not\in T, j  \in S} b_{ij} + \sum_{i \in T, j \not\in S} b_{ij}.$$
$$ \vol(\Pi_1,\Pi_2):=
\tr \Phi(\Pi_1) + \tr \Phi^*(\Pi_2)$$
and $$ \cut(\Pi_1, \Pi_2):= \tr \Pi_2 \Phi(I_{d_1} - \Pi_1) + \tr (I_{d_2} - \Pi_2) \Phi(\Pi_1).$$
\end{definition}

We now cite a slight generalization of \cite{FM20}.
%Recall the function $$f^{\Phi}:X \mapsto \frac{d_1}{d_2} \log\det(\Phi(X)) - \log\det (X).$$

\begin{lemma} [\cite{FM20}, \cite{KLR19}]\label{lem:op-cheeger} There exist absolute constants $c, C$ such if $\eps < c \ch(\Phi)^2$ and $\Phi$ is $\eps$-balanced, then $\Phi$ is a
$$ \max\left\{1/2, 1 -  \ch(\Phi)^2 + C \frac{\eps}{\ch(\Phi)^2} \right\}$$
quantum expander.
\end{lemma}
We proceed to bound the Cheeger constant of a random operator. The Cheeger constant of an operator is scale-invariant, so for convenience we let $\Phi$ have Kraus operators $X_1, \dots, X_n$, each drawn from $\cN(0,  I_{d_1} \ot I_{d_2}).$ Our main observation is the following.

\begin{lemma}\label{fact:chi} Let $\Pi_1:\C^{d_1} \to \C^{d_1}, \Pi_2: \C^{d_2} \to \C^{d_2}$ be orthogonal projections, of rank $r_1, r_2$, respectively. Then $\cut(\Pi_1, \Pi_2), \vol(\Pi_1, \Pi_2), \vol(I_{d_1}, I_{d_2})$ is jointly distributed as
$$ R_1, R_1 + 2R_2, 2R_1 + 2 R_2 + 2R_3$$ where
$R_1, R_2, R_3$ are independent $\chi^2$ random variables with $F_1:=n r_1(d_2 - r_2) + n r_2(d_1-r_1), F_2:= n r_1r_2, F_3:= n(d_1 - r_1)(d_2 - r_2)$ degrees of freedom, respectively.
\end{lemma}
\begin{proof} As the distribution of $\Phi$ is invariant under the action of unitaries, the distribution of $\cut(\Pi_1, \Pi_2), \vol(\Pi_1, \Pi_2)$ depends only on the rank of $\Pi_1, \Pi_2$. Thus we may compute in the case that $\Pi_1, \Pi_2$ are coordinate projections, in which case one may verify the fact straightforwardly.
\end{proof}


 We show a sufficient condition for the Cheeger constant being bounded away from $1$ that is amenable to the previous distributional description.
\begin{lemma}\label{lem:suff}
Let $r_1, r_2$ not both zero be the ranks of projections $\Pi_1: \C^{d_1} \to \C^{d_1}, \Pi_2: \C^{d_2} \to \C^{d_2}$, and let $F_1:= n r_1(d_2 - r_2) + n r_2(d_1-r_1)$ and $F_2:=n r_1r_2.$ If
\begin{itemize}
\item for all $\Pi_1, \Pi_2$ such that $F_2 \geq (4/9) n d_1 d_2$ we have
\begin{gather}\vol(\Pi_1, \Pi_2) \geq (1/2 - \delta) \vol(I_{d_1}, I_{d_2}),\label{eq:vol}\end{gather} and
\item for all $\Pi_1, \Pi_2$ such that $F_2 < (4/9) n d_1 d_2$, we have
\begin{gather} \vol(\Pi_1, \Pi_2) \leq (4/3 + \delta)(F_1 + 2 F_2) \textrm{ and } \cut(\Pi_1, \Pi_2) \geq (2/3 - \delta) F_1,\label{eq:cut} \end{gather}
\end{itemize}
then $\ch(\Phi) \geq 1/6 - O(\delta)$ for $\delta \leq c$.
\end{lemma}
\begin{proof} By the first assumption, it remains to show that $F_1/(F_1 + 2 F_2) \geq 1/3$ provided $F_2 < (4/9) n d_1 d_2$, or $r_1 r_2 < (4/9) d_1 d_2$. Indeed, if either $r_1 = 0$ or $r_2 = 0$, then $F_2 = 0$ and $F_1>0$ and the claim holds, else
\begin{align*}F_1/(F_1 + 2 F_2) &= \frac{r_1 d_2 + r_2 d_1 - 2 r_1 r_2}{r_1 d_2 + r_2 d_1}\\
 &= 1 -2 \sqrt\frac{ r_1 r_2}{d_1 d_2} \frac{1}{ \sqrt{ r_1 d_2/r_2 d_1} + \sqrt{r_2 d_1/ r_1 d_2}} \\
 &\geq 1 - \sqrt{4/9} = 1/3.
\end{align*}

In the last inequality we used that $a + a^{-1} \geq 2$ for all $a \in \R_+$ and that $r_1 r_2 < (4/9) d_1 d_2$. \end{proof}


Next we use this to show that for fixed $\Pi_1, \Pi_2$, with high probability the events in \cref{lem:suff} hold.
\begin{lemma}\label{lem:probabilities}
Let $r_1, r_2$ not both zero be the ranks of projections $\Pi_1: \C^{d_1} \to \C^{d_1}, \Pi_2: \C^{d_2} \to \C^{d_2}$, and let $F_1:= n r_1(d_2 - r_2) + n r_2(d_1-r_1)$ and $F_2 = n r_1 r_2$. Then
\begin{itemize}
\item if $F_2 \geq (4/9) n d_1 d_2$, then \cref{eq:vol} holds with $\delta = 0$ with probability at least $1 - e^{-\Omega( n d_1 d_2)}$.
\item else, \cref{eq:cut} holds with $\delta = 0$ with probability at least $1 - e^{-\Omega( F_1)}$.
\item Finally, $\vol(\Pi_1, \Pi_2) \geq \frac{1}{2}\tr \Phi(I_{d_1}) (d_1/d_2)$ with probability at least $1 - e^{- \Omega(n r_1 d_2 + n r_2 d_1)}$.
\end{itemize}
\end{lemma}


\begin{proof}
Recall from \cref{fact:chi} that, $\cut(\Pi_1, \Pi_2), \vol(\Pi_1, \Pi_2), \vol(I_{d_1}, I_{d_2})$ are jointly distributed as $R_1, R_1 + 2R_2, 2R_1 + 2R_2 + 2R_3$ for $R_1, R_2, R_3$ independent $\chi^2$ random variables with $F_1, F_2, F_3$ degrees of freedom, respectively. Thus it is enough to show that
\begin{itemize}
\item If $nr_1 r_2 \geq (4/9) n d_1 d_2$, then with probability $1 - e^{- \Omega( n d_1 d_2)}$ we have $R_2 > R_3$, and
\item if $nr_1 r_2 \leq (2/3) n d_1 d_2$, then with probability $1 - e^{- \Omega(F_1)}$ we have $R_1 \geq (2/3) F_1$ and $R_1 + 2R_2 \leq (4/3) (F_1 + 2 F_2),$
\item and with probability $1 - e^{- \Omega(F_1 + 2 F_2)}$, $R_1 + 2R_2 \geq (2/3) (F_1 + 2 F_2) = (2/3) n (r_1 d_2 + r_2 d_1)$ and $R_1 + R_2 + R_3 \leq (4/3)(F_1 + F_2 + F_3) = (4/3)n d_1 d_2$.
\end{itemize}
All three follow from standard results for concentration of $\chi^2$ random variables \cite{W19}. To prove the first item, first note that $F_1 + 2 F_2 \geq (4/3)(F_1 + F_2 + F_3)$, because
\begin{align*}
(F_1 + 2 F_2)/( F_1 + F_2 + F_3) &= \frac{r_1}{d_1} + \frac{r_2}{d_2}\\
 &= \sqrt{ \frac{r_1 r_2}{d_1 d_2}}\left( \sqrt{ \frac{r_1 d_2}{r_2 d_1}} + \sqrt{ \frac{r_2 d_1}{r_1 d_2}}\right) \geq (2/3) \cdot 2 \geq 4/3.
\end{align*}
In particular, $F_2 \geq (2/3)(F_2 + F_3)$. Thus, with probability $1 - e^{- c F_2}$, $R_2 \geq (5/9) (F_2 + F_3)$ and $R_2 + R_3 \leq (10/9) (F _2 + F_3),$ so $R_2 > R_3$ with probability $1 - e^{- c F_2} \geq 1 - e^{- c n d_1 d_2}$. The second and third items are straightforward.
\end{proof}

Finally, we show using an epsilon net that the Cheeger constant is large for \emph{all} projections.
\begin{lemma}[\cite{FM20}]\label{lem:net} There is a $\delta$-net $N$ of the rank $r$ orthogonal projections $\Pi: \C^d \to \C^d$ with $|N| = \exp(O(d r |\ln \delta|))$.
\end{lemma}
As a corollary, the number of pairs of projections $\Pi_1, \Pi_2$ of rank $r_1, r_2$ has a $\delta$-net of size on the order of $(r_1 d_1 + r_2 d_2) |\ln \delta|$.

\begin{lemma}[A net suffices]\label{lem:net-suffices}
Suppose $\|\Pi'_1 -\Pi_2\|_F, \|\Pi'_2 - \Pi_2\|_F \leq \delta$. Then
\begin{align*} |\cut(\Pi_1, \Pi_2) - \cut(\Pi'_1, \Pi'_2)| \leq4\delta \tr \Phi(I_{d_1})\\
\textrm{ and }|\vol(\Pi_1, \Pi_2) - \vol(\Pi'_1, \Pi'_2)| \leq 4\delta \tr \Phi(I_{d_1}).
\end{align*}
\end{lemma}
\begin{proof}
We first show the first inequality.
\begin{align*}|\cut(\Pi'_1, \Pi'_2) - \cut(\Pi_1, \Pi_2)| & \leq |\tr \Pi'_2 \Phi(I_{d_1} - \Pi'_1) - \tr \Pi_2 \Phi(I_{d_1} - \Pi_1)|\\
&  + |\tr (I_{d_2} - \Pi'_2) \Phi(\Pi'_2) - \tr (I_{d_2} - \Pi_2) \Phi(\Pi_2)|.
\end{align*}
We begin with the first term.
\begin{align*}&|\tr \Pi'_2 \Phi(I_{d_1} - \Pi'_1) - \tr \Pi_2 \Phi(I_{d_1} - \Pi_1)|\\
&= |\tr (\Pi'_2 - \Pi_2) \Phi(I_{d_1} - \Pi'_1) + \tr \Pi_2 \Phi(\Pi_1 - \Pi'_1)|\\
&\leq \delta\| \Phi(I_{d_1} - \Pi'_1)\|_F + \delta\| \tr \Phi^*(\Pi_2)\|_F\\
& \leq 2 \delta \tr \Phi(I_{d_1}).
\end{align*}
The second term follows by symmetry. The proof of the second inequality is similar.
\end{proof}

\begin{lemma}[Applying union bound]\label{lem:union}
Let $d_1 < d_2$. Suppose $n \geq C \frac{d_2}{d_1} \log (d_2/d_1)$. Then $\ch(\Phi) = \Omega(1)$ with failure probability $O(e^{- \Omega(n d_1)})$.
\end{lemma}
\begin{proof} Let $\delta' \leq c d_1/d_2$. Let $\cN(r_1, r_2)$ be a $\delta'$-net for the pairs of projections of rank $r_1, r_2$, respectively, with $|\cN(r_1, r_2)| = e^{O((d_1r_1 + d_2 r_2) \log(1/\delta'))}$, and $N = \bigcup_{r_1, r_2} \cN(r_1, r_2)$. We claim that it is enough to show that with probability $\exp( - c n d_1 )$, for all $r_1, r_2$ not both zero we have
\begin{enumerate}
\item \cref{eq:vol} holds with $\delta = 0$ for every $\Pi_1,\Pi_2 \in \cN(r_1, r_2)$ when $r_1 r_2 \geq (4/9) d_1 d_2$,
\item  and \cref{eq:cut} holds with $\delta =0$ for all $\Pi_1, \Pi_2 \in \cN(r_1, r_2)$ otherwise.
\item $\vol(\Pi_1, \Pi_2) \geq \tr \Phi(I) (d_1/d_2)$.
\end{enumerate}
Let us check that the hypotheses of \cref{lem:suff} with $\delta \leq c$ are implied by these three items; this will imply that conditioned on the three items we have $\ch(\Phi) \geq \Omega(1)$. Because every pair $(\Pi'_1,\Pi'_2)$ of projections of ranks $r_1,r_2$ is most $\delta$ far from some element $(\Pi_1, \Pi_2)$ of $\cN(r_1,r_2)$, then by \cref{lem:net-suffices} (and the inequality $\vol(\Pi_1, \Pi_2) \geq \tr \Phi(I)(d_1/d_2)$) we have
\begin{align*} (1 - 4 \delta'  \cdot d_2/d_1) \vol(\Pi_1, \Pi_2) \leq  \vol(\Pi_1', \Pi_2') \leq  (1 + 4 \delta'  \cdot d_2/d_1) \vol(\Pi_1, \Pi_2).\end{align*}
By assumption, $4 \delta' \cdot d_2/d_1 \leq c$. This shows \cref{eq:vol} holds with $\delta \leq c$ when $r_1 r_2 \geq (4/9) d_1 d_2$. It remains to show that \cref{eq:cut} holds otherwise. Firstly, when $r_1 r_2 < (4/9) d_1 d_2$ we have
\begin{gather} \vol(\Pi_1', \Pi_2') \leq (1 + c) \vol(\Pi_1, \Pi_2) \leq  (1 + c)(4/3)(F_1 + 2 F_2).\label{eq:not-net-9a}\end{gather}
  Next, observe that
$$  \cut(\Pi_1', \Pi_2') \geq \cut(\Pi_1, \Pi_2) - c \vol(\Pi_1, \Pi_2).$$
In the proof of \cref{lem:suff} it is shown that if $r_1 r_2 < (4/9) d_1 d_2$ then $F_1 \geq \frac{1}{3} (F_1 + 2 F_2)$, in which case
\begin{align}
\cut(\Pi_1', \Pi_2') &\geq \cut(\Pi_1, \Pi_2) - c \vol(\Pi_1, \Pi_2) \geq \nonumber\\
& \geq (2/3) F_1 -  c (4/3)(F_1 + 2 F_2) \geq (2/3 - c) F_1.\label{eq:not-net-9b}
\end{align}

Taken together, \cref{eq:not-net-9a,eq:not-net-9b} show that \cref{eq:cut} holds when $r_1 r_2 < (4/9) d_1 d_2$.

We must next show that the three conditions hold with the desired probability. We show that for fixed $r_1, r_2$, each item holds with probability at least $1 - e^{n (r_1 d_2 + r_2 d_1)}$. The sum of $e^{-\Omega(n (r_1d_2 + r_2 d_1))}$ over all $0 \leq r_1 \leq d_1, 0 \leq r_2 \leq d_2$ apart from $r_1 = r_2 = 0$ is $O(e^{- \Omega( n d_1)})$, so the conditions hold for all $r_1, r_2$ with the desired probability. Note that by our choice of $n$ we have $(d_1r_1 + d_2 r_2) \log(1/\delta') \leq c n (r_1d_2 + r_2 d_1)$ for $r_1, r_2$ not both zero.

We first bound the failure probability for the first item. By \cref{lem:probabilities}, if $r_1 r_2 \geq (4/9) d_1 d_2$ then \cref{eq:vol} holds for every $\Pi \in \cN(r_1, r_2)$ with probability
\begin{align*}
1 - |\cN(r_1, r_2)|e^{- \Omega( n d_1 d_2) } &= 1 - |\cN(r_1, r_2)| e^{ - \Omega(n (r_2d_1 + r_1d_2))}\\
&= 1 - e^{ - \Omega(n (r_2d_1 + r_1d_2))}.
\end{align*}

Next we bound the probability for the second item. By \cref{lem:probabilities}, \cref{eq:cut} holds for fixed $\Pi \in \cN(r_1, r_2)$ with probability $1 - e^{-\Omega( F_1)}$, but as in the proof of \cref{lem:suff} we have $F_1 \geq \frac{1}{3} (F_1 + 2 F_2)$ when $r_1 r_2 < (4/9) d_1 d_2$, so $F_1 = \Omega(n (r_1d_2 + r_2 d_1))$. Now, by the union bound and the lower bound on $n$, \cref{eq:cut} holds for every element of $\cN(r_1, r_2)$ with probability $1 - |\cN(r_1,r_2)| e^{-\Omega(n (r_1d_2 + r_2 d_1)} = 1 - e^{-\Omega(n (r_1d_2 + r_2 d_1)}$.


The third item holds with probability $1 - e^{-\Omega(n (r_1d_2 + r_2 d_1))}$ by \cref{lem:probabilities}, so by a similar application of the union bound and our choice of $n$ it holds for all elements of $\cN(r_1, r_2)$ with probability $1 - e^{-\Omega(n (r_1d_2 + r_2 d_1))}$. \end{proof}

\begin{proof}[Proof of \cref{thm:operator-cheeger}]
To prove \cref{thm:operator-cheeger}, we apply \cref{lem:op-cheeger} using \cref{prop:gradient-bound} to bound the balancedness of $\Phi$ and \cref{lem:union} to bound $\ch(\Phi)$. Indeed, $\|\nabla_a f\|_{op} \leq \eps_0$ for $a \in \{1,2\}$ if and only if $\Phi$ is $\eps_0$-balanced, so by \cref{prop:gradient-bound} the operator $\Phi$ is $\eps_0$-balanced with probability $1 -  e^{-\Omega(n d_1 \eps_0^2)} - e^{-\Omega(n d_2 \eps_0^2)} \geq 1 - 2e^{-\Omega(n d_1 \eps_0^2)}$ provided $n \geq C\eps_0^{-2} d_2/d_1 $. Setting $\eps_0 = \eps \sqrt{\frac{d_2 }{n d_1}}$ proves the balancedness claim. For the expansion, \cref{lem:union} shows $\ch(\Phi) = \Omega(1)$ with failure probability $O(e^{- \Omega(n d_1)}) = O(e^{- \Omega(d_2 \eps^2)})$. By \cref{lem:op-cheeger}, $\Phi$ is a $1 - c$-quantum expander.
\end{proof}


\section{Convergence of flip-flop algorithms}

In this section we prove that the flip-flop algorithms for the matrix and tensor normal models converge quickly to the MLE estimator with high probability. We begin by stating the flip-flop algorithm and then... 

\begin{Algorithm}
\textbf{Input}: Samples $\vec{X} = (X_1, \ldots, X_n)$ where $X_i \in \R^D$ is sampled from a (unknown) centered normal distribution $\cN(0, \Sigma)$, where each entry of $X_i$ is encoded in binary, with bit size $\le b$. Approximation parameter $\eps > 0$, given in binary representation. \\[.3ex]

\textbf{Output}: $\underline{\Sigma} \in \PD_D$ such that $d_F(\underline{\Sigma}, \hat{\Sigma}) < \eps$, where $\hat{\Sigma}$ is the MLE for $\Sigma$. \\[.3ex]

\textbf{Algorithm}:
\begin{enumerate}
\item\label{it:flip-flop step 1} Set $\Theta_a = I_a$ for each $a \in [k]$.
\item\label{it:flip-flop step 2} For $t=1,\dots,T = \poly(D, b) \cdot \log(1/\eps)$, repeat the following:
\begin{itemize}
\item Compute each component of the gradient $\nabla_a := \nabla_a f_{\vec X}(\Theta_1, \ldots, \Theta_k)$ of $\nabla f_{\vec X}(\Theta_1, \ldots, \Theta_k)$ and find the index $a \in [k]$ for which $\norm{\nabla_a}_F$ is largest. 
\item
If $\norm{\nabla_a}_F < \eps/k$, output $\left( \bigotimes_{a =1}^k \Theta_a \right)^{-1}$ and return.
% S\Bigl(\frac {I_{n_{k^{t}}}{n_{k^{t}}}} \big\Vert \rho^{t}_{k^{t}}\Bigr)$ is largest.
\item Otherwise, set $\Theta_a \leftarrow \det(\rho^{(a)})^{1/d_a} (\rho^{(a)})^{-1} \cdot \Theta_a$.
\end{itemize}
\end{enumerate}
\caption{Flip-flop algorithm}\label{alg:flip-flop}
\end{Algorithm}

\TODO{General descent-lemma + strong convexity folklore}
\subsection{Tensor flip-flop convergence}
\TODO{Proof of \cref{thm:tensor-flipflop}; vanilla strong convexity in a sublevel set stuff.}
\subsection{Matrix flip-flop convergence}
\TODO{Proof of \cref{thm:matrix-flipflop}}
\CF{prove that flip-flop works as soon as estimation in operator norm works. This goes by using KLR to show strong convexity holds in a ball about the optimizer}.






\section{Noise}
\TODO{make sure things work under some error in the data; somewhat optional}


\section{Complex tensors}
\TODO{explain how to generalize to complex tensor models, maybe some quantum motivation?!}

\section{Open problems}
\TODO{$d_{op}$} for tensors??








\appendix


%One can calculate $$ H_{I, v}(X, X) = \langle w, \Pi(X)^2 w \rangle - \langle w, \Pi(X) w \rangle^2 $$ where $w = v/\|v\|$. We may calculate the Hessian $H_{P,v}$ using $H_{P, v} = H_{I, \sqrt{P}v}$.


\section{Operator and tensor scaling}\label{sec:scaling}
\TODO{lower bound Hessian for operators and tensors for all different formats; we hope to get strong convexity with $\prod d_i /(d_1^2 + \dots + d_k^2)$ samples. I am concerned that a KLR19-style operator norm type theorem is needed to get $\tilde{O}$ of this, but we will do what we can with the Frobenius bounds for now; I'd expect to need at least $\max_i \sqrt{d_i}$ too many samples.}\\
\TODO{It would also be nice to have that tight example for the $\log$ in KLR19...}

We recall the moment map and Hessian calculations
\[ \partial_{t=0} f(e^{tX_{a}}) = \langle \nabla_{a}, X \rangle = \langle Q^{a} - sI_{a}, X \rangle   \]
\[ \partial_{t=0}^{2} f(e^{tX_{a}}) = \langle X, (\nabla^{2})_{aa} X \rangle = \langle Q^{a}, X^{2} \rangle  \]
\[ \partial_{s=0} \partial_{t=0} f(e^{tX_{a}} \otimes e^{sY_{b}}) = \langle Y, (\nabla^{2})_{ab} X \rangle = \langle Q^{ab}, X \otimes Y \rangle   \]

\section{Operator Scaling}
In this section we have $n$ samples of $X \sim \cN(0,\frac{1}{n} (\frac{1}{d_{1}} I_{1}) \otimes (\frac{1}{d_{2}} I_{2}))$. We will denote $D := d_{1} d_{2}$. In order to use the KLR analysis, we will show that the one-body marginals have low error in $\|\cdot\|_{op}$ and the whole operator is a sufficient expander at the start.

\subsection{Bernstein Proof of $\|\mu\|_{op}$}
This is proven using matrix concentration

\begin{theorem} [Bernstein]
Consider independent $\{X_{k}\}$ such that $\E X_{k} = 0$ and $\lambda_{max}(X_{k}) \leq R$ almost surely. Further let the variance be $\sigma^{2} := \|\sum_{k} \E X_{k}^{2} \|_{op}$.
\begin{eqnarray*} \Pr [ \lambda_{max} \left( \sum_{k} X_{k}  \right) \geq t ] & \leq & d \exp\left( - \frac{\Omega(t^{2})}{\sigma^{2} + t R} \right)
\\ & \leq & \begin{cases}
d \exp ( - \Omega(t^{2}/\sigma^{2}) ) & \text{if $t \leq \sigma^{2}/R$ }
\\ d \exp ( - \Omega(t/R) )           & \text{if $t \geq \sigma^{2}/R$}
\end{cases}
\end{eqnarray*}
\end{theorem}

In our setting, $Q^{a}$ is comprised of $N := \frac{TD}{d_{a}}$ copies of a rank one $g g^{*}$ where each gaussian is $g \sim \mathcal{N}(0, N^{-1} \frac{1}{d_{a}} I_{a} ) = \mathcal{N}(0, \frac{1}{TD} I_{a}) $. We will drop subscripts for $d_{a}, I_{a}$ etc when they can be understood from context. Therefore we define $X := g g^{*} - \frac{1}{TD} I_{a}$ and note the following parameters:
\[ \lambda_{max}(X) = \|g\|_{2}^{2} - \frac{1}{TD} \hspace{10mm} \lambda_{min}(X) = - \frac{1}{TD}   \]
While $\|g\|_{2}$ is unbounded, we can threshold our distribution with a small loss in probability. Since we will be using $\chi^{2}$ distributions much from now on, we will do a quick exercise to prove our threshold bounds:

\begin{definition}
$\chi(\mu,d)$ denotes the $\chi^{2}$ distribution with mean $\mu$ and $d$ degrees of freedom. Explicitly $X \sim \chi(\mu,d) \implies X = \frac{\mu}{d} \sum_{i=1}^{d} g_{i}^{2}$
where $g \sim \mathcal{N}(0,1)$.
\end{definition}

\begin{lemma}
For $X \sim \chi(\mu,d)$ we have the following (explicit and approximate) formula for the MGF, $\forall \theta < \left(O(\frac{\mu}{d}) \right)^{-1}$:
\begin{eqnarray*} \log \E \exp(\theta X) & = & - \frac{d}{2} \log \left(1 - 2 \theta \frac{\mu}{d} \right)
\\ & \leq & \theta \mu + \theta^{2} \frac{O(\mu^{2})}{2 d}
\end{eqnarray*}
\end{lemma}

\begin{theorem} [Sub-exp variables]
The above MGF bound gives tail decay:
\[ \forall \theta < b^{-1}: \log \E \exp(\theta (X - \E X)) \leq \theta^{2} \frac{\sigma^{2}}{2} \]
\[ \implies  \Pr[X - \mu \geq t] \leq \begin{cases}
\exp( - \Omega(t^{2}/\sigma^{2}) ) & t \leq \sigma^{2}/b
\\ \exp( - \Omega(t/b) ) & t \geq \sigma^{2}/b
\end{cases}   \]
\end{theorem}

With these bounds in mind, note our variables $\|g\|_{2}^{2} \sim \chi(\frac{d_{a}}{TD},d_{a})$ so we have $\sigma^{2} = \frac{d}{(TD)^{2}}, b = \frac{1}{TD} \implies \sigma^{2}/b = \frac{d}{TD}$
\[ \Pr[ \exists k: \lambda_{max}(X_{k}) \geq M \sqrt{\log N}\frac{d}{TD} ] \leq \exp( - \Omega(M^{2}) )  \]
If we're happy with $1/poly$ failure probability we will take $M^{2} \sim \log D$, so in our matrix bound $R_{max} \leq \frac{d \log D}{TD}$

\[ \E X^{2} = \E (g g^{*})^{2} - \frac{1}{(TD)^{2}} I = \E \|g\|_{2}^{4} \hat{g} \hat{g}^{*} - \frac{1}{(TD)^{2}} \]
\[ = \frac{1}{(TD)^{2}} ( (3d + d(d-1)) \frac{1}{d} I - I ) = \frac{d+1}{(TD)^{2}} I   \]
Here $\hat{g} := g / \|g\|_{2}$ and the calculation is done by independence of $\|g\|_{2}, \hat{g}$. So we also have the variance parameter
\[ \sigma^{2} = N \|\E X^{2}\|_{op} = \frac{TD}{d} \frac{d+1}{(TD)^{2}} \sim \frac{1}{TD}  \]

\begin{corollary}
We have the following operator norm concentration
\[ \Pr[ \|Q^{a} - sI_{a}\|_{op} \geq t ] \leq d \exp \left( - \frac{\Omega(t^{2} TD)}{1 + t d_{a} \log D }  \right)  \]
Since we require $\|\cdot\|_{op}$ error $\ll \frac{1}{d_{a} \log D}$, if we are happy with $1/poly$ failure probability we require $TD \gg \max_{a} d_{a}^{2} \log^{3} D$.
\end{corollary}

\begin{remark} Note I'm using $\min_{a} d_{a} < \max_{a} d_{a} < D$ in a couple places so the $\log$ term may be slightly sharpened. But the exponent is tight as we require $TD > \max_{a} d_{a}^{2}$ samples for existence/ uniqueness of the solution.
\end{remark}


\subsection{Gaussian proof of $\|\mu\|_{op}$}
The above method of first thresholding the gaussians then using Bernstein-style concentration on a bounded random matrix feels a bit square-peg round-hole - y. Turns out there are better results specifically for the case of gaussian matrices. Recall again that in our setting $Q^{a}$ is the sum of $N := \frac{nD}{d_{a}}$ copies of $X X^{*}$ where $X \sim \mathcal{N}(0,\frac{1}{nD} I_{a})$. Note first the following fact which allows us to use these specialized inequalities

\begin{fact}
$\sum_{i=1}^{N} X_{i} X_{i}^{*} \equiv G G^{*} $ where $G := \{X_{1}, ..., X_{N}\}$.
This means if we denote $\{\lambda_{1}, ..., \lambda_{d}\}$ the spectrum of $\sum_{i=1}^{N} X_{i} X_{i}^{*}$, this is the same as $\{s_{1}^{2}, ..., s_{d}^{2}\}$ where $s_{j} := s_{j}(G)$ the $j$-th singular value. By Taylor expansion of $\sqrt{1+x}$ we have:
\[ \lambda_{1},\lambda_{d}(GG^{*}) \in \frac{1}{d_{a}} \left( 1 \pm \frac{1}{\log d_{a}} \right) \iff s_{1},s_{d}(G) \in  \frac{1}{\sqrt{d_{a}}} \left( 1 \pm \frac{1}{\log d_{a}} \right)  \]
\end{fact}

% https://arxiv.org/pdf/1011.3027.pdf

\begin{corollary} [Corollary 5.35]%\label{cor:vershynin}
Let $G_{d,N} \in \R^{d \times N}$ for $d < N$ have independent standard gaussian entries. Then for $t \geq 0$, the following occurs with $\leq 2 \exp(-t^{2}/2)$ failure probability:
\[ \sqrt{N} - \sqrt{d} - t \leq s_{d}(G) \leq s_{1}(G) \leq \sqrt{N} + \sqrt{d} + t  \]
\end{corollary}

\begin{corollary}
If $nD \gtrsim d_{a}^{2} \log^{2} d_{a}$ then $\|Q^{a} - \frac{1}{d_{a}} I_{a} \|_{op} \ll \frac{1}{d_{a} \log d_{a}}$ with failure probability $\leq \exp( - \Omega(d_{a}))$
\end{corollary}
\begin{proof}
We have the following with $\leq 2 \exp(-t^{2}/2)$ failure probability:
\[ s_{1},s_{d}\left( \frac{1}{\sqrt{nD}} G_{d,N} \right) \in \frac{1}{\sqrt{nD}} \left( \sqrt{\frac{nD}{d_{a}}} \pm (\sqrt{d_{a}} + t) \right) = \frac{1}{\sqrt{d_{a}}}\left( 1 \pm \frac{d_{a} + t \sqrt{d_{a}}}{\sqrt{nD}}  \right)  \]
Choosing $t \sim \sqrt{d_{a}}$ and $nD \gtrsim d_{a}^{2} \log^{2} d_{a}$ gives the required bound.
\end{proof}

\subsection{Pisier's proof of expansion}
We have shown above that the diagonal blocks $\nabla^{2}_{aa} f \approx I$ \CF{ I got rid of the $1/{d_a}$ factor to reflect \cref{lem:hessian}, and changed $I_{a}$ to $I$ because it's an $I$ on matrices not $\CC^{d_a}$, we should figure out how to denote it properly at some point}. Therefore to show strong convexity we would like to bound the off-diagonal blocks
\[ \forall X \perp I_{a},Y \perp I_{b}:  \langle \nabla^{2}_{ab} f, X \otimes Y \rangle \leq \lambda \frac{\|X\|_{F} \|Y\|_{F}}{\sqrt{d_{a} d_{b}}}   \]
\CF{based on new convention (see \cref{dfn:g-convexity,lem:hessian}) we may want to omit the denominator $\sqrt{d_{a} d_{b}}$ now.} \AR{I think it makes sense to implement our new notational convention in the corollary we use, but this proof is more clear if it's about standard gaussian imo} Pisier's method of proof uses the trace method along with Gaussian concentration in Banach spaces.

\begin{theorem}
We denote a random gaussian matrix $Y \in \R^{m \times n}$ (for $m < n$) with $Y_{ij} \sim \mathcal{N}(0,1)$, we have the following bound
\[ \E \|Y\|_{op} \leq \sqrt{m} + \sqrt{n} \]
\end{theorem}

%

\begin{theorem} [Pisier]
Let $Y$ be a standard gaussian in \CF{real, finite dimensional?} \AR{Theorem 1.5 in https://link.springer.com/chapter/10.1007/BFb0076302 gives the only constraint that the Banach space is separable} Banach space with norm $\|\cdot\|$. Then $\|Y\|$ is subgaussian with parameter $\sigma^{2} := \sup \{ \E \langle Y, \xi \rangle^{2} \mid \|\xi\|_{*} \leq 1 \}; $ that is
\[ \Pr [ | \|Y\| - \E \|Y\| | \geq t ] \leq \exp \left( - \frac{\Omega(t^{2})}{\sigma^{2}} \right).   \]
\end{theorem}

\begin{corollary}\CF{is there a constraint on $p$?} \AR{Added $p \geq 2$ constraint}
\[ \forall p \geq 2: (\E \|Y\|^{p})^{\frac{1}{p}} \leq \E \|Y\| + O \left( \sqrt{\frac{p}{\sigma^{2}}} \right)   \]
\end{corollary}

\begin{theorem}[Non-commutative H\"older inequality]\label{thm:holder}
For $p = 2^{m}$, we have the following (Holder's type) inequality
\[ |\tr[\prod_{i=1}^{p} A_{i}]| \leq \prod_{i=1}^{p} \|A_{i}\|_{p}   \]
\end{theorem}
\begin{proof}
\AR{Tao notes} Done by induction. Note if $A$ are Hermitian, there is a simple tensor trick which allows us to prove the above for all $p \in \mathbb{N}$.
\end{proof}

% Appendix - \url{https://arxiv.org/abs/1209.2059}
% Theorem 16.6 - \url{https://arxiv.org/pdf/1101.4195.pdf}
\CF{$P$ already is used to mean a subset of positive-definite matrices. Maybe just switch to $\Pi$, a projection to traceless matrices.} \CF{at some point include a sentence about the setting of $\alpha_i$, maybe in the main body}\CF{choose different letter for $n$}
\begin{theorem}
Let $\Pi: \mat(m) \to \mat(m)$ denote the projection onto the span of $I_m$, and $\{Y_{i}\}$ are iid standard gaussians in $\mat(n,m)$ \CF{ I changed it to $\mat$ because this needs to go $\mat(m) \to \mat(n)$ if I understand correctly}. There are constants $c,C > 0$ such that for all $m \leq n$ we have 
\[ \|\sum_{i=1}^{N} \alpha_{i} Y_{i} \otimes Y_{i} (I - \Pi) \|_{op} \leq C t^{2} \|\alpha\|_{2} \left( \E \|Y\|_{op} \right)^{2} \]
with probability at least $ 1- t^{-c(m+n)}$. 
\end{theorem}
\begin{proof}
We first begin by a standard symmetrization trick to linearize: if $Y$ is a standard Gaussian on $\mat(n,m)$, then by an entrywise calculation, 
\[ \E Y \otimes Y (I - \Pi) = vec(I_{n}) vec(I_{m})^{*} (I - \Pi) = 0  \]
%then $\E Y \otimes Y (I-\Pi) = 0$ because for any $X \in \mat(m)$ we have
%$$ \E Y \otimes Y (I-\Pi)X = \E (Y \ot Y) X - \frac{1}{m} (\tr X) \E (Y \ot Y)  I_m,$$ which is easily calculated to be zero.
%\CF{I think $m$ and $n$ are switched below}
%\[\E Y^{*} I_{m} Y = m I_{n} \implies \E Y \otimes Y (I-P) = vec(I_{n}) vec(I_{m})^{*} (I-\Pi) = 0\]
Therefore we can add the like term with new iid variables $\{Z_{i}\}$: using Jensen's inequality and the convexity of the operator norm, we have
\[ \E_{Y} \|\sum_{i=1}^{N} \alpha_{i} Y_{i} \otimes Y_{i} (I - \Pi)\|_{op} \leq \E_{Y,Z} \|\sum_{i=1}^{N} \alpha_{i} Y_{i} \otimes Y_{i} (I - \Pi) - \sum_{i=1}^{N} \alpha_{i} Z_{i} \otimes Z_{i} (I - \Pi) \|_{op}  \] Now note $(Y,Z)$ has the same distribution as $(\frac{Y+Z}{\sqrt{2}},\frac{Y-Z}{\sqrt{2}})$, so the right-hand-side of the above equation is
\[\frac{1}{2}\E_{Y,Z} \|\sum_{i=1}^{N} \alpha_{i} (Y_{i}+Z_{i}) \otimes (Y_{i}+Z_{i}) (I - \Pi) - \sum_{i=1}^{N} \alpha_{i} (Y_{i}-Z_{i}) \otimes (Y_{i}-Z_{i}) (I - \Pi) \|_{op}  \]
\[ = \E_{Y,Z} \|\sum_{i} \alpha_{i} Y_{i} \otimes Z_{i} (I-\Pi) + \alpha_{i} Z_{i} \otimes Y_{i} (I-\Pi)\|_{op}  \]
\[ \leq 2 \E_{Y,Z} \|\sum_{i} \alpha_{i} Y_{i} \otimes Z_{i}\|_{op}.   \]
Note we've lost the projection, but the left and right operators are independent. The operator norm is not continuous, but is well-approximated by the Schatten $p$-norm for a high enough $p$; and we can control these norms using concentration of moments of gaussians.  
\CF{quick blurb about what the trace method is to motivate calculation, basically what's done at the end of the proof. } \AR{Ok with this blurb?}
\begin{align*} \E \|\sum_{i} \alpha_{i} Y_{i} \otimes Z_{i}\|_{2p}^{2p} & = \E \tr [ \left( \sum_{ij} \alpha_{i} \alpha_{j} Y_{i}^{*} Y_{j} \otimes Z_{i}^{*} Z_{j} \right)^{p} ]  \\
& = \sum_{\vec{i},\vec{j} \in [N]^{p}} \alpha^{\vec{i}} \alpha^{\vec{j}} \E_{Y,Z} \tr [ Y_{i_{1}}^{*} Y_{j_{1}} ... Y_{i_{p}}^{*} Y_{j_{p}} \otimes Z_{i_{1}}^{*} Z_{j_{1}} ... Z_{i_{p}}^{*} Z_{j_{p}}  ]   \\
& = \sum_{\vec{i},\vec{j}} \alpha^{\vec{i}} \alpha^{\vec{j}} (\E_{Y} \tr [ Y_{i_{1}}^{*} Y_{j_{1}} ... Y_{i_{p}}^{*} Y_{j_{p}} ] ) (\E_{Z} \tr [ Z_{i_{1}}^{*} Z_{j_{1}} ... Z_{i_{p}}^{*} Z_{j_{p}} ] )  
\end{align*}
Here we used independence of $(Y,Z)$. We eventually want to charge to $\E \|\sum_{i} \alpha_{i} Y_{i}\|_{op}$. First note that expectations of gaussian monomials are non-zero and positive iff the polynomial is even. Therefore the coefficient $\alpha^{\vec{i}} \alpha^{\vec{j}}$ from all non-vanishing terms is a square, and in particular positive. So we can upper bound each term individually by the nc-Holder inequality (\cref{thm:holder}) to find: 
\[\E \|\sum_{i} \alpha_{i} Y_{i} \otimes Z_{i}\|_{2p}^{2p} \leq \sum_{\vec{i},\vec{j}} \alpha^{\vec{i}} \alpha^{\vec{j}} (\E_{Y} \tr [ Y_{i_{1}}^{*} Y_{j_{1}} ... Y_{i_{p}}^{*} Y_{j_{p}} ] ) ( \E_{Z} \|Z_{i_{1}}^{*}\|_{2p} ... \|Z_{j_{p}}\|_{2p} )  \]

We now consider the term $\E_{Z} \|Z_{i_{1}}^{*}\|_{2p} ... \|Z_{j_{p}}\|_{2p}$. Different indeces are independent, and for a repeated index we use Jensen's: 
\[ \forall k \leq 2p: \E \|Z_j\|_{2p}^{k} = \E (\|Z_j\|_{2p}^{2p})^{k/{2p}} \leq  ( \E \|Z_j\|_{2p}^{2p} )^{k/2p}  \] 
Thus, we can collect like terms:

%\begin{claim}
%For iid $\{Z_{i}\}$ and $\sum_{i} q_{i} = 2p$:
%\[ \E \|Z_{1}\|_{2p}^{q_{1}} ... \|Z_{k}\|_{2p}^{q_{k}} \leq \prod_{i} (\E \|Z_{i}\|_{2p}^{q_{i} \cdot 2p/q_{i}} )^{q_{i}/2p} = \E \|Z\|_{2p}^{2p}    \]
%\end{claim}

\[ \E \|\sum_{i} \alpha_{i} Y_{i} \otimes Z_{i}\|_{2p}^{2p}
\leq \left( \E_{Z} \|Z\|_{2p}^{2p} \right) \left( \sum_{\vec{i},\vec{j}} \alpha^{\vec{i}} \alpha^{\vec{j}} (\E_{Y} \tr [ Y_{i_{1}}^{*} Y_{j_{1}} ... Y_{i_{p}}^{*} Y_{j_{p}} ] ) \right) \]
\[ = \left( \E_{Z} \|Z\|_{2p}^{2p} \right) \left( \E_{Y} \left\| \sum_{i} \alpha_{i} Y_{i} \right\|_{2p}^{2p} \right)
= \left( \E_{Z} \|Z\|_{2p}^{2p} \right) \left( \sum_{i} \alpha_{i}^{2} \right)^{p} \left( \E_{Y} \|Y\|_{2p}^{2p} \right)    \]
In the last step we used unitary invariance of the joint $\{Y_{i}\}$ distribution, i.e. that $\sum_{i} c_{i} Y_{i}$ has the same distribution as $\|c\|_{2} Y_{1}$.
Finally we can bound $\|\cdot\|_{op}$ by the trace method. Assume wlog (by homogeneity) that $\|\alpha\|_{2} = 1$:
\[ \E_{Y,Z} \|\sum_{i} \alpha_{i} Y_{i} \otimes Z_{i} \|_{op}^{2p} \leq \E_{Y,Z} \|\sum_{i} \alpha_{i} Y_{i} \otimes Z_{i} \|_{2p}^{2p} \]
\[ \leq ( \E \|Y\|_{2p}^{2p} )^{2} \leq ( m \E \|Y\|_{op}^{2p} )^{2} \leq m^{2} \left(\E \|Y\|_{op} + O(\sqrt{\frac{2p}{\sigma^{2}}})  \right)^{4p} \]
Here we've used that $\|Y\|_{op}$ is subgaussian with parameter 
\[ \sigma^{2} = \sup_{\xi} \frac{ \E \langle Y, \xi \rangle^{2} }{\|\xi\|_{1}^{2}} = \E Y_{11}^{2} \sup_{\xi} \frac{ \|\xi\|_{F}^{2} }{\|\xi\|_{1}^{2} } = 1     \]
So again assuming $\|\alpha\|_{2} = 1$, we can apply Markov's to get the bound: \CF{define probabilty macro}
\[ \Pr[\|\sum_{i} \alpha_{i} Y_{i} \otimes Y_{i}\|_{op} \geq (2t \E \|Y\|_{op})^{2} ] \leq    \frac{m^{2} (\E\|Y\|_{op} + C \sqrt{p} )^{4p}}{(2t \E\|Y\|_{op})^{4p}}    \]
Now we choose $C\sqrt{p} \leq \E \|Y\|_{op} = \sqrt{m} + \sqrt{n}$, i.e. $p = \left( \frac{\sqrt{m} + \sqrt{n}}{C} \right)^{2}$: 
\[ \leq \frac{m^{2} (2\E \|Y\|_{op})^{4p}}{(2t \E \|Y\|_{op})^{4p}} \leq \frac{m^{2}}{\exp(\frac{4 \log t}{C^{2}} (\sqrt{m} + \sqrt{n})^{2})}    \]
The statement follows by $m \leq n$. 
\end{proof}

\AR{I may have made a calculation error but it seems we could even pick $c \sim \frac{n^{1/3}}{\log mn}$ so that we get $\exp(-\Omega(\frac{n^{1/3}}{\log n}))$ failure probability?} \CF{ I agree - let's implement it!}

\begin{corollary}
For $Y \in \R^{d_{a} \times d_{b}}$ where $Y_{ij} \sim \mathcal{N}(0,1)$, our $n$ samples give Kraus operators of the form $\frac{1}{\sqrt{nD}} Y$, and our off-diagonal operator is
\[ \sum_{i=1}^{nD/d_{a}d_{b}} \frac{1}{nD} Y_{i} \otimes Y_{i}   \]
The norm $\|\cdot\|_{0}$ on its restriction to $\Sym_{d_{b}}^{0} \to \Sym_{d_{a}}^{0}$ is less than the quantity in the theorem, which is on the order $\frac{d_{a}+d_{b}}{\sqrt{d_{a} d_{b} nD}}$ whp, so for constant expansion it suffices that $nD \gg \max_{a} d_{a}^{2}$
\end{corollary}


\section{Some old}

\begin{lemma}\label{lem:perturbation-old}
If $f$ is $\lambda$-strongly convex at $I$ and $\forall a: d_{a} \|(\nabla f)_{a}\|_{op} \leq \eps \ll 1/k$, then for $Z$ such that $\forall a: \|Z_{a}\|_{op} \leq \delta_{a} \ll 1/k$, the function $f$ at $e^{Z}$ is $\lambda - O(k \sum_{a} \delta_{a})$ strongly convex.
\end{lemma}

\begin{lemma}\label{lem:perturbation} \CF{what I think this lemma should say} Let $f = f_{\vec X}$. There is a constant $c > 0$ such that if $f$ is $\lambda$-strongly convex at $I$ and that $\|(\nabla f)_{a}\|_{op} \leq \eps \leq c k ^{-1}$ for all $ a \in [k]$, then the function $f$ is 
$$ \lambda - O(k \sum \|Z_a\|_{op})$$
-strongly convex at $Z \in P$ provided $\|Z_a\|_{op} \leq c k^{-1}$ for all $a \in [k]$. \CF{just define operator norm on $P$?}

\end{lemma}




The bulk of the work goes towards an intermediate lemma showing that each block $\nabla^2_{ab} f$ of the Hessian changes fairly little on the operator norm ball.


\begin{lemma}\label{lem:block-perturbation-old}
For perturbation $v \to \otimes_{a} e^{\delta_{a}} \cdot v =: w$ where $\forall a: \|\delta_{a}\|_{op} \ll 1$, and let $\{\sigma_{1}^{ab}, \sigma_{2}^{ab}\}$ be the matrix norm $\|\cdot\|_{F} \to \|\cdot\|_{F}$ and matrix norm on subspace $\perp$ to $(I,I)$ for each bipartite part respectively:
\[ \forall a,b: \sigma_{2}^{ab}(w w^{*}) - \sigma_{2}^{ab}(v v^{*}) \leq O \left( \sum_{a} \|\delta_{a}\|_{op}  \right) \sigma_{1}^{ab}(v v^{*})   \]
The same is true for the diagonal blocks.
\end{lemma}

\CF{I think we are safe to just say $\|\nabla^2_{ab}f\|_{op}:=\|\nabla^2_{ab}f\|_{F\to F}$.}

\begin{lemma}\label{lem:block-perturbation}\CF{prev lemma in new notation; not married to the $\Pi$'s.} Let $\Pi$ denote the projection to the traceless matrices. There is a constant $c>0$ such that if $\|Z_a\|_{op} \leq c $ for all $a \in [k]$ we have
$$\|\nabla^2_{ab} f(Z) \circ \Pi\|_{op} - \|\nabla^2_{ab}f(I)  \circ \Pi \|_{op}  = O\left( \sum_{a \in [k]} \|Z_a\|_{op} \|\nabla^2_{ab}f\|_{op}\right)$$
for all $a,b \in [k]$.

\end{lemma}
\begin{proof}\CF{needs to be updated to new notation. At this point in the paper, there's no $M$.}
To lower bound the diagonal block, we just need a spectral lower bound on $\{\rho^{a}\}$, since $\langle vec(X), \nabla_{aa}^{2} (vec(X)) \rangle := \langle \rho^{a}, X^{2} \rangle$.
\[ \| e^{\delta_{a}} \rho^{a} e^{\delta_{a}} - Q_{a}\|_{op} \leq O(\|\delta_{a}\|_{op}) \|Q_{a}\|_{op}   \]
Now we address a perturbation on $b \neq a$. For a spectral lower bound, we choose test $Z \succeq 0$ and let $\delta := e^{2\delta_{b}} - I$:
\[ \langle e^{\delta_{b}} \rho e^{\delta_{b}} - \rho, I_{\overline{a}} \otimes Z_{a} \rangle
= \langle \rho, \delta \otimes Z \rangle = \langle Z, V^{*} \delta V \rangle   \]
Here $V \in \R^{d_{b} \times d_{a}}$ is the matricized version of $\rho$. But now since $Z \succeq 0$, the argument is clear
\[ \leq \langle Z, V^{*} |\delta| V \rangle \leq \|\delta\|_{op} \langle Z, V^{*} I V \rangle = \|\delta\|_{op} \langle \rho, I_{\overline{a}} \otimes Z \rangle \leq \|\delta\|_{op} \|\rho^{a}\|_{op} \|Z\|_{1}    \]

The argument for the off-diagonal blocks is similar. We first argue the change is small under perturbations just on those parts. 
\[ \langle vec(Y), M_{v}^{ab}(vec(Z)) \rangle := \langle v v^{*}, I_{\overline{ab}} \otimes Z \otimes Y \rangle \]
\[ \langle vec(Y), M_{w}^{ab}(vec(Z)) \rangle := \langle w w^{*}, I_{\overline{ab}} \otimes Z \otimes Y \rangle\]

\[ \implies M_{w} = (e^{\delta_{b}} \otimes e^{\delta_{b}}) M_{v} (e^{\delta_{a}} \otimes e^{\delta_{a}})   \]
\[ \implies \|M_{w} - M_{v}\|_{op} \leq O(\|\delta_{a}\|_{op} + \|\delta_{b}\|_{op}) \|M_{v}\|_{op}   \]
where in the last step we used $\delta \ll 1$.
\CF{comment things that we wouldn't want to accidentally leave in, as I have done in the next sentence}
The more difficult part of the argument to see \AR{at least for me} is the change caused be some other part $c \neq a,b$. First we define $\delta := e^{2 \delta_{c}} - I$, and test vectors $Z,Y$:
\[ \langle w w^{*} - v v^{*}, I_{\overline{ab}} \otimes Z \otimes Y \rangle = \langle v v^{*}, \delta \otimes Z \otimes Y \rangle  = \langle Z \otimes Y, V^{*} \delta V \rangle \]
Here $V \in \R^{d_{c} \times d_{a}d_{b}}$ is the matricized version of $v$, i.e. the $k$-th element of $ij$-th column is $(V_{ij})_{k} := v_{ijk}$. Now in order to use our operator norm bounds, we need to deal with cancelations, so we split into positive and negative parts $Z := Z_{+} - Z_{-}, Y := Y_{+} - Y_{-}$:
\[ |\langle Z \otimes Y, V^{*} \delta V \rangle| \leq |\langle Z_{\pm} \otimes Y_{\pm}, V^{*} \delta V \rangle |  \]
Now we analyze each of these terms:
\[ \leq |\langle Z_{\pm} \otimes Y_{\pm}, V^{*} |\delta| V \rangle| \leq \|\delta\|_{op} |\langle Z_{\pm} \otimes Y_{\pm}, V^{*} V \rangle| = \|\delta\|_{op} |\langle v v^{*}, I_{\overline{ab}} \otimes Z_{\pm} \otimes Y_{\pm} \rangle|   \]
Each of these terms we can bound by $\sigma^{ab}_{1} \|Z\|_{F} \|Y\|_{F}$. So by iterating this argument over all $c$, we get the desired bound.
\end{proof}

\begin{proof}[Proof of \cref{lem:perturbation}]
\[ \langle X, \nabla_{aa}^{2} X \rangle = \langle \rho^{a}, X^{2} \rangle \leq \|\rho^{a}\|_{op} \|X^{2}\|_{1} = \|\rho^{a}\|_{op} \|X\|_{F}^{2} \]
\AR{I am probably wrong on dimension factors here, but it's the right idea}
By the condition on the gradient \TODO{what condition? cref it}, we have that
\[ \forall a,b: \|\nabla_{ab}^{2}\|_{op}^{2} \leq \|\nabla_{aa}^{2}\|_{op} \|\nabla_{bb}^{2}\|_{op} = \|\rho^{a}\|_{op} \|\rho^{b}\|_{op} \leq \frac{1+\eps}{d_{a} d_{b}}   \]
We apply the perturbation lemma to each part sucessively, and if $\delta$ are small enough we can assume this bound holds in weaker form $1+\epsilon \leq 2$ for all iterations. The above lemma shows for each part and any test vectors
\[ \langle w w^{*} - v v^{*}, I_{\overline{ab}} \otimes \frac{Z}{\|Z\|_{F}} \otimes \frac{Y}{\|Y\|_{F}} \rangle \leq \frac{O( \sum_{a} \delta_{a} )}{\sqrt{d_{a} d_{b}}} =: \frac{\delta}{\sqrt{d_{a} d_{b}}}   \]
Here the suppressed constants are $\leq 7$. Therefore the difference between Hessians can be bounded
\[ |\langle Y, \nabla^{2} f(e^{Z}) - \nabla^{2} f(I), Y \rangle|  \leq \delta \left( \sum_{a} \frac{\|Y_{a}\|_{F}^{2}}{d_{a}} + \sum_{a \neq b} \frac{\|Y_{a}\|_{F} \|Y_{b}\|_{F}}{\sqrt{d_{a} d_{b}}} \right) \leq k \delta \|Y\|^{2}   \]
\end{proof}


After one more simple lemma, we will be ready to prove our second strong convexity result, \cref{lem:perturbation}.
\begin{lemma} [Lemma 3.6 in \TODO{cite KLR}; \CF{where is this used?}]
\AR{The amount we lose in robustness is related to the worst quadratic form in the whole space (not $\perp I$) since we have to break up into $\pm$ parts. }
\[ \|\nabla_{ab}^{2}\|_{F \to F}^{2} \leq \|\nabla_{aa}^{2}\|_{F \to F} \|\nabla_{bb}^{2}\|_{F \to F}   \]
\end{lemma}
\begin{proof}
\AR{New simple proof:} By convexity we know $\begin{pmatrix} \nabla_{aa}^{2} & \nabla_{ab}^{2} \\ \nabla_{ba}^{2} & \nabla_{bb}^{2}  \end{pmatrix} \succeq 0$. The result follows from e.g. Schur complements. 
\end{proof}



\subsection{Proof of \cref{thm:tensor-frobenius}}
We are now ready to prove the main result of this section according to the plan outlined in \cref{subsec:outline}.
\TODO{restate it here?}
\begin{proof}[Proof of \cref{thm:tensor-frobenius}]By \cref{it:reduce} in \cref{subsec:outline}, it is enough to prove the theorem assuming $\Theta = I$.
We seek to apply \cref{lem:convex-ball}, so we need $\lambda \kappa > \|\nabla f\|_{\vec d}$, where $\lambda$ is the strong convexity we can maintain in a $\kappa$-ball. Set $\delta = \eps/\max \sqrt{k d_a}$; by our first assumption on $\eps$, we have $N_a \geq C d_a/\delta^2$ for all $a \in [k]$, we may apply \cref{prop:gradient-bound}. Thus, with failure probability $O\left( \sum_a \exp ( - N_a \delta^2)\right)$ we have $\|(\nabla f)_{a}\|_{op} \leq \frac{\delta}{d_{a}}$ for all $a \in [k]$ and so
\[  \|\nabla f\|_{\vec d}^{2} = \sum_{a} d_{a} \|(\nabla f)_{a}\|_{F}^{2} \leq  k \delta^{2}.  \]
We next bound $\kappa$. By \cref{thm:tensor-convexity}, with failure probability $\CF{1/poly(D)}$ we have that $f$ is $1-o_{d}(1)$ strongly convex in norm $\|\cdot\|_{\vec d}$ at $I$, and further by \cref{lem:perturbation} $f$ is $1/2$ strongly convex at any point $e^{Z}$ with $\forall a: \|Z_{a}\|_{op} \leq  c/k$ for $c$ a small enough constant. Because $\|Z_a\|_{op} \leq \|Z_a\|_F \leq \sqrt{d_a} \|Z\|_{\vec d},$ we may take $\kappa = c/\max_a \sqrt{d_a}$ and $\lambda = 1/2$.

By \cref{lem:convex-ball}, provided $\sqrt{k} \delta \leq c/\max_a \sqrt{d_a} $ there is an optimizer $\widehat{\Theta}$ in a geodesic $2 \sqrt{k} \delta$-ball about $I$. The inequality follows from our second assumption on $\eps$. Because $\widehat{\Theta}$ is in the $\kappa$ ball, $\|\widehat{\Theta}_a\|_{op} \leq c$ and so $\|I_a - \widehat{\Theta}_a\|_F = O( \| \log \widehat{\Theta}_a\|_F) = O( \|\log \widehat{\Theta}\|_{\vec d} \sqrt{d_a})  \leq C \delta \sqrt{k d_a}.$ Applying $\eps = \delta \sqrt{k \max d_a}$ completes the proof.
\end{proof}



%If $\langle \cdot , \cdot \rangle_P$ is the metric at a point $P$, then the new metric $\langle \cdot, \cdot \rangle_{R,P}$ will be given by
%$$ \langle X, Y \rangle_{R,P} = \langle R^{1/2} X R^{1/2}, R^{1/2} Y R^{1/2} \rangle_{P}.$$

%At the identity the metric takes the form $\langle X, Y \rangle_R = \tr \sqrt{R} X  \sqrt{R} Y $. In particular the length of the geodesic $t\mapsto \sqrt{P}e^{Xt}\sqrt{P}$ for $t \in [0,1]$ is $\langle X, X \rangle_R$.

\bibliographystyle{alpha}
\bibliography{refs}

\end{document}
