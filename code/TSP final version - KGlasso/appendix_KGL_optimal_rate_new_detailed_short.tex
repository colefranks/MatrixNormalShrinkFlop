\begin{IEEEproof}
For ease of notation, let $\lambda_X^{(k)}=\frac{\bar{\lambda}_X^{(k)}}{f}$ and $\lambda_Y^{(k)}=\frac{\bar{\lambda}_Y^{(k)}}{p}$ for all $k \geq 1$. We show that the first iteration of the KGL algorithm yields a fast statistical convergence rate of $O_P\left( \sqrt{\frac{(p+f)\log M}{n}} \right)$ by appropriately adjusting the regularization parameters. A simple induction finishes the proof. %Second, we show that the second iteration maintains the same convergence rate as the first. Finally, we consider the second iteration as the base case and fix the regularization parameters to $\lambda_X \asymp \lambda_Y  \asymp \left( \frac{1}{\sqrt{f}} + \frac{1}{\sqrt{p}} \right) \sqrt{\frac{\log M}{n}}$ and use induction to show that the same fast rate holds for all subsequent iterations.
Adopt the notation from the proof of Thm. \ref{thm: FF_optimal_rate}.

%Let us first establish some asymptotic notation. For random matrices $\bX_1$ and $\bX_2$ of same order:
%\begin{equation*}
%	\bX_1 \cong \bX_2 \Longrightarrow \nn\bX_1-\bX_2\nn_F = o_p\left( \sqrt{\frac{M_{p,f}^2 \log(M_{p,f})}{n}} \right)
%\end{equation*}
%as $p,f,n\to\infty$.

Lemma \ref{lemma: large_dev_optimal} implies that for
\begin{equation} \label{condition_0}
	n \geq \frac{12 (2+\tau)^2}{\sqrt{\psi(\frac{1}{2+\tau})}} \log M
\end{equation}
then w.h.p.,
\begin{equation} \label{tilde_B_0_rate}
	|\tilde{\bB}^0|_\infty \leq \max_k [\bB_0]_{k,k} \nn\bA_{init}^{-1} \bA_0\nn_2 \sqrt{12 \psi(\frac{1}{2+\tau})} \sqrt{\frac{\log M}{np}}
\end{equation}
where $\tilde{\bB}^0=\hat{\bB}(\bA_{init})-\bB_*$. From Proposition \ref{prop: Glasso_optimal_rate} and (\ref{tilde_B_0_rate}), we obtain w.h.p.,
\begin{equation} \label{Y_Frob_error}
	\nn \bY_1-\bY_* \nn_F \leq \frac{2\sqrt{2} (1+c_y) \sqrt{1+c_{Y_0}}}{\lambda_{min}(\bB_*)^2} \sqrt{\frac{f\log M}{np}}
\end{equation}
where we also used $s_{Y_0}\leq c_{Y_0} f$ and $\bY_1:=\bG(\hat{\bB}(\bA_{init}),\lambda_Y^{(1)})=\bB_1^{-1}$. Note that $fp^{-1}\log M=o(n)$ was used here.

%Let $\check{\bB}^0:=\bB_1 - \bB_*$. Then, from the dual program (\ref{dualY}) and the triangle inequality, we have
%\begin{align}
%	|\check{\bB}^0|_\infty &\leq |\bB_1-\hat{\bB}(\bA_{init})|_\infty + |\tilde{\bB}^0|_\infty \nonumber \\
%		&\leq \lambda_Y^{(1)} + |\tilde{\bB}^0|_\infty \nonumber \\
%		&= O_P\left( \sqrt{\frac{\log M}{np}} \right) \label{check_B_0_rate}
%\end{align}

Let $\grave{\bA}^1:=\hat{\bA}(\bB_1) - \bA_*$. Then, we have
\begin{align}
	\vec(\grave{\bA}^1) &= \frac{1}{f} \hat{\bR}_A \vec(\bY_1) - \vec(\bA_*) \nonumber \\
		%&= \frac{1}{f} \hat{\bR}_A \vec(\bY_1-\bY_*) + \frac{1}{f}\tilde{\bR}_A \vec(\bB_*^{-1}) \nonumber \\
		%&= \frac{1}{f} \bR_A \vec(\bY_1-\bY_*) + \vec(\hat{\bA}(\bB_*)-\bA_*) + \frac{1}{f} \tilde{\bR}_A \vec(\bY_1-\bY_*) \nonumber \\
		&= \frac{\tr(\bB_0(\bY_1-\bY_*))}{f} \vec(\bA_0) + \vec(\hat{\bA}(\bB_*)-\bA_*) \nonumber \\
		&\quad + \frac{1}{f} \tilde{\bR}_A \vec(\bY_1-\bY_*) \label{grave_A_1}
\end{align}
where we used $\bR_A=\vec(\bA_0)\vec(\bB_0^T)^T$ (see Eq. (91) in \cite{EstCovMatKron}). Recall the definition of a mixed norm \cite{HornJohnson}:
\begin{equation} \label{mixed_norm_def}
	\nn \bW \nn_{\alpha,\beta} = \sup_{\nn \bv \nn_{\alpha}=1} \nn \bW \bv \nn_\beta
\end{equation}
From (\ref{Y_Frob_error}) and (\ref{mixed_norm_def}), we have w.h.p.,
\begin{align}
	\frac{1}{f} & \nn \tilde{\bR}_A \vec(\bY_1-\bY_*)\nn_\infty \leq \frac{\nn \tilde{\bR}_A \nn_{2,\infty}}{f} \nn \bY_1-\bY_* \nn_F \nonumber \\
		&\leq 4(C_1+\frac{C_2}{\kappa}) \max(1,\max_i [\bA_0]_{i,i} \frac{\nn\bB_0\nn_2}{\sqrt{f}}) \nonumber \\
		&\quad \times \frac{2\sqrt{2}(1+c_y)\sqrt{1+c_{Y_0}}}{\lambda_{min}(\bB_*)^2}  \frac{1}{\sqrt{p}} \frac{\log M}{n} \label{bound_infty}
\end{align}
where we used the bound on the mixed-norm-i.e., for
\begin{equation} \label{condition_1}
	n>16(C_1 \kappa + C_2)^2 \log M
\end{equation}
then w.h.p.,
\begin{align*}
	\frac{\nn \tilde{\bR}_A \nn_{2,\infty}}{f} &=\frac{1}{f} \sup_{\nn \mathbf{v}\nn_2=1} \nn \tilde{\bR}_A \mathbf{v}\nn_\infty \\\
		&= \sup_{\nn \mathbf{v}\nn_2=1} \nn \frac{1}{f} \hat{\bR}_A \mathbf{v} - \frac{<\vec(\bB_0),\mathbf{v}>}{f} \vec(\bA_0) \nn_\infty \nonumber \\
		&\leq 4(C_1+\frac{C_2}{\kappa}) \max(1,\max_i [\bA_0]_{i,i} \frac{\nn \bB_0 \nn_2}{\sqrt{f}} )  \sqrt{\frac{\log M}{nf}}
\end{align*}
where we used Lemma \ref{lemma: large_dev_Zhou}.


From (\ref{grave_A_1}), applying the triangle inequality and using the Cauchy-Schwarz inequality, (\ref{Y_Frob_error}), (\ref{bound_infty}), w.h.p.,
\begin{align}
	|\grave{\bA}^1|_\infty %&\leq \frac{|\tr(\bB_0(\bY_1-\bY_*))|}{f} |\bA_0|_\infty + |\hat{\bA}(\bB_*)-\bA_*|_\infty + \frac{1}{f}\nn \tilde{\bR}_A\vec(\bY_1-\bY_*) \nn_\infty \nonumber \\
		&\leq \frac{\sqrt{f} \nn \bB_0\nn_2 \nn \bY_1-\bY_*\nn_F}{f} |\bA_0|_\infty + |\hat{\bA}(\bB_*)-\bA_*|_\infty \nonumber \\
		&\quad + \frac{1}{f}\nn \tilde{\bR}_A\vec(\bY_1-\bY_*) \nn_\infty \nonumber \\
		&\leq \bar{C}_1 \left( \frac{1}{\sqrt{p}} + \frac{1}{\sqrt{f}} \right) \sqrt{\frac{\log M}{n}} + \bar{C}_2 \frac{1}{\sqrt{p}} \frac{\log M}{n} 	\label{grave_A_1_bound}
\end{align}
where we used Lemma \ref{lemma: large_dev_optimal}. The constants $\bar{C}_1$ and $\bar{C}_2$ are given by:
\begin{align*}
	\bar{C}_1 &= \kappa(\bA_0) \nn\bA_{init}\nn_2 \max \Big(\sqrt{12\psi(\frac{1}{2+\tau})}, \\
		&\quad \kappa(\bB_0) \nn\bY_0\nn_2 \left|\frac{p}{\tr(\bA_0\bA_{init}^{-1})}\right| 2\sqrt{2}(1+c_y)\sqrt{1+c_{Y_0}} \Big) \\
	\bar{C}_2 &= 4 (C_1+\frac{C_2}{\kappa}) \max(1,\max_i [\bA_0]_{i,i} \frac{\nn \bB_0\nn_2}{\sqrt{f}}) \nn \bY_0\nn_2^2 \\
		&\quad \times (\frac{p}{\tr(\bA_0\bA_{init}^{-1})})^2 2\sqrt{2} (1+c_y) \sqrt{1+c_{Y_0}}
\end{align*}
For 
\begin{equation} \label{condition_2}
	n> (\frac{\bar{C}_2}{\bar{C}_1 c'})^2 \frac{\log M}{(1+\sqrt{\frac{p}{f}})^2}
\end{equation}
the bound in (\ref{grave_A_1_bound}) simplifies to
\begin{equation} \label{grave_A_1_ub}
	|\grave{\bA}^1|_\infty \leq \bar{C}_1 (1+c') \left(\frac{1}{\sqrt{p}}+\frac{1}{\sqrt{f}}\right) \sqrt{\frac{\log M}{n}}
\end{equation}



From Proposition \ref{prop: Glasso_optimal_rate} and (\ref{grave_A_1_ub}), we obtain w.h.p.:
\begin{align}
	\nn &\bX_1-\bX_* \nn_F \leq \frac{2\sqrt{2}(1+c_x)\sqrt{1+c_{X_0}}}{\lambda_{min}(\bA_*)^2} \bar{C}_1 (1+c') \nonumber \\
		&\quad \times \left( 1 + \sqrt{\frac{p}{f}} \right) \sqrt{\frac{\log M}{n}} \label{X_Frob_error}
\end{align}
where we used $s_{X_0}\leq c_{X_0} p$ and $\bX_1:=\bG(\hat{\bA}(\bB_1), \lambda_X^{(1)})$, $\bX_*:=\bA_*^{-1}$. Note that $(1+\sqrt{p/f})^2\log M=o(n)$ was used here.

Finally, using (\ref{Y_Frob_error}) and (\ref{X_Frob_error}), we obtain w.h.p.:
\begin{align}
	& \nn \bTheta_{KGL}(2)-\bTheta_0 \nn_F = \nn \bX_1 \otimes \bY_1 - \bX_* \otimes \bY_* \nn_F \nonumber \\
		&\leq \nn \bY_1-\bY_*\nn_F \sqrt{p} \nn \bX_*\nn_2 + \nn \bX_1-\bX_*\nn_F \sqrt{f} \nn \bY_*\nn_2 \nonumber \\
		&\quad + \nn \bY_1-\bY_*\nn_F \nn \bX_1-\bX_*\nn_F \nonumber \\
		&\leq \bar{C}_3 (2\sqrt{f}+\sqrt{p}) \sqrt{\frac{\log M}{n}} + \bar{C}_4 (1+\sqrt{\frac{f}{p}}) \frac{\log M}{n} \label{Theta_bound}
\end{align}
where the constants are given by
\begin{align*}
	\bar{C}_3 &= 2\sqrt{2} \nn \bTheta_0\nn_2 \max\Big((1+c_y)\sqrt{1+c_{Y_0}} \nn\bY_0\nn_2\left|\frac{p}{\tr(\bA_0\bA_{init}^{-1})}\right|, \\
		&\quad (1+c_x)\sqrt{1+c_{X_0}} \bar{C}_1(1+c') \kappa(\bA_0) \nn\bA_{init}^{-1}\nn_2 \Big) \\
	\bar{C}_4 &= 8 \nn\bTheta_0\nn_2^2 (1+c_y)\sqrt{1+c_{Y_0}} (1+c_x)\sqrt{1+c_{X_0}} \bar{C}_1 (1+c')
\end{align*}
For
\begin{equation}
	n>(\frac{\bar{C}_4}{\bar{C}_3 c''})^2 \frac{(1+\sqrt{f/p})^2}{(2\sqrt{f}+\sqrt{p})^2} \log M
\end{equation}
the bound (\ref{Theta_bound}) further becomes:
\begin{equation*}
	\nn \bTheta_{KGL}(2)-\bTheta_0 \nn_F \leq \bar{C}_3 (1+c'') (2\sqrt{f}+\sqrt{p}) \sqrt{\frac{\log M}{n}}
\end{equation*}

Note that $\nn \bTheta_{KGL}(2)-\bTheta_0 \nn_F^2 = O_P\left( \frac{ (p+f+\sqrt{pf}) \log M}{n} \right) =  O_P\left( \frac{ (p+f) \log M}{n} \right)$ as $p,f,n\to\infty$. This concludes the first part of the proof. The rest of the proof follows by similar bounding arguments coupled with induction. The rate remains the same as the number of iterations increases, but the constant on front changes.

Next, we show that the convergence rate in the covariance matrix Frobenius error is on the same order as the inverse. From (\ref{Y_Frob_error}), for
\begin{equation}
	n>\frac{ (\epsilon' 2\sqrt{2} (1+c_y)\sqrt{1+c_{Y_0}})^2}{\lambda_{min}(\bB_*)^2} \kappa(\bY_0)^2 fp^{-1} \log M
\end{equation}
we have w.h.p. $\lambda_{min}(\bY_1) \geq \lambda_{min}(\bY_*)-\nn \bY_1-\bY_* \nn_F\geq \frac{\lambda_{min}(\bY_*)}{\epsilon'}$, which in turn implies w.h.p.,
\begin{align}
	\nn & \bB_1-\bB_* \nn_F \leq (1-\frac{1}{\epsilon'})^{-1} (2\sqrt{2} (1+c_y)\sqrt{1+c_{Y_0}}) \kappa(\bY_0)^2 \nonumber \\
		&\quad \times \sqrt{\frac{f}{p}} \sqrt{\frac{\log M}{n}} \label{B1_Frob_error}
\end{align}
\footnote{Here, $\bB_1 = \bY_1^{-1}$ exists since $\bY_1$ is positive definite (see (\ref{G_operator})).}
Using a similar argument, from (\ref{X_Frob_error}), for 
\begin{align*}
	n &>\frac{(\epsilon' 2\sqrt{2}(1+c_x)\sqrt{1+c_{X_0}} \bar{C}_1(1+c'))^2}{\lambda_{min}(\bA_*)^2} \kappa(\bX_0)^2 \\
		&\times (1+\sqrt{\frac{p}{f}})^2 \log M
\end{align*}
we have w.h.p.,
\begin{align}
	\nn &\bA_1-\bA_* \nn_F \leq (1-\frac{1}{\epsilon'})^{-1} (2\sqrt{2}(1+c_x)\sqrt{1+c_{X_0}} \bar{C}_1(1+c')) \nonumber \\
		&\quad \times \kappa(\bX_0)^2 \left(1+\sqrt{\frac{p}{f}}\right) \sqrt{\frac{\log M}{n}} \label{A1_Frob_error}
\end{align}
where $\bA_1=\bX_1^{-1}$.

Let $\bSigma_{KGL}(2):=\bTheta_{KGL}(2)^{-1}=\bA_1\otimes \bB_1$. Then, w.h.p.,
\begin{align}
	\nn & \bSigma_{KGL}(2)-\bSigma_0 \nn_F \leq \nn \bA_1-\bA_*\nn_F \nn \bB_*\nn_F \nonumber \\
		&\quad + \nn \bB_1-\bB_*\nn_F \nn \bA_*\nn_F + \nn\bA_1-\bA_*\nn_F \nn\bB_1-\bB_*\nn_F \nonumber \\
		&\leq \bar{D}_1 (2\sqrt{f}+\sqrt{p}) \sqrt{\frac{\log M}{n}} + \bar{D}_2 (1+\sqrt{\frac{f}{p}}) \frac{\log M}{n} \label{Sigma_KGL_error}
\end{align}
where the constants are:
\begin{align*}
	\bar{D}_1 &= (1-\frac{1}{\epsilon'})^{-1} 2\sqrt{2} \max\Big((1+c_x)\sqrt{1+c_{X_0}} \bar{C}_1 (1+c') \\
		&\quad \times \kappa(\bX_0)^2, (1+c_y) \sqrt{1+c_{Y_0}} \kappa(\bY_0)^2 \Big) \\
	\bar{D}_2 &= 8(1-\frac{1}{\epsilon'})^{-2} (1+c_x)\sqrt{1+c_{X_0}}(1+c_y)\sqrt{1+c_{Y_0}} \\
		&\quad \times \kappa(\bX_0)^2\kappa(\bY_0)^2 \bar{C}_1(1+c')
\end{align*}
For
\begin{equation*}
	n>(\frac{\bar{D}_2}{\bar{D}_1 d})^2 \left(\frac{1+\sqrt{\frac{f}{p}}}{2\sqrt{f}+\sqrt{p}}\right)^2 \log M
\end{equation*}
then (\ref{Sigma_KGL_error}) implies w.h.p.,
\begin{equation*}
	\nn \bSigma_{KGL}(2)-\bSigma_0 \nn_F \leq \bar{D}_1 (1+d) (2\sqrt{f}+\sqrt{p}) \sqrt{\frac{\log M}{n}}
\end{equation*}
Thus, the same rate $O_P\left( \sqrt{\frac{(p+f) \log M}{n}} \right)$ holds for the error in the covariance matrix.



%Next, define the error quantities:
%\begin{align*}
%	\check{\bA}^1 &= \bA_1 - \bA_* \\
%	\grave{\bB}^2 &= \hat{\bB}(\bA_1) - \bB_* \\
%	\check{\bB}^1 &= \bB_2 - \bB_* \\
%	\grave{\bA}^2 &= \hat{\bA}(\bB_2) - \bA_*
%\end{align*}
%where $\bA_1 := \bX_1^{-1}$.
%
%Continuing to propagate the statistics into the second KGL iteration, we obtain the base case:
%\begin{align}
%	|\check{\bA}^1|_\infty &= O_P\left( \left(\frac{1}{\sqrt{p}} + \frac{1}{\sqrt{f}} \right) \sqrt{\frac{\log M}{n}} \right) \label{eq_base_case_1} \\
%	|\grave{\bB}^2|_\infty &= O_P\left( \left(\frac{1}{\sqrt{p}} + \frac{1}{\sqrt{f}} \right) \sqrt{\frac{\log M}{n}} \right) \label{eq_base_case_2} \\
%	|\check{\bB}^1|_\infty &= O_P\left( \left(\frac{1}{\sqrt{p}} + \frac{1}{\sqrt{f}} \right) \sqrt{\frac{\log M}{n}} \right) \label{eq_base_case_3} \\
%	|\grave{\bA}^2|_\infty &= O_P\left( \left(\frac{1}{\sqrt{p}} + \frac{1}{\sqrt{f}} \right) \sqrt{\frac{\log M}{n}} \right) \label{eq_base_case_4}
%\end{align}
%Eq. (\ref{eq_base_case_1}) follows from $\lambda_X^{(1)}\asymp \left(\frac{1}{\sqrt{p}} + \frac{1}{\sqrt{f}} \right) \sqrt{\frac{\log M}{n}}$ and using (\ref{grave_A_1_bound}) in:
%\begin{align*}
%	|\check{\bA}^1|_\infty &\leq |\bA_1 - \hat{\bA}(\bB_1)|_\infty + |\hat{\bA}(\bB_1)-\bA_*|_\infty \\
%		&\leq \lambda_X^{(1)} + |\grave{\bA}^1|_\infty
%\end{align*}
%
%Using similar techniques as before, we can write:
%\begin{align}
%	\vec( &\grave{\bB}^2) = \frac{1}{p} \hat{\bR}_B \vec(\bX_1) - \vec(\bB_*)\nonumber \\
%		&=  \frac{\tr(\bA_0(\bX_1-\bX_*))}{p}\vec(\bB_0) + \vec(\hat{\bB}(\bA_*)-\bB_*) + \frac{1}{p}\tilde{\bR}_B \vec(\bX_1-\bX_*) \label{grave_B_2}
%\end{align}
%Using similar bounds parallel to (\ref{bound_infty}) and (\ref{grave_A_1_bound}), it follows that (\ref{eq_base_case_2}) holds from:
%\begin{align*}
%	|\grave{\bB}^2|_\infty &\leq \frac{\nn \bA_0\nn_2 \nn \bX_1-\bX_* \nn_F}{\sqrt{p}} |\bB_0|_\infty + |\hat{\bB}(\bA_*)-\bB_*|_\infty + \frac{1}{p} \nn \tilde{\bR}_B \vec(\bX_1-\bX_*) \nn_\infty \\
%	&= O_P\left( \left(\frac{1}{\sqrt{p}} + \frac{1}{\sqrt{f}} \right) \sqrt{\frac{\log M}{n}} \right)
%\end{align*}
%Eq. (\ref{eq_base_case_3}) similarly follows from using $\lambda_Y^{(2)}\asymp \left(\frac{1}{\sqrt{p}} + \frac{1}{\sqrt{f}} \right) \sqrt{\frac{\log M}{n}}$ and (\ref{eq_base_case_2}) in:
%\begin{align*}
%	|\check{\bB}^1|_\infty &\leq |\bB_2 - \hat{\bB}(\bA_1)|_\infty + |\hat{\bB}(\bA_1)-\bB_*|_\infty \\
%		&\leq \lambda_Y^{(2)} + |\grave{\bB}^2|_\infty
%\end{align*}
%From (\ref{eq_base_case_2}), $s_{Y_0}=O(f)$, $\bY_2 = \bG(\hat{\bB}(\bA_1),\lambda_Y^{(2)})$, we have:
%\begin{equation*}
%	\nn \bY_2 - \bY_* \nn_F = O_P\left( \left(1 + \sqrt{\frac{f}{p}}\right) \sqrt{\frac{\log M}{n}} \right)
%\end{equation*}
%Eq. (\ref{eq_base_case_4}) follows by using similar bounds as the ones to show (\ref{eq_base_case_2}).
%
%Similarly, from (\ref{eq_base_case_4}), $s_{X_0}=O(p)$, $\bX_2 = \bG(\hat{\bA}(\bB_2),\lambda_X^{(2)})$, we have:
%\begin{equation*}
%	\nn \bX_2 - \bX_* \nn_F = O_P\left( \left(1 + \sqrt{\frac{p}{f}}\right) \sqrt{\frac{\log M}{n}} \right)
%\end{equation*}
%
%Finally, we consider Eq. (\ref{eq_base_case_1})-(\ref{eq_base_case_2}) as the base case and use induction to finish the proof. Define the intermediate error quantities for $k\geq 2$:
%\begin{align*}
%	\check{\bA}^{k-1} &= \bA_{k-1} - \bA_* \\
%	\grave{\bB}^k &= \hat{\bB}(\bA_{k-1}) - \bB_* \\
%	\check{\bB}^{k-1} &= \bB_k - \bB_* \\
%	\grave{\bA}^k &= \hat{\bA}(\bB_k) - \bA_* 
%\end{align*}
%Acording to the above arguments, $|\check{\bA}^{k-1}|_\infty=O_P(\left(\frac{1}{\sqrt{p}} + \frac{1}{\sqrt{f}} \right) \sqrt{\frac{\log M}{n}})$ implies $|\grave{\bA}^k|_\infty =O_P(\left(\frac{1}{\sqrt{p}} + \frac{1}{\sqrt{f}} \right) \sqrt{\frac{\log M}{n}})$. Assume $|\grave{\bA}^k|_\infty = O_P(\left(\frac{1}{\sqrt{p}} + \frac{1}{\sqrt{f}} \right) \sqrt{\frac{\log M}{n}})$ for some $k\geq 2$. To finish the proof, it suffices to show this implies $|\grave{\bA}^{k+1}|_\infty = O_P(\left(\frac{1}{\sqrt{p}} + \frac{1}{\sqrt{f}} \right) \sqrt{\frac{\log M}{n}})$.
%
%Then, continuing as before, we have:
%\begin{align}
%	|\check{\bA}^k|_\infty &\leq |\bA_* - \hat{\bA}(\bB_k)|_\infty + |\hat{\bA}(\bB_k) - \bA_*|_\infty \nonumber \\
%		&\leq \lambda_X^{(k)} + |\grave{\bA}^k|_\infty \nonumber \\
%		&=O_P\left( \left(\frac{1}{\sqrt{p}} + \frac{1}{\sqrt{f}} \right) \sqrt{\frac{\log M}{n}} \right) \label{check_A_k}
%\end{align}
%Using (\ref{check_A_k}), we obtain:
%\begin{align}
%	|\grave{\bB}^{k+1}|_\infty &\leq \frac{\sqrt{p} \nn\bA_0\nn_2 \nn\bX_k-\bX_*\nn_F}{p}|\bB_0|_\infty + |\hat{\bB}(\bA_*)-\bB_*|_\infty \nonumber \\
%		&\quad + \frac{1}{p} \nn \tilde{\bR}_B \vec(\bX_k-\bX_*) \nn_\infty \\
%		&=O_P\left( \left(\frac{1}{\sqrt{p}} + \frac{1}{\sqrt{f}} \right) \sqrt{\frac{\log M}{n}} \right) \label{grave_B_kplus1}
%\end{align}
%Repeating this argument, we have:
%\begin{align}
%	|\check{\bB}^k|_\infty &\leq |\bB_{k+1} - \hat{\bB}(\bA_k)|_\infty + |\hat{\bB}(\bA_k)-\bB_*|_\infty \nonumber \\
%		&\leq \lambda_Y^{(k+1)} + |\grave{\bB}^{k+1}|_\infty \nonumber \\
%		&=O_P\left( \left(\frac{1}{\sqrt{p}} + \frac{1}{\sqrt{f}} \right) \sqrt{\frac{\log M}{n}} \right) \label{check_B_k}
%\end{align}
%Using (\ref{check_B_k}), we obtain:
%\begin{align}
%	|\grave{\bA}^{k+1}|_\infty &\leq \frac{\sqrt{f} \nn\bB_0\nn_2 \nn\bY_{k+1}-\bY_*\nn_F}{f}|\bA_0|_\infty + |\hat{\bA}(\bB_*)-\bA_*|_\infty \nonumber \\
%		&\quad + \frac{1}{f} \nn \tilde{\bR}_A \vec(\bY_{k+1}-\bY_*) \nn_\infty \nonumber \\
%		&=O_P\left( \left(\frac{1}{\sqrt{p}} + \frac{1}{\sqrt{f}} \right) \sqrt{\frac{\log M}{n}} \right)
%\end{align}
%
%We thus conclude that $|\check{\bA}^{k-1}|_\infty, |\grave{\bB}^k|_\infty, |\check{\bB}^{k-1}|_\infty, |\grave{\bA}^k|_\infty$ all have the same rate $O_P\left( \left(\frac{1}{\sqrt{p}} + \frac{1}{\sqrt{f}} \right) \sqrt{\frac{\log M}{n}} \right)$ for $k\geq 2$. This implies that for $k\geq 2$, since $s_{Y_0}=O(f)$, $\bY_k = \bG(\hat{\bB}(\bA_{k-1}),\lambda_Y^{(k)})$, we have:
%\begin{equation*}
%	\nn \bY_k - \bY_* \nn_F = O_P\left( \left(1 + \sqrt{\frac{f}{p}} \right) \sqrt{\frac{\log M}{n}} \right)
%\end{equation*}
%and since $s_{X_0}=O(p)$, $\bX_k = \bG(\hat{\bA}(\bB_k),\lambda_X^{(k)})$, we have:
%\begin{equation*}
%	\nn \bX_k - \bX_* \nn_F = O_P\left( \left(1 + \sqrt{\frac{p}{f}} \right) \sqrt{\frac{\log M}{n}} \right)
%\end{equation*}
%The triangle inequality concludes the proof (see (\ref{total_error})).
%

\end{IEEEproof}
